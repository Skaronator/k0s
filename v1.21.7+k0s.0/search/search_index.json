{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>k0s is an all-inclusive Kubernetes distribution, configured with all of the features needed to build a Kubernetes cluster simply by copying and running an executable file on each target host.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Available as a single static binary</li> <li>Offers a self-hosted, isolated control plane</li> <li>Supports a variety of storage backends, including etcd, SQLite, MySQL (or any compatible), and PostgreSQL.</li> <li>Offers an Elastic control plane</li> <li>Vanilla upstream Kubernetes</li> <li>Supports custom container runtimes (containerd is the default)</li> <li>Supports custom Container Network Interface (CNI) plugins (calico is the default)</li> <li>Supports x86_64 and arm64</li> </ul>"},{"location":"#join-the-community","title":"Join the Community","text":"<p>We welcome your help in building k0s! If you are interested, we invite you to check out the k0s Contributing Guide and our Code of Conduct.</p>"},{"location":"#demo","title":"Demo","text":""},{"location":"#downloading-k0s","title":"Downloading k0s","text":"<p>Download k0s for linux amd64 and arm64 architectures.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Quick Start Guide for creating a full Kubernetes cluster with a single node.</p>"},{"location":"CODE_OF_CONDUCT/","title":"k0s Community Code Of Conduct","text":"<p>Please refer to our contributor code of conduct.</p>"},{"location":"FAQ/","title":"Frequently asked questions","text":""},{"location":"FAQ/#how-is-k0s-pronounced","title":"How is k0s pronounced?","text":"<p>kay-zero-ess</p>"},{"location":"FAQ/#how-do-i-run-a-single-node-cluster","title":"How do I run a single node cluster?","text":"<p>The cluster can be started with:</p> <pre><code>k0s controller --single\n</code></pre> <p>See also the Getting Started tutorial.</p>"},{"location":"FAQ/#how-do-i-connect-to-the-cluster","title":"How do I connect to the cluster?","text":"<p>You find the config in <code>${DATADIR}/pki/admin.conf</code> (default: <code>/var/lib/k0s/pki/admin.conf</code>). Copy this file, and change the <code>localhost</code> entry to the public ip of the controller. Use the modified config to connect with kubectl:</p> <pre><code>export KUBECONFIG=/path/to/admin.conf\nkubectl ...\n</code></pre>"},{"location":"FAQ/#why-doesnt-kubectl-get-nodes-list-the-k0s-controllers","title":"Why doesn't <code>kubectl get nodes</code> list the k0s controllers?","text":"<p>As a default, the control plane does not run kubelet at all, and will not accept any workloads, so the controller will not show up on the node list in kubectl. If you want your controller to accept workloads and run pods, you do so with: <code>k0s controller --enable-worker</code> (recommended only as test/dev/POC environments).</p>"},{"location":"FAQ/#is-k0sproject-really-open-source","title":"Is k0sproject really open source?","text":"<p>Yes, k0sproject is 100% open source. The source code is under Apache 2 and the documentation is under the Creative Commons License. Mirantis, Inc. is the main contributor and sponsor for this OSS project: building all the binaries from upstream, performing necessary security scans and calculating checksums so that it's easy and safe to use. The use of these ready-made binaries are subject to Mirantis EULA and the binaries include only open source software.</p>"},{"location":"airgap-install/","title":"Airgap install","text":"<p>You can install k0s in an environment with restricted Internet access. Airgap installation requires an image bundle, which contains all the needed container images. There are two options to get the image bundle:</p> <ul> <li>Use a ready-made image bundle, which is created for each k0s release. It can be downloaded from the releases page.</li> <li>Create your own image bundle. In this case, you can easily customize the bundle to also include container images, which are not used by default in k0s.</li> </ul>"},{"location":"airgap-install/#prerequisites","title":"Prerequisites","text":"<p>In order to create your own image bundle, you need</p> <ul> <li>A working cluster with at least one controller, to be used to build the image bundle. For more information, refer to the Quick Start Guide.</li> <li>The containerd CLI management tool <code>ctr</code>, installed on the worker machine (refer to the ContainerD getting-started guide).</li> </ul>"},{"location":"airgap-install/#1-create-your-own-image-bundle-optional","title":"1. Create your own image bundle (optional)","text":"<p>k0s/containerd uses OCI (Open Container Initiative) bundles for airgap installation. OCI bundles must be uncompressed. As OCI bundles are built specifically for each architecture, create an OCI bundle that uses the same processor architecture (x86-64, ARM64, ARMv7) as on the target system.</p> <p>k0s offers two methods for creating OCI bundles, one using Docker and the other using a previously set up k0s worker. Be aware, though, that you cannot use the Docker method for the ARM architectures due to kube-proxy image multiarch manifest problem.</p>"},{"location":"airgap-install/#docker","title":"Docker","text":"<ol> <li> <p>Pull the images.</p> <pre><code>k0s airgap list-images | xargs -I{} docker pull {}\n</code></pre> </li> <li> <p>Create a bundle.</p> <pre><code>docker image save $(k0s airgap list-images | xargs) -o bundle_file\n</code></pre> </li> </ol>"},{"location":"airgap-install/#previously-set-up-k0s-worker","title":"Previously set up k0s worker","text":"<p>As containerd pulls all the images during the k0s worker normal bootstrap, you can use it to build the OCI bundle with images.</p> <p>Use the following commands on a machine with an installed k0s worker:</p> <pre><code>ctr --namespace k8s.io \\\n--address /run/k0s/containerd.sock \\\nimages export bundle_file $(k0s airgap list-images | xargs)\n</code></pre>"},{"location":"airgap-install/#2a-sync-the-bundle-file-with-the-airgapped-machine-locally","title":"2a. Sync the bundle file with the airgapped machine (locally)","text":"<p>Copy the <code>bundle_file</code> you created in the previous step or downloaded from the releases page to the target machine into the <code>images</code> directory in the k0s data directory. Copy the bundle only to the worker nodes. Controller nodes don't use it.</p> <pre><code># mkdir -p /var/lib/k0s/images\n# cp bundle_file /var/lib/k0s/images/bundle_file\n</code></pre>"},{"location":"airgap-install/#2b-sync-the-bundle-file-with-the-airgapped-machines-remotely-with-k0sctl","title":"2b. Sync the bundle file with the airgapped machines (remotely with k0sctl)","text":"<p>As an alternative to the previous step, you can use k0sctl to upload the bundle file to the worker nodes. k0sctl can also be used to upload k0s binary file to all nodes. Take a look at this example (k0sctl.yaml) with one controller and one worker node to upload the bundle file and k0s binary:</p> <pre><code>apiVersion: k0sctl.k0sproject.io/v1beta1\nkind: Cluster\nmetadata:\nname: k0s-cluster\nspec:\nhosts:\n- ssh:\naddress: &lt;ip-address-controller&gt;\nuser: ubuntu\nkeyPath: /path/.ssh/id_rsa\nrole: controller\nuploadBinary: true\nk0sBinaryPath: /path/to/k0s_binary/k0s\n- ssh:\naddress: &lt;ip-address-worker&gt;\nuser: ubuntu\nkeyPath: /path/.ssh/id_rsa\nrole: worker\nuploadBinary: true\nk0sBinaryPath: /path/to/k0s_binary/k0s\nfiles:\n- name: bundle-file\nsrc: /path/to/bundle-file/airgap-bundle-amd64.tar\ndstDir: /var/lib/k0s/images/\nperm: 0755\nk0s:\nversion: 1.21.7+k0s.0\n</code></pre>"},{"location":"airgap-install/#3-ensure-pull-policy-in-the-k0syaml-optional","title":"3. Ensure pull policy in the k0s.yaml (optional)","text":"<p>Use the following <code>k0s.yaml</code> to ensure that containerd does not pull images for k0s components from the Internet at any time.</p> <pre><code>apiVersion: k0s.k0sproject.io/v1beta1\nkind: Cluster\nmetadata:\nname: k0s\nspec:\nimages:\ndefault_pull_policy: Never\n</code></pre>"},{"location":"airgap-install/#4-set-up-the-controller-and-worker-nodes","title":"4. Set up the controller and worker nodes","text":"<p>Refer to the Manual Install for information on setting up the controller and worker nodes locally. Alternatively, you can use k0sctl.</p> <p>Note: During the worker start up k0s imports all bundles from the <code>$K0S_DATA_DIR/images</code> before starting <code>kubelet</code>.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>Note: As k0s is a new and dynamic project, the product architecture may occasionally outpace the documentation. The high level concepts and patterns, however, should always apply.</p>"},{"location":"architecture/#packaging","title":"Packaging","text":"<p>The k0s package is a single, self-extracting binary that embeds Kubernetes binaries, the benefits of which include:</p> <ul> <li>Statically compiled</li> <li>No OS-level dependencies</li> <li>Requires no RPMs, dependencies, snaps, or any other OS-specific packaging</li> <li>Provides a single package for all operating systems</li> <li>Allows full version control for each dependency</li> </ul> <p></p>"},{"location":"architecture/#control-plane","title":"Control plane","text":"<p>As a single binary, k0s acts as the process supervisor for all other control plane components. As such, there is no container engine or kubelet running on controllers by default, which thus means that a cluster user cannot schedule workloads onto controller nodes.</p> <p></p> <p>Using k0s you can create, manage, and configure each of the components, running each as a \"naked\" process. Thus, there is no container engine running on the controller node.</p>"},{"location":"architecture/#storage","title":"Storage","text":"<p>Kubernetes control plane typically supports only etcd as the datastore. k0s, however, supports many other datastore options in addition to etcd, which it achieves by including kine. Kine allows the use of a wide variety of backend data stores, such as MySQL, PostgreSQL, SQLite, and dqlite (refer to the <code>spec.storage</code> documentation).</p> <p>In the case of k0s managed etcd, k0s manages the full lifecycle of the etcd cluster. For example, by joining a new controller node with <code>k0s controller \"long-join-token\"</code> k0s  atomatically adjusts the etcd cluster membership info to allow the new member to join the cluster.</p> <p>Note: k0s cannot shrink the etcd cluster. As such, to shut down the k0s controller on a node that node must first be manually removed from the etcd cluster.</p>"},{"location":"architecture/#worker-node","title":"Worker node","text":"<p>As with the control plane, with k0s you can create and manage the core worker components as naked processes on the worker node.</p> <p>By default, k0s workers use containerd as a high-level runtime and runc as a low-level runtime. Custom runtimes are also supported, refer to Using custom CRI runtime.</p>"},{"location":"backup/","title":"Backup/Restore overview","text":"<p>k0s has integrated support for backing up cluster state and configuration. The k0s backup utility is aiming to back up and restore k0s managed parts of the cluster.</p> <p>The backups created by <code>k0s backup</code> command have following pieces of your cluster:</p> <ul> <li>certificates (the content of the <code>&lt;data-dir&gt;/pki</code> directory)</li> <li>etcd snapshot, if the etcd datastore is used</li> <li>Kine/SQLite snapshot, if the Kine/SQLite datastore is used</li> <li>k0s.yaml</li> <li>any custom defined manifests under the <code>&lt;data-dir&gt;/manifests</code></li> <li>any image bundles located under the <code>&lt;data-dir&gt;/images</code></li> <li>any helm configuration</li> </ul> <p>Parts NOT covered by the backup utility:</p> <ul> <li>PersistentVolumes of any running application</li> <li>datastore, in case something else than etcd or Kine/SQLite is used</li> <li>any configuration to the cluster introduced by manual changes (e.g. changes that weren't saved under the <code>&lt;data-dir&gt;/manifests</code>)</li> </ul> <p>Any of the backup/restore related operations MUST be performed on the controller node.</p>"},{"location":"backup/#backuprestore-a-k0s-node-locally","title":"Backup/restore a k0s node locally","text":""},{"location":"backup/#backup-local","title":"Backup (local)","text":"<p>To create backup run the following command on the controller node:</p> <pre><code>k0s backup --save-path=&lt;directory&gt;\n</code></pre> <p>The directory used for the <code>save-path</code> value must exist and be writable. The default value is the current working directory. The command provides backup archive using following naming convention: <code>k0s_backup_&lt;ISODatetimeString&gt;.tar.gz</code></p> <p>Because of the DateTime usage, it is guaranteed that none of the previously created archives would be overwritten.</p>"},{"location":"backup/#restore-local","title":"Restore (local)","text":"<p>To restore cluster state from the archive use the following command on the controller node:</p> <pre><code>k0s restore /tmp/k0s_backup_2021-04-26T19_51_57_000Z.tar.gz\n</code></pre> <p>The command would fail if the data directory for the current controller has overlapping data with the backup archive content.</p> <p>The command would use the archived <code>k0s.yaml</code> as the cluster configuration description.</p> <p>In case if your cluster is HA, after restoring single controller node, join the rest of the controller nodes to the cluster. E.g. steps for N nodes cluster would be:</p> <ul> <li>Restore backup on fresh machine</li> <li>Run controller there</li> <li>Join N-1 new machines to the cluster the same way as for the first setup.</li> </ul>"},{"location":"backup/#backuprestore-a-k0s-cluster-using-k0sctl","title":"Backup/restore a k0s cluster using k0sctl","text":"<p>With k0sctl you can perform cluster level backup and restore remotely with one command.</p>"},{"location":"backup/#backup-remote","title":"Backup (remote)","text":"<p>To create backup run the following command:</p> <pre><code>k0sctl backup\n</code></pre> <p>k0sctl connects to the cluster nodes to create a backup. The backup file is stored in the current working directory.</p>"},{"location":"backup/#restore-remote","title":"Restore (remote)","text":"<p>To restore cluster state from the archive use the following command:</p> <pre><code>k0sctl apply --restore-from /path/to/backup_file.tar.gz\n</code></pre> <p>The control plane load balancer address (externalAddress) needs to remain the same between backup and restore. This is caused by the fact that all worker node components connect to this address and cannot currently be re-configured.</p>"},{"location":"cis_benchmark/","title":"CIS Benchmark","text":"<p>The Center for Internet Security provides set of benchmarks that help us harden our clusters. k0s clusters by default will pass CIS benchmarks with couple of exceptions.</p> <p>For CIS compliance verification we are using <code>kube-bench</code>.</p>"},{"location":"cis_benchmark/#run","title":"Run","text":"<p><code>kube-bench</code> is CIS compliance verification tool.</p> <p>Follow install instructions.</p> <p>After installing the kube-bench on the host that is running <code>k0s</code> cluster run follwing command:</p> <pre><code>kube-bench run --config-dir docs/kube-bench/cfg/ --benchmark k0s-1.0\n</code></pre>"},{"location":"cis_benchmark/#summary-of-disabled-checks","title":"Summary of disabled checks","text":""},{"location":"cis_benchmark/#master-node-security-configuration","title":"Master Node Security Configuration","text":"<p>Current configuration has in total of 8 master checks disabled:</p> <ol> <li> <p>id: 1.2.10 - EventRateLimit requires external yaml config. It is left for the users to configure it</p> <pre><code>type: skip\ntext: \"Ensure that the admission control plugin EventRateLimit is set (Manual)\"\n</code></pre> </li> <li> <p>id: 1.2.12 - By default this isn't passed to the apiserver for air-gap functionality</p> <pre><code>type: skip\ntext: \"Ensure that the admission control plugin AlwaysPullImages is set (Manual)\"\n</code></pre> </li> <li> <p>id: 1.2.22 - For sake of simplicity of k0s all audit configuration are skipped. It is left for the users to configure it</p> <pre><code>type: skip\ntext: \"Ensure that the --audit-log-path argument is set (Automated)\"\n</code></pre> </li> <li> <p>id: 1.2.23 - For sake of simplicity of k0s all audit configuration are skipped. It is left for the users to configure it</p> <pre><code>type: skip\ntext: \"Ensure that the --audit-log-maxage argument is set to 30 or as appropriate (Automated)\"\n</code></pre> </li> <li> <p>id: 1.2.24 - For sake of simplicity of k0s all audit configuration are skipped. It is left for the users to configure it</p> <pre><code>type: skip\ntext: \"Ensure that the --audit-log-maxbackup argument is set to 10 or as appropriate (Automated)\"\n</code></pre> </li> <li> <p>id: 1.2.25 - For sake of simplicity of k0s all audit configuration are skipped. It is left for the users to configure it</p> <pre><code>type: skip\ntext: \"Ensure that the --audit-log-maxsize argument is set to 100 or as appropriate (Automated)\"\n</code></pre> </li> <li> <p>id: 1.2.33 - By default it is not enabled. Left for the users to decide</p> <pre><code>type: skip\ntext: \"Ensure that the --encryption-provider-config argument is set as appropriate (Manual)\"\n</code></pre> </li> <li> <p>id: 1.2.34 - By default it is not enabled. Left for the users to decide</p> <pre><code>type: skip\ntext: \"Ensure that encryption providers are appropriately configured (Manual)\"\n</code></pre> </li> </ol>"},{"location":"cis_benchmark/#worker-node-security-configuration","title":"Worker Node Security Configuration","text":"<p>and 4 node checks disabled:</p> <ol> <li> <p>id: 4.1.1 - not applicable since k0s does not use kubelet service file</p> <pre><code>type: skip\ntext: \"Ensure that the kubelet service file permissions are set to 644 or more restrictive (Automated)\"\n</code></pre> </li> <li> <p>id: 4.1.2 - not applicable since k0s does not use kubelet service file</p> <pre><code>type: skip\ntext: \"Ensure that the kubelet service file ownership is set to root:root (Automated)\"\n</code></pre> </li> <li> <p>id: 4.2.6 - k0s does not set this. See https://github.com/kubernetes/kubernetes/issues/66693</p> <pre><code>type: skip\ntext: \"Ensure that the --protect-kernel-defaults argument is set to true (Automated)\"\n</code></pre> </li> <li> <p>id: 4.2.10 - k0s doesn't set this because certs get auto rotated</p> <pre><code>type: skip\ntext: \"Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate (Manual)\"\n</code></pre> </li> </ol>"},{"location":"cis_benchmark/#control-plane-configuration","title":"Control Plane Configuration","text":"<p>3 checks for control plane:</p> <ol> <li> <p>id: 3.1.1 - For purpose of being fully automated k0s is skipping this check</p> <pre><code>type: skip\ntext: \"Client certificate authentication should not be used for users (Manual)\"\n</code></pre> </li> <li> <p>id: 3.2.1 - out-of-the box configuration does not have any audit policy configuration but users can customize it in spec.api.extraArgs section of the config</p> <pre><code>type: skip\ntext: \"Ensure that a minimal audit policy is created (Manual)\"\n</code></pre> </li> <li> <p>id: 3.2.2 - Same as previous</p> <pre><code>type: skip\ntext: \"Ensure that the audit policy covers key security concerns (Manual)\"\n</code></pre> </li> </ol>"},{"location":"cis_benchmark/#kubernetes-policies","title":"Kubernetes Policies","text":"<p>Entire policies checks are disabled too. The checks are manual and are up to the end user to decide on them.</p>"},{"location":"cloud-providers/","title":"Cloud providers","text":"<p>k0s builds Kubernetes components in providerless mode, meaning that cloud providers are not built into k0s-managed Kubernetes components. As such, you must externally configure the cloud providers to enable their support in your k0s cluster (for more information on running Kubernetes with cloud providers, refer to the Kubernetes documentation.</p>"},{"location":"cloud-providers/#external-cloud-providers","title":"External Cloud Providers","text":""},{"location":"cloud-providers/#enable-cloud-provider-support-in-kubelet","title":"Enable cloud provider support in kubelet","text":"<p>Even when all components are built with providerless mode, you must be able to enable cloud provider mode for kubelet. To do this, run the workers with <code>--enable-cloud-provider=true</code>.</p>"},{"location":"cloud-providers/#deploy-the-cloud-provider","title":"Deploy the cloud provider","text":"<p>The easiest way to deploy cloud provider controllers is on the k0s cluster.</p> <p>Use the built-in manifest deployer built into k0s to deploy your cloud provider as a k0s-managed stack. Next, just drop all required manifests into the <code>/var/lib/k0s/manifests/aws/</code> directory, and k0s will handle the deployment.</p> <p>Note: The prerequisites for the various cloud providers can vary (for example, several require that configuration files be present on all of the nodes). Refer to your chosen cloud provider's documentation as necessary.</p>"},{"location":"cloud-providers/#k0s-cloud-provider","title":"k0s Cloud Provider","text":"<p>Alternatively, k0s provides its own lightweight cloud provider that can be used to statically assign <code>ExternalIP</code> values to worker nodes via Kubernetes annotations.  This is beneficial for those who need to expose worker nodes externally via static IP assignments.</p> <p>To enable this functionality, add the parameter <code>--enable-k0s-cloud-provider=true</code> to all controllers, and <code>--enable-cloud-provider=true</code> to all workers.</p> <p>Adding a static IP address to a node using <code>kubectl</code>:</p> <pre><code>kubectl annotate \\\n    node &lt;node&gt; \\\n    k0sproject.io/node-ip-external=&lt;external IP&gt;\n</code></pre> <p>Both IPv4 and IPv6 addresses are supported.</p>"},{"location":"cloud-providers/#defaults","title":"Defaults","text":"<p>The default node refresh interval is <code>2m</code>, which can be overridden using the <code>--k0s-cloud-provider-update-frequency=&lt;duration&gt;</code> parameter when launching the controller(s).</p> <p>The default port that the cloud provider binds to can be overridden using the <code>--k0s-cloud-provider-port=&lt;int&gt;</code> parameter when launching the controller(s).</p>"},{"location":"configuration-validation/","title":"Configuration validation","text":"<p>k0s command-line interface has the ability to validate config syntax:</p> <pre><code>k0s validate config --config path/to/config/file\n</code></pre> <p><code>validate config</code> sub-command can validate the following:</p> <ol> <li>YAML formatting</li> <li>SAN addresses</li> <li>Network providers</li> <li>Worker profiles</li> </ol>"},{"location":"configuration/","title":"Configuration options","text":""},{"location":"configuration/#using-a-configuration-file","title":"Using a configuration file","text":"<p>k0s can be installed without a config file. In that case the default configuration will be used. You can, though, create and run your own non-default configuration (used by the k0s controller nodes).</p> <ol> <li> <p>Generate a yaml config file that uses the default settings.</p> <pre><code>k0s default-config &gt; k0s.yaml\n</code></pre> </li> <li> <p>Modify the new yaml config file according to your needs, refer to Configuration file reference below.</p> </li> <li> <p>Install k0s with your new config file.</p> <pre><code>sudo k0s install controller -c /path/to/your/config/file\n</code></pre> </li> <li> <p>If you need to modify your existing configuration later on, you can change your config file also when k0s is running, but remember to restart k0s to apply your configuration changes.</p> <pre><code>sudo k0s stop\nsudo k0s start\n</code></pre> </li> </ol>"},{"location":"configuration/#configuration-file-reference","title":"Configuration file reference","text":"<p>CAUTION: As many of the available options affect items deep in the stack, you should fully understand the correlation between the configuration file components and your specific environment before making any changes.</p> <p>A YAML config file follows, with defaults as generated by the <code>k0s default-config</code> command:</p> <pre><code>apiVersion: k0s.k0sproject.io/v1beta1\nkind: Cluster\nmetadata:\nname: k0s\nspec:\napi:\naddress: 192.168.68.104\nport: 6443\nk0sApiPort: 9443\nexternalAddress: my-lb-address.example.com\nsans:\n- 192.168.68.104\nstorage:\ntype: etcd\netcd:\npeerAddress: 192.168.68.104\nnetwork:\npodCIDR: 10.244.0.0/16\nserviceCIDR: 10.96.0.0/12\nprovider: kuberouter\ncalico: null\nkuberouter:\nmtu: 0\npeerRouterIPs: \"\"\npeerRouterASNs: \"\"\nautoMTU: true\npodSecurityPolicy:\ndefaultPolicy: 00-k0s-privileged\ntelemetry:\nenabled: true\ninstallConfig:\nusers:\netcdUser: etcd\nkineUser: kube-apiserver\nkonnectivityUser: konnectivity-server\nkubeAPIserverUser: kube-apiserver\nkubeSchedulerUser: kube-scheduler\nimages:\nkonnectivity:\nimage: us.gcr.io/k8s-artifacts-prod/kas-network-proxy/proxy-agent\nversion: v0.0.25\nmetricsserver:\nimage: gcr.io/k8s-staging-metrics-server/metrics-server\nversion: v0.3.7\nkubeproxy:\nimage: k8s.gcr.io/kube-proxy\nversion: v1.21.7\ncoredns:\nimage: docker.io/coredns/coredns\nversion: 1.7.0\ncalico:\ncni:\nimage: docker.io/calico/cni\nversion: v3.18.1\nnode:\nimage: docker.io/calico/node\nversion: v3.18.1\nkubecontrollers:\nimage: docker.io/calico/kube-controllers\nversion: v3.18.1\nkuberouter:\ncni:\nimage: docker.io/cloudnativelabs/kube-router\nversion: v1.2.1\ncniInstaller:\nimage: quay.io/k0sproject/cni-node\nversion: 0.1.0\ndefault_pull_policy: IfNotPresent\nkonnectivity:\nagentPort: 8132\nadminPort: 8133\n</code></pre>"},{"location":"configuration/#spec-key-detail","title":"<code>spec</code> Key Detail","text":""},{"location":"configuration/#specapi","title":"<code>spec.api</code>","text":"Element Description <code>externalAddress</code> The loadbalancer address (for k0s controllers running behind a loadbalancer). Configures all cluster components to connect to this address and also configures this address for use  when joining new nodes to the cluster. <code>address</code> Local address on wihich to bind an API. Also serves as one of the addresses pushed on the k0s create service certificate on the API. Defaults to first non-local address found on the node. <code>sans</code> List of additional addresses to push to API servers serving the certificate. <code>extraArgs</code> Map of key-values (strings) for any extra arguments to pass down to Kubernetes api-server process. <code>port</code>\u00b9 Custom port for kube-api server to listen on (default: 6443) <code>k0sApiPort</code>\u00b9 Custom port for k0s-api server to listen on (default: 9443) <p>\u00b9 If <code>port</code> and <code>k0sApiPort</code> are used with the <code>externalAddress</code> element, the loadbalancer serving at <code>externalAddress</code> must listen on the same ports.</p>"},{"location":"configuration/#specstorage","title":"<code>spec.storage</code>","text":"Element Description <code>type</code> Type of the data store (valid values:<code>etcd</code> or <code>kine</code>). Note: Type <code>etcd</code> will cause k0s to create and manage an elastic etcd cluster within the controller nodes. <code>etcd.peerAddress</code> Node address used for etcd cluster peering. <code>kine.dataSource</code> kine datasource URL."},{"location":"configuration/#specnetwork","title":"<code>spec.network</code>","text":"Element Description <code>provider</code> Network provider (valid values: <code>calico</code>, <code>kuberouter</code>, or <code>custom</code>). For <code>custom</code>, you can push any network provider (default: <code>kuberouter</code>). Be aware that it is your responsibility to configure all of the CNI-related setups, including the CNI provider itself and all necessary host levels setups (for example, CNI binaries). Note: Once you initialize the cluster with a network provider the only way to change providers is through a full cluster redeployment. <code>podCIDR</code> Pod network CIDR to use in the cluster. <code>serviceCIDR</code> Network CIDR to use for cluster VIP services."},{"location":"configuration/#specnetworkcalico","title":"<code>spec.network.calico</code>","text":"Element Description <code>mode</code> <code>vxlan</code> (default) or <code>ipip</code> <code>vxlanPort</code> The UDP port for VXLAN (default: <code>4789</code>). <code>vxlanVNI</code> The virtual network ID for VXLAN (default: <code>4096</code>). <code>mtu</code> MTU for overlay network (default: <code>0</code>, which causes Calico to detect optimal MTU during bootstrap). <code>wireguard</code> Enable wireguard-based encryption (default: <code>false</code>). Your host system must be wireguard ready (refer to the Calico documentation for details). <code>flexVolumeDriverPath</code> The host path for Calicos flex-volume-driver(default: <code>/usr/libexec/k0s/kubelet-plugins/volume/exec/nodeagent~uds</code>). Change this path only if the default path is unwriteable (refer to Project Calico Issue #2712 for details). Ideally, you will pair this option with a custom <code>volumePluginDir</code> in the profile you use for your worker nodes. <code>ipAutodetectionMethod</code> Use to force Calico to pick up the interface for pod network inter-node routing (default: <code>\"\"</code>, meaning not set, so that Calico will instead use its defaults). For more information, refer to the Calico documentation."},{"location":"configuration/#specnetworkkuberouter","title":"<code>spec.network.kuberouter</code>","text":"Element Description <code>autoMTU</code> Autodetection of used MTU (default: <code>true</code>). <code>mtu</code> Override MTU setting, if <code>autoMTU</code> must be set to <code>false</code>). <code>peerRouterIPs</code> Comma-separated list of global peer addresses. <code>peerRouterASNs</code> Comma-separated list of global peer ASNs. <p>Note: Kube-router allows many networking aspects to be configured per node, service, and pod (for more information, refer to the Kube-router user guide).</p>"},{"location":"configuration/#specpodsecuritypolicy","title":"<code>spec.podSecurityPolicy</code>","text":"<p>Use the <code>spec.podSecurityPolicy</code> key to configure the default PSP.</p> <p>k0s creates two PSPs out-of-the-box:</p> PSP Description <code>00-k0s-privileged</code> Default; no restrictions; used also for Kubernetes/k0s level system pods. <code>99-k0s-restricted</code> Does not allow any host namespaces or root users, nor any bind mounts from the host <p>Note: Users can create supplemental PSPs and bind them to users / access accounts as necessary.</p>"},{"location":"configuration/#speccontrollermanager","title":"<code>spec.controllerManager</code>","text":"Element Description <code>extraArgs</code> Map of key-values (strings) for any extra arguments you want to pass down to the Kubernetes controller manager process."},{"location":"configuration/#specscheduler","title":"<code>spec.scheduler</code>","text":"Element Description <code>extraArgs</code> Map of key-values (strings) for any extra arguments you want to pass down to Kubernetes scheduler process."},{"location":"configuration/#specworkerprofiles","title":"<code>spec.workerProfiles</code>","text":"<p>Array of <code>spec.workerProfiles.workerProfile</code>. Each element has following properties:</p> Property Description <code>name</code> String; name to use as profile selector for the worker process <code>values</code> Mapping object <p>For each profile, the control plane creates a separate ConfigMap with <code>kubelet-config yaml</code>. Based on the <code>--profile</code> argument given to the <code>k0s worker</code>, the corresponding ConfigMap is used to extract the <code>kubelet-config.yaml</code> file. <code>values</code> are recursively merged with default <code>kubelet-config.yaml</code></p> <p>Note that there are several fields that cannot be overridden:</p> <ul> <li><code>clusterDNS</code></li> <li><code>clusterDomain</code></li> <li><code>apiVersion</code></li> <li><code>kind</code></li> </ul>"},{"location":"configuration/#examples","title":"Examples","text":"<p>mapping:</p> <pre><code>spec:\nworkerProfiles:\n- name: custom-role\nvalues:\nkey: value\nmapping:\ninnerKey: innerValue\n</code></pre> <p>Custom volumePluginDir:</p> <pre><code>spec:\nworkerProfiles:\n- name: custom-role\nvalues:\nvolumePluginDir: /var/libexec/k0s/kubelet-plugins/volume/exec\n</code></pre>"},{"location":"configuration/#specimages","title":"<code>spec.images</code>","text":"<p>Nodes under the <code>images</code> key all have the same basic structure:</p> <pre><code>spec:\nimages:\ncoredns:\nimage: docker.io/coredns/coredns\nversion: 1.7.0\n</code></pre>"},{"location":"configuration/#available-keys","title":"Available keys","text":"<ul> <li><code>spec.images.konnectivity</code></li> <li><code>spec.images.metricsserver</code></li> <li><code>spec.images.kubeproxy</code></li> <li><code>spec.images.coredns</code></li> <li><code>spec.images.calico.cni</code></li> <li><code>spec.images.calico.flexvolume</code></li> <li><code>spec.images.calico.node</code></li> <li><code>spec.images.calico.kubecontrollers</code></li> <li><code>spec.images.kuberouter.cni</code></li> <li><code>spec.images.kuberouter.cniInstaller</code></li> <li><code>spec.images.repository</code>\u00b9</li> </ul> <p>\u00b9 If <code>spec.images.repository</code> is set and not empty, every image will be pulled from <code>images.repository</code></p> <p>If <code>spec.images.default_pull_policy</code> is set and not empty, it will be used as a pull policy for each bundled image.</p>"},{"location":"configuration/#example","title":"Example","text":"<pre><code>images:\nrepository: \"my.own.repo\"\nkonnectivity:\nimage: calico/kube-controllers\nversion: v3.16.2\nmetricsserver:\nimage: gcr.io/k8s-staging-metrics-server/metrics-server\nversion: v0.3.7\n</code></pre> <p>In the runtime the image names are calculated as <code>my.own.repo/calico/kube-controllers:v3.16.2</code> and <code>my.own.repo/k8s-staging-metrics-server/metrics-server</code>. This only affects the the imgages pull location, and thus omitting an image specification here will not disable component deployment.</p>"},{"location":"configuration/#specextensionshelm","title":"<code>spec.extensions.helm</code>","text":"<p><code>spec.extensions.helm</code> is the config file key in which you configure the list of Helm repositories and charts to deploy during cluster bootstrap (for more information, refer to Helm Charts).</p>"},{"location":"configuration/#speckonnectivity","title":"<code>spec.konnectivity</code>","text":"<p>The <code>spec.konnectivity</code> key is the config file key in which you configure Konnectivity-related settings.</p> <ul> <li><code>agentPort</code> agent port to listen on (default 8132)</li> <li><code>adminPort</code> admin port to listen on (default 8133)</li> </ul>"},{"location":"configuration/#spectelemetry","title":"<code>spec.telemetry</code>","text":"<p>To improve the end-user experience k0s is configured by defaul to collect telemetry data from clusters and send it to the k0s development team. To disable the telemetry function, change the <code>enabled</code> setting to <code>false</code>.</p> <p>The telemetry interval is ten minutes.</p> <pre><code>spec:\ntelemetry:\nenabled: true\n</code></pre>"},{"location":"conformance-testing/","title":"Kubernetes conformance testing for k0s","text":"<p>We run the conformance testing for the last RC build for a release. Follow the instructions as the conformance testing repository.</p> <p>In a nutshell, you need to:</p> <ul> <li>Setup k0s on some VMs/bare metal boxes</li> <li>Download, if you do not already have, sonobuoy tool</li> <li>Run the conformance tests with something like <code>sonobuoy run --mode=certified-conformance</code></li> <li>Wait for couple hours</li> <li>Collect results</li> </ul>"},{"location":"containerd_config/","title":"Containerd config","text":"<p>See runtime.</p>"},{"location":"custom-cri-runtime/","title":"Custom cri runtime","text":"<p>See runtime.</p>"},{"location":"dual-stack/","title":"Dual-stack Networking","text":"<p>Note: Dual stack networking setup requires that you configure Calico or a custom CNI as the CNI provider.</p> <p>Use the following <code>k0s.yaml</code> as a template to enable dual-stack networking. This configuration will set up bundled calico CNI, enable feature gates for the Kubernetes components, and set up <code>kubernetes-controller-manager</code>.</p> <pre><code>spec:\nnetwork:\npodCIDR: \"10.244.0.0/16\"\nserviceCIDR: \"10.96.0.0/12\"\ncalico:\nmode: \"bird\"\ndualStack:\nenabled: true\nIPv6podCIDR: \"fd00::/108\"\nIPv6serviceCIDR: \"fd01::/108\"\n</code></pre>"},{"location":"dual-stack/#cni-settings-calico","title":"CNI Settings: Calico","text":"<p>For cross-pod connectivity, use BIRD for the backend. Calico does not support tunneling for the IPv6, and thus VXLAN and IPIP backends do not work.</p> <p>Note: In any Calico mode other than cross-pod, the pods can only reach pods on the same node.</p>"},{"location":"dual-stack/#cni-settings-external-cni","title":"CNI Settings: External CNI","text":"<p>Although the <code>k0s.yaml</code> dualStack section enables all of the neccessary feature gates for the Kubernetes components, for use with an external CNI it must be set up to support IPv6.</p>"},{"location":"dual-stack/#additional-resources","title":"Additional Resources","text":"<ul> <li>https://kubernetes.io/docs/concepts/services-networking/dual-stack/</li> <li>https://kubernetes.io/docs/tasks/network/validate-dual-stack/</li> <li>https://www.projectcalico.org/dual-stack-operation-with-calico-on-kubernetes/</li> <li>https://docs.projectcalico.org/networking/ipv6</li> </ul>"},{"location":"experimental-windows/","title":"Run k0s worker nodes in Windows","text":"<p>IMPORTANT: Windows support for k0s is under active development and must be considered experimental.</p>"},{"location":"experimental-windows/#prerequisites","title":"Prerequisites","text":"<p>The cluster must be running at least one worker node and control plane on Linux. You ca use Windows to run additional worker nodes.</p>"},{"location":"experimental-windows/#build-k0sexe","title":"Build k0s.exe","text":"<p>Invoke the <code>make clean k0s.exe</code> command to create k0s.exe with staged kubelet.exe and kube-proxy.exe.</p> <p>Note: The k0s.exe supervises kubelet.exe and kube-proxy.exe.</p> <p>During the first run, the calico install script is creaated as <code>C:\\bootstrap.ps1</code>. This bootstrap script downloads the calico binaries, builds pause container and set ups vSwitch settings.</p>"},{"location":"experimental-windows/#run-k0s","title":"Run k0s","text":"<p>Install Mirantis Container Runtime on the Windows node(s), as it is required for the initial Calico set up).</p> <pre><code>k0s worker --cri-socket=docker:tcp://127.0.0.1:2375 --cidr-range=&lt;cidr_range&gt; --cluster-dns=&lt;clusterdns&gt; --api-server=&lt;k0s api&gt; &lt;token&gt;\n</code></pre> <p>You must initiate the Cluster control with the correct config.</p>"},{"location":"experimental-windows/#configuration","title":"Configuration","text":""},{"location":"experimental-windows/#strict-affinity","title":"Strict-affinity","text":"<p>You must enable strict affinity to run the windows node.</p> <p>If the <code>spec.network.calico.withWindowsNodes</code> field is set to <code>true</code> (it is set to <code>false</code> by default) the additional calico related manifest <code>/var/lib/k0s/manifests/calico/calico-IPAMConfig-ipamconfig.yaml</code> is created with the following values:</p> <pre><code>---\napiVersion: crd.projectcalico.org/v1\nkind: IPAMConfig\nmetadata:\nname: default\nspec:\nstrictAffinity: true\n</code></pre> <p>Alternately, you can manually execute calicoctl:</p> <pre><code>calicoctl ipam configure --strictaffinity=true\n</code></pre>"},{"location":"experimental-windows/#network-connectivity-in-aws","title":"Network connectivity in AWS","text":"<p>Disable the <code>Change Source/Dest. Check</code> option for the network interface attached to your EC2 instance. In AWS, the console option for the network interface is in the Actions menu.</p>"},{"location":"experimental-windows/#hacks","title":"Hacks","text":"<p>k0s offers the following CLI arguments in lieu of a formal means for passing cluster settings from controller plane to worker:</p> <ul> <li>cidr-range</li> <li>cluster-dns</li> <li>api-server</li> </ul>"},{"location":"experimental-windows/#useful-commands","title":"Useful commands","text":""},{"location":"experimental-windows/#run-pod-with-cmdexe-shell","title":"Run pod with cmd.exe shell","text":"<pre><code>kubectl run win --image=hello-world:nanoserver --command=true -i --attach=true -- cmd.exe\n</code></pre>"},{"location":"experimental-windows/#manifest-for-pod-with-iis-web-server","title":"Manifest for pod with IIS web-server","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: iis\nspec:\ncontainers:\n- name: iis\nimage: mcr.microsoft.com/windows/servercore/iis\nimagePullPolicy: IfNotPresent\n</code></pre>"},{"location":"extensions/","title":"Cluster extensions","text":"<p>k0s allows users to use extensions to extend cluster functionality.</p> <p>At the moment the only supported type of extensions is helm based charts.</p> <p>The default configuration has no extensions.</p>"},{"location":"extensions/#helm-based-extensions","title":"Helm based extensions","text":""},{"location":"extensions/#configuration-example","title":"Configuration example","text":"<pre><code>helm:\nrepositories:\n- name: stable\nurl: https://charts.helm.sh/stable\n- name: prometheus-community\nurl: https://prometheus-community.github.io/helm-charts\ncharts:\n- name: prometheus-stack\nchartname: prometheus-community/prometheus\nversion: \"11.16.8\"\nvalues: |\nstorageSpec:\nemptyDir:\nmedium: Memory\nnamespace: default\n</code></pre> <p>By using the configuration above, the cluster would:</p> <ul> <li>add stable and prometheus-community chart repositories</li> <li>install the <code>prometheus-community/prometheus</code> chart of the specified version to the <code>default</code> namespace.</li> </ul> <p>The chart installation is implemented by using CRD <code>helm.k0sproject.io/Chart</code>. For every given helm extension the cluster creates a Chart CRD instance. The cluster has a controller which monitors for the Chart CRDs, supporting the following operations:</p> <ul> <li>install</li> <li>upgrade</li> <li>delete</li> </ul> <p>For security reasons, the cluster operates only on Chart CRDs instantiated in the <code>kube-system</code> namespace, however, the target namespace could be any.</p>"},{"location":"extensions/#crd-definition","title":"CRD definition","text":"<pre><code>apiVersion: helm.k0sproject.io/v1beta1\nkind: Chart\nmetadata:\ncreationTimestamp: \"2020-11-10T14:17:53Z\"\ngeneration: 2\nlabels:\nk0s.k0sproject.io/stack: helm\nname: k0s-addon-chart-test-addon\nnamespace: kube-system\nresourceVersion: \"627\"\nselfLink: /apis/helm.k0sproject.io/v1beta1/namespaces/kube-system/charts/k0s-addon-chart-test-addon\nuid: ebe59ed4-1ff8-4d41-8e33-005b183651ed\nspec:\nchartName: prometheus-community/prometheus\nnamespace: default\nvalues: |\nstorageSpec:\nemptyDir:\nmedium: Memory\nversion: 11.16.8\nstatus:\nappVersion: 2.21.0\nnamespace: default\nreleaseName: prometheus-1605017878\nrevision: 2\nupdated: 2020-11-10 14:18:08.235656 +0000 UTC m=+41.871656901\nversion: 11.16.8\n</code></pre> <p>The <code>Chart.spec</code> defines the chart information.</p> <p>The <code>Chart.status</code> keeps the information about the last operation performed by the operator.</p>"},{"location":"helm-charts/","title":"Helm Charts","text":"<p>Defining your extensions as Helm charts is one of two methods you can use to run k0s with your preferred extensions (the other being through the use of Manifest Deployer.</p> <p>k0s supports two methods for deploying applications using Helm charts:</p> <ul> <li>Use Helm command in runtime to install applications. Refer to the Helm Quickstart Guide for more information.</li> <li>Insert Helm charts directly into the k0s configuration file, <code>k0s.yaml</code>. This method does not require a separate install of <code>helm</code> tool and the charts automatically deploy at the k0s bootstrap phase.</li> </ul>"},{"location":"helm-charts/#helm-charts-in-k0s-configuration","title":"Helm charts in k0s configuration","text":"<p>Adding Helm charts into the k0s configuration file gives you a declarative way in which to configure the cluster. k0s controller manages the setup of Helm charts that are defined as extensions in the k0s configuration file.</p>"},{"location":"helm-charts/#example","title":"Example","text":"<p>In the example, Prometheus is configured from \"stable\" Helms chart repository. Add the following to <code>k0s.yaml</code> and restart k0s, after which Prometheus should start automatically with k0s.</p> <pre><code>spec:\nextensions:\nhelm:\nrepositories:\n- name: stable\nurl: https://charts.helm.sh/stable\n- name: prometheus-community\nurl: https://prometheus-community.github.io/helm-charts\ncharts:\n- name: prometheus-stack\nchartname: prometheus-community/prometheus\nversion: \"11.16.8\"\nvalues: |\nstorageSpec:\nemptyDir:\nmedium: Memory\nnamespace: default\n</code></pre> <p>Example extensions that you can use with Helm charts include:</p> <ul> <li>Ingress controllers: Nginx ingress, Traefix ingress (refer to the k0s documentation for Installing the Traefik Ingress Controller)</li> <li>Volume storage providers: OpenEBS, Rook, Longhorn</li> <li>Monitoring: Prometheus, Grafana</li> </ul>"},{"location":"high-availability/","title":"Control Plane High Availability","text":"<p>You can create high availability for the control plane by distributing the control plane across multiple nodes and installing a load balancer on top. Etcd can be colocated with the controller nodes (default in k0s) to achieve highly available datastore at the same time.</p> <p></p>"},{"location":"high-availability/#network-considerations","title":"Network considerations","text":"<p>You should plan to allocate the control plane nodes into different zones. This will avoid failures in case one zone fails.</p> <p>For etcd high availability it's recommended to configure 3 or 5 controller nodes. For more information, refer to the etcd documentation.</p>"},{"location":"high-availability/#load-balancer","title":"Load Balancer","text":"<p>Control plane high availability requires a tcp load balancer, which acts as a single point of contact to access the controllers. The load balancer needs to allow and route traffic to each controller through the following ports:</p> <ul> <li>6443 (for Kubernetes API)</li> <li>8132 (for Konnectivity agent)</li> <li>8133 (for Konnectivity server)</li> <li>9443 (for controller join API)</li> </ul> <p>The load balancer can be implemented in many different ways and k0s doesn't have any additional requirements. You can use for example HAProxy, NGINX or your cloud provider's load balancer.</p>"},{"location":"high-availability/#example-configuration-haproxy","title":"Example configuration: HAProxy","text":"<p>Change the default mode to tcp under the 'defaults' section of haproxy.cfg.</p> <p>Add the following lines to the end of the haproxy.cfg:</p> <pre><code>frontend kubeAPI\n    bind :6443\n    default_backend back\nfrontend konnectivityAgent\n    bind :8132\n    default_backend back\nfrontend konnectivityServer\n    bind :8133\n    default_backend back\nfrontend controllerJoinAPI\n    bind :9443\n    default_backend back\n\nbackend back\n    server k0s-controller1 &lt;ip-address1&gt;\n    server k0s-controller2 &lt;ip-address2&gt;\n    server k0s-controller3 &lt;ip-address3&gt;\n</code></pre> <p>Restart HAProxy to apply the configuration changes.</p>"},{"location":"high-availability/#k0s-configuration","title":"k0s configuration","text":"<p>The load balancer address must be configured to k0s either by using <code>k0s.yaml</code> or by using k0sctl to automatically deploy all controllers with the same configuration:</p>"},{"location":"high-availability/#configuration-using-k0syaml-for-each-controller","title":"Configuration using k0s.yaml (for each controller)","text":"<p>Note to update your load balancer's public ip address into two places.</p> <pre><code>spec:\napi:\nexternalAddress: &lt;load balancer public ip address&gt;\nsans:\n- &lt;load balancer public ip address&gt;\n</code></pre>"},{"location":"high-availability/#configuration-using-k0sctlyaml-for-k0sctl","title":"Configuration using k0sctl.yaml (for k0sctl)","text":"<p>Add the following lines to the end of the k0sctl.yaml. Note to update your load balancer's public ip address into two places.</p> <pre><code>  k0s:\nconfig:\nspec:\napi:\nexternalAddress: &lt;load balancer public ip address&gt;\nsans:\n- &lt;load balancer public ip address&gt;\n</code></pre> <p>For greater detail about k0s configuration, refer to the Full configuration file reference.</p>"},{"location":"install/","title":"Quick Start Guide","text":"<p>On completion of the Quick Start you will have a full Kubernetes cluster with a single node that includes both the controller and the worker. Such a setup is ideal for environments that do not require high-availability and multiple nodes.</p>"},{"location":"install/#prerequisites","title":"Prerequisites","text":"<p>Note: Before proceeding, make sure to review the System Requirements.</p> <p>Though the Quick Start material is written for Debian/Ubuntu, you can use it for any Linux distro that is running either a Systemd or OpenRC init system.</p>"},{"location":"install/#install-k0s","title":"Install k0s","text":"<ol> <li> <p>Download k0s</p> <p>Run the k0s download script to download the latest stable version of k0s and make it executable from /usr/bin/k0s.</p> <pre><code>curl -sSLf https://get.k0s.sh | sudo sh\n</code></pre> </li> <li> <p>Install k0s as a service</p> <p>The <code>k0s install</code> sub-command installs k0s as a system service on the local host that is running one of the supported init systems: Systemd or OpenRC. You can execute the install for workers, controllers or single node (controller+worker) instances.</p> <p>Run the following command to install a single node k0s that includes the controller and worker functions with the default configuration:</p> <pre><code>sudo k0s install controller --single\n</code></pre> <p>The <code>k0s install controller</code> sub-command accepts the same flags and parameters as the <code>k0s controller</code>. Refer to manual install for a custom config file example.</p> </li> <li> <p>Start k0s as a service</p> <p>To start the k0s service, run:</p> <pre><code>sudo k0s start\n</code></pre> <p>The k0s service will start automatically after the node restart.</p> <p>A minute or two typically passes before the node is ready to deploy applications.</p> </li> <li> <p>Check service, logs and k0s status</p> <p>To get general information about your k0s instance's status, run:</p> <pre><code>$ sudo k0s status\nVersion: v0.11.0\nProcess ID: 436\nParent Process ID: 1\nRole: controller+worker\nInit System: linux-systemd\n</code></pre> </li> <li> <p>Access your cluster using kubectl</p> <p>Note: k0s includes the Kubernetes command-line tool kubectl.</p> <p>Use kubectl to deploy your application or to check your node status:</p> <pre><code>$ sudo k0s kubectl get nodes\nNAME   STATUS   ROLES    AGE    VERSION\nk0s    Ready    &lt;none&gt;   4m6s   v1.21.7-k0s1\n</code></pre> </li> </ol>"},{"location":"install/#uninstall-k0s","title":"Uninstall k0s","text":"<p>The removal of k0s is a two-step process.</p> <ol> <li> <p>Stop the service.</p> <pre><code>sudo k0s stop\n</code></pre> </li> <li> <p>Execute the <code>k0s reset</code> command.</p> <p>The <code>k0s reset</code> command cleans up the installed system service, data directories, containers, mounts and network namespaces.</p> <pre><code>sudo k0s reset\n</code></pre> </li> <li> <p>Reboot the system.</p> <p>A few small k0s fragments persist even after the reset (for example, iptables). As such, you should initiate a reboot after the running of the <code>k0s reset</code> command.</p> </li> </ol>"},{"location":"install/#next-steps","title":"Next Steps","text":"<ul> <li>Install using k0sctl: Deploy multi-node clusters using just one command</li> <li>Manual Install: (Advanced) Manually deploy multi-node clusters</li> <li>Control plane configuration options: Networking and datastore configuration</li> <li>Worker node configuration options: Node labels and kubelet arguments</li> <li>Support for cloud providers: Load balancer or storage configuration</li> <li>Installing the Traefik Ingress Controller:   Ingress deployment information</li> <li>Airgap/Offline installation: Airgap deployment</li> </ul>"},{"location":"k0s-in-docker/","title":"Run k0s in Docker","text":"<p>You can create a k0s cluster on top of docker. In such a scenario, by default, both controller and worker nodes are run in the same container to provide an easy local testing \"cluster\".</p>"},{"location":"k0s-in-docker/#prerequisites","title":"Prerequisites","text":"<p>You will require a Docker environment running on a Mac, Windows, or Linux system.</p>"},{"location":"k0s-in-docker/#container-images","title":"Container images","text":"<p>The k0s containers are published both on Docker Hub and GitHub. For reasons of simplicity, the examples given here use Docker Hub (GitHub requires a separate authentication that is not covered). Alternative links include:</p> <ul> <li>docker.io/k0sproject/k0s:latest</li> <li>docker.pkg.github.com/k0sproject/k0s/k0s:\"version\"</li> </ul>"},{"location":"k0s-in-docker/#start-k0s","title":"Start k0s","text":""},{"location":"k0s-in-docker/#1-initiate-k0s","title":"1. Initiate k0s","text":"<p>You can run your own k0s in Docker:</p> <pre><code>docker run -d --name k0s --hostname k0s --privileged -v /var/lib/k0s -p 6443:6443 docker.io/k0sproject/k0s:latest\n</code></pre>"},{"location":"k0s-in-docker/#2-optional-create-additional-workers","title":"2. (Optional) Create additional workers","text":"<p>You can attach multiple workers nodes into the cluster to then distribute your application containers to separate workers.</p> <p>For each required worker:</p> <ol> <li> <p>Acquire a join token for the worker:</p> <pre><code>token=$(docker exec -t -i k0s k0s token create --role=worker)\n</code></pre> </li> <li> <p>Run the container to create and join the new worker:</p> <pre><code>docker run -d --name k0s-worker1 --hostname k0s-worker1 --privileged -v /var/lib/k0s docker.io/k0sproject/k0s:latest k0s worker $token\n</code></pre> </li> </ol>"},{"location":"k0s-in-docker/#3-access-your-cluster","title":"3. Access your cluster","text":"<p>Access your cluster using kubectl:</p> <pre><code>docker exec k0s kubectl get nodes\n</code></pre> <p>Alternatively, grab the kubeconfig file with <code>docker exec k0s cat /var/lib/k0s/pki/admin.conf</code> and paste it into Lens.</p>"},{"location":"k0s-in-docker/#use-docker-compose-alternative","title":"Use Docker Compose (alternative)","text":"<p>As an alternative you can run k0s using Docker Compose:</p> <pre><code>version: \"3.9\"\nservices:\nk0s:\ncontainer_name: k0s\nimage: docker.io/k0sproject/k0s:latest\ncommand: k0s controller --config=/etc/k0s/config.yaml --enable-worker\nhostname: k0s\nprivileged: true\nvolumes:\n- \"/var/lib/k0s\"\ntmpfs:\n- /run\n- /var/run\nports:\n- \"6443:6443\"\nnetwork_mode: \"bridge\"\nenvironment:\nK0S_CONFIG: |-\napiVersion: k0s.k0sproject.io/v1beta1\nkind: Cluster\nmetadata:\nname: k0s\n# Any additional configuration goes here ...\n</code></pre>"},{"location":"k0s-in-docker/#known-limitations","title":"Known limitations","text":""},{"location":"k0s-in-docker/#no-custom-docker-networks","title":"No custom Docker networks","text":"<p>Currently, k0s nodes cannot be run if the containers are configured to use custom networks (for example, with <code>--net my-net</code>). This is because Docker sets up a custom DNS service within the network which creates issues with CoreDNS. No completely reliable workaounds are available, however no issues should arise from running k0s cluster(s) on a bridge network.</p>"},{"location":"k0s-in-docker/#next-steps","title":"Next Steps","text":"<ul> <li>Install using k0sctl: Deploy multi-node clusters using just one command</li> <li>Control plane configuration options: Networking and datastore configuration</li> <li>Worker node configuration options: Node labels and kubelet arguments</li> <li>Support for cloud providers: Load balancer or storage configuration</li> <li>Installing the Traefik Ingress Controller: Ingress deployment information</li> </ul>"},{"location":"k0s-multi-node/","title":"Manual Install (Advanced)","text":"<p>You can manually set up k0s nodes by creating a multi-node cluster that is locally managed on each node. This involves several steps, to first install each node separately, and to then connect the node together using access tokens.</p>"},{"location":"k0s-multi-node/#prerequisites","title":"Prerequisites","text":"<p>Note: Before proceeding, make sure to review the System Requirements.</p> <p>Though the Manual Install material is written for Debian/Ubuntu, you can use it for any Linux distro that is running either a Systemd or OpenRC init system.</p> <p>You can speed up the use of the <code>k0s</code> command by enabling shell completion.</p>"},{"location":"k0s-multi-node/#install-k0s","title":"Install k0s","text":""},{"location":"k0s-multi-node/#1-download-k0s","title":"1. Download k0s","text":"<p>Run the k0s download script to download the latest stable version of k0s and make it executable from /usr/bin/k0s.</p> <pre><code>curl -sSLf https://get.k0s.sh | sudo sh\n</code></pre> <p>The download script accepts the following environment variables:</p> Variable Purpose `K0S_VERSION=v1.21.7+k0s.0 Select the version of k0s to be installed <code>DEBUG=true</code> Output commands and their arguments at execution. <p>Note: If you require environment variables and use sudo, you can do:</p> <pre><code>curl -sSLf https://get.k0s.sh | sudo K0S_VERSION=v1.21.7+k0s.0 sh\n</code></pre>"},{"location":"k0s-multi-node/#2-bootstrap-a-controller-node","title":"2. Bootstrap a controller node","text":"<p>Create a configuration file:</p> <pre><code>k0s default-config &gt; k0s.yaml\n</code></pre> <p>Note: For information on settings modification, refer to the configuration documentation.</p> <pre><code>sudo k0s install controller -c k0s.yaml\n</code></pre> <pre><code>sudo k0s start\n</code></pre> <p>k0s process acts as a \"supervisor\" for all of the control plane components. In moments the control plane will be up and running.</p>"},{"location":"k0s-multi-node/#3-create-a-join-token","title":"3. Create a join token","text":"<p>You need a token to join workers to the cluster. The token embeds information that enables mutual trust between the worker and controller(s) and which allows the node to join the cluster as worker.</p> <p>To get a token, run the following command on one of the existing controller nodes:</p> <pre><code>k0s token create --role=worker\n</code></pre> <p>The resulting output is a long token string, which you can use to add a worker to the cluster.</p> <p>For enhanced security, run the following command to set an expiration time for the token:</p> <pre><code>k0s token create --role=worker --expiry=100h &gt; token-file\n</code></pre>"},{"location":"k0s-multi-node/#4-add-workers-to-the-cluster","title":"4. Add workers to the cluster","text":"<p>To join the worker, run k0s in the worker mode with the join token you created:</p> <pre><code>sudo k0s install worker --token-file /path/to/token/file\n</code></pre> <pre><code>sudo k0s start\n</code></pre>"},{"location":"k0s-multi-node/#about-tokens","title":"About tokens","text":"<p>The join tokens are base64-encoded kubeconfigs for several reasons:</p> <ul> <li>Well-defined structure</li> <li>Capable of direct use as bootstrap auth configs for kubelet</li> <li>Embedding of CA info for mutual trust</li> </ul> <p>The bearer token embedded in the kubeconfig is a bootstrap token. For controller join tokens and worker join tokens k0s uses different usage attributes to ensure that k0s can validate the token role on the controller side.</p>"},{"location":"k0s-multi-node/#5-add-controllers-to-the-cluster","title":"5. Add controllers to the cluster","text":"<p>Note: Either etcd or an external data store (MySQL or Postgres) via kine must be in use to add new controller nodes to the cluster. Pay strict attention to the high availability configuration and make sure the configuration is identical for all controller nodes.</p> <p>To create a join token for the new controller, run the following command on an existing controller:</p> <pre><code>k0s token create --role=controller --expiry=1h &gt; token-file\n</code></pre> <p>On the new controller, run:</p> <pre><code>sudo k0s install controller --token-file /path/to/token/file\n</code></pre> <pre><code>k0s start\n</code></pre>"},{"location":"k0s-multi-node/#6-check-k0s-status","title":"6. Check k0s status","text":"<p>To get general information about your k0s instance's status:</p> <pre><code>$ sudo k0s status\nVersion: v1.21.7+k0s.0\nProcess ID: 2769\nParent Process ID: 1\nRole: controller\nInit System: linux-systemd\nService file: /etc/systemd/system/k0scontroller.service\n</code></pre>"},{"location":"k0s-multi-node/#7-access-your-cluster","title":"7. Access your cluster","text":"<p>Use the Kubernetes 'kubectl' command-line tool that comes with k0s binary to deploy your application or check your node status:</p> <pre><code>$ sudo k0s kubectl get nodes\nNAME   STATUS   ROLES    AGE    VERSION\nk0s    Ready    &lt;none&gt;   4m6s   v1.21.7-k0s1\n</code></pre> <p>You can also access your cluster easily with Lens, simply by copying the kubeconfig and pasting it to Lens:</p> <pre><code>sudo cat /var/lib/k0s/pki/admin.conf\n</code></pre> <p>Note: To access the cluster from an external network you must replace <code>localhost</code> in the kubeconfig with the host ip address for your controller.</p>"},{"location":"k0s-multi-node/#next-steps","title":"Next Steps","text":"<ul> <li>Install using k0sctl: Deploy multi-node clusters using just one command</li> <li>Control plane configuration options: Networking and datastore configuration</li> <li>Worker node configuration options: Node labels and kubelet arguments</li> <li>Support for cloud providers: Load balancer or storage configuration</li> <li>Installing the Traefik Ingress Controller: Ingress deployment information</li> </ul>"},{"location":"k0s-single-node/","title":"K0s single node","text":"<p>See the Quick Start Guide.</p>"},{"location":"k0sctl-install/","title":"Install using k0sctl","text":"<p>k0sctl is a command-line tool for bootstrapping and managing k0s clusters. k0sctl connects to the provided hosts using SSH and gathers information on the hosts, with which it forms a cluster by configuring the hosts, deploying k0s, and then connecting the k0s nodes together.</p> <p></p> <p>With k0sctl, you can create multi-node clusters in a manner that is automatic and easily repeatable. This method is recommended for production cluster installation.</p> <p>Note: The k0sctl install method is necessary for automatic upgrade.</p>"},{"location":"k0sctl-install/#prerequisites","title":"Prerequisites","text":"<p>You can execute k0sctl on any system that supports the Go language. Pre-compiled k0sctl binaries are availble on the k0sctl releases page).</p> <p>Note: For target host prerequisites information, refer to the k0s System Requirements.</p>"},{"location":"k0sctl-install/#install-k0s","title":"Install k0s","text":""},{"location":"k0sctl-install/#1-install-k0sctl-tool","title":"1. Install k0sctl tool","text":"<p>k0sctl is a single binary, the instructions for downloading and installing of which are available in the k0sctl github repository.</p>"},{"location":"k0sctl-install/#2-configure-the-cluster","title":"2. Configure the cluster","text":"<ol> <li> <p>Run the following command to create a k0sctl configuration file:</p> <pre><code>k0sctl init &gt; k0sctl.yaml\n</code></pre> <p>This action creates a <code>k0sctl.yaml</code> file in the current directory:</p> <pre><code>apiVersion: k0sctl.k0sproject.io/v1beta1\nkind: Cluster\nmetadata:\nname: k0s-cluster\nspec:\nhosts:\n- role: controller\nssh:\naddress: 10.0.0.1 # replace with the controller's IP address\nuser: root\nkeyPath: ~/.ssh/id_rsa\n- role: worker\nssh:\naddress: 10.0.0.2 # replace with the worker's IP address\nuser: root\nkeyPath: ~/.ssh/id_rsa\n</code></pre> </li> <li> <p>Provide each host with a valid IP address that is reachable by k0ctl, and the connection details for an SSH connection.</p> </li> </ol> <p>Note: Refer to the k0sctl documentation for k0sctl configuration specifications.</p>"},{"location":"k0sctl-install/#3-deploy-the-cluster","title":"3. Deploy the cluster","text":"<p>Run <code>k0sctl apply</code> to perform the cluster deployment:</p> <pre><code>$ k0sctl apply --config k0sctl.yaml\n\n\u2800\u28ff\u28ff\u2847\u2800\u2800\u2880\u28f4\u28fe\u28ff\u281f\u2801\u28b8\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u287f\u281b\u2801\u2800\u28b8\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u2800\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\n\u2800\u28ff\u28ff\u2847\u28e0\u28f6\u28ff\u287f\u280b\u2800\u2800\u2800\u28b8\u28ff\u2847\u2800\u2800\u2800\u28e0\u2800\u2800\u2880\u28e0\u2846\u28b8\u28ff\u28ff\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2588\u2588\u2588          \u2588\u2588\u2588    \u2588\u2588\u2588\n\u2800\u28ff\u28ff\u28ff\u28ff\u28df\u280b\u2800\u2800\u2800\u2800\u2800\u28b8\u28ff\u2847\u2800\u28b0\u28fe\u28ff\u2800\u2800\u28ff\u28ff\u2847\u28b8\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u2800\u2588\u2588\u2588          \u2588\u2588\u2588    \u2588\u2588\u2588\n\u2800\u28ff\u28ff\u284f\u283b\u28ff\u28f7\u28e4\u2840\u2800\u2800\u2800\u2838\u281b\u2801\u2800\u2838\u280b\u2801\u2800\u2800\u28ff\u28ff\u2847\u2808\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u28b9\u28ff\u28ff\u2800\u2588\u2588\u2588          \u2588\u2588\u2588    \u2588\u2588\u2588\n\u2800\u28ff\u28ff\u2847\u2800\u2800\u2819\u28bf\u28ff\u28e6\u28c0\u2800\u2800\u2800\u28e0\u28f6\u28f6\u28f6\u28f6\u28f6\u28f6\u28ff\u28ff\u2847\u28b0\u28f6\u28f6\u28f6\u28f6\u28f6\u28f6\u28f6\u28f6\u28fe\u28ff\u28ff\u2800\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\u2588    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n\nINFO k0sctl 0.0.0 Copyright 2021, Mirantis Inc.\nINFO Anonymized telemetry will be sent to Mirantis.\nINFO By continuing to use k0sctl you agree to these terms:\nINFO https://k0sproject.io/licenses/eula\nINFO ==&gt; Running phase: Connect to hosts\nINFO [ssh] 10.0.0.1:22: connected\nINFO [ssh] 10.0.0.2:22: connected\nINFO ==&gt; Running phase: Detect host operating systems\nINFO [ssh] 10.0.0.1:22: is running Ubuntu 20.10\nINFO [ssh] 10.0.0.2:22: is running Ubuntu 20.10\nINFO ==&gt; Running phase: Prepare hosts\nINFO [ssh] 10.0.0.1:22: installing kubectl\nINFO ==&gt; Running phase: Gather host facts\nINFO [ssh] 10.0.0.1:22: discovered 10.12.18.133 as private address\nINFO ==&gt; Running phase: Validate hosts\nINFO ==&gt; Running phase: Gather k0s facts\nINFO ==&gt; Running phase: Download K0s on the hosts\nINFO [ssh] 10.0.0.2:22: downloading k0s 0.11.0\nINFO [ssh] 10.0.0.1:22: downloading k0s 0.11.0\nINFO ==&gt; Running phase: Configure K0s\nWARN [ssh] 10.0.0.1:22: generating default configuration\nINFO [ssh] 10.0.0.1:22: validating configuration\nINFO [ssh] 10.0.0.1:22: configuration was changed\nINFO ==&gt; Running phase: Initialize K0s Cluster\nINFO [ssh] 10.0.0.1:22: installing k0s controller\nINFO [ssh] 10.0.0.1:22: waiting for the k0s service to start\nINFO [ssh] 10.0.0.1:22: waiting for kubernetes api to respond\nINFO ==&gt; Running phase: Install workers\nINFO [ssh] 10.0.0.1:22: generating token\nINFO [ssh] 10.0.0.2:22: writing join token\nINFO [ssh] 10.0.0.2:22: installing k0s worker\nINFO [ssh] 10.0.0.2:22: starting service\nINFO [ssh] 10.0.0.2:22: waiting for node to become ready\nINFO ==&gt; Running phase: Disconnect from hosts\nINFO ==&gt; Finished in 2m2s\nINFO k0s cluster version 0.11.0 is now installed\nINFO Tip: To access the cluster you can now fetch the admin kubeconfig using:\nINFO      k0sctl kubeconfig\n</code></pre>"},{"location":"k0sctl-install/#4-access-the-cluster","title":"4. Access the cluster","text":"<p>To access your k0s cluster, use k0sctl to generate a <code>kubeconfig</code> for the purpose.</p> <pre><code>k0sctl kubeconfig &gt; kubeconfig\n</code></pre> <p>With the <code>kubeconfig</code>, you can access your cluster using either kubectl or Lens.</p> <pre><code>$ kubectl get pods --kubeconfig kubeconfig -A\nNAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE\nkube-system   calico-kube-controllers-5f6546844f-w8x27   1/1     Running   0          3m50s\nkube-system   calico-node-vd7lx                          1/1     Running   0          3m44s\nkube-system   coredns-5c98d7d4d8-tmrwv                   1/1     Running   0          4m10s\nkube-system   konnectivity-agent-d9xv2                   1/1     Running   0          3m31s\nkube-system   kube-proxy-xp9r9                           1/1     Running   0          4m4s\nkube-system   metrics-server-6fbcd86f7b-5frtn            1/1     Running   0          3m51s\n</code></pre>"},{"location":"k0sctl-install/#known-limitations","title":"Known limitations","text":"<ul> <li>k0sctl does not perform any discovery of hosts, and thus it only operates on the hosts listed in the provided configuration.</li> <li>k0sctl can only add more nodes to the cluster. It cannot remove existing nodes.</li> </ul>"},{"location":"k0sctl-install/#next-steps","title":"Next Steps","text":"<ul> <li>Control plane configuration options: Networking and datastore configuration</li> <li>Worker node configuration options: Node labels and kubelet arguments</li> <li>Support for cloud providers: Load balancer or storage configuration</li> <li>Installing the Traefik Ingress Controller: Ingress deployment information</li> </ul>"},{"location":"manifests/","title":"Manifest Deployer","text":"<p>Included with k0s, Manifest Deployer is one of two methods you can use to run k0s with your preferred extensions (the other being by defining your extensions as Helm charts.</p>"},{"location":"manifests/#overview","title":"Overview","text":"<p>Manifest Deployer runs on the controller nodes and provides an easy way to automatically deploy manifests at runtime.</p> <p>By default, k0s reads all manifests under <code>/var/lib/k0s/manifests</code> and ensures that their state matches the cluster state. Moreover, on removal of a manifest file, k0s will automatically prune all of it associated resources.</p> <p>The use of Manifest Deployer is quite similar to the use the <code>kubectl apply</code> command. The main difference between the two is that Manifest Deployer constantly monitors the directory for changes, and thus you do not need to manually apply changes that are made to the manifest files.</p>"},{"location":"manifests/#note","title":"Note","text":"<ul> <li>Each directory that is a direct descendant of <code>/var/lib/k0s/manifests</code> is considered to be its own \"stack\". Nested directories (further subfolders), however, are excluded from the stack mechanism and thus are not automatically deployed by the Manifest Deployer.</li> </ul> <ul> <li>k0s uses the indepenent stack mechanism for some of its internal in-cluster components, as well as for other resources. Be sure to only touch the manifests that are not managed by k0s.</li> </ul> <ul> <li>Explicitly define the namespace in the manifests (Manifest Deployer does not have a default namespace).</li> </ul>"},{"location":"manifests/#example","title":"Example","text":"<p>To try Manifest Deployer, create a new folder under <code>/var/lib/k0s/manifests</code> and then create a manifest file (such as <code>nginx.yaml</code>) with the following content:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\nname: nginx\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: nginx-deployment\nnamespace: nginx\nspec:\nselector:\nmatchLabels:\napp: nginx\nreplicas: 3\ntemplate:\nmetadata:\nlabels:\napp: nginx\nspec:\ncontainers:\n- name: nginx\nimage: nginx:latest\nports:\n- containerPort: 80\n</code></pre> <p>New pods will appear soon thereafter.</p> <pre><code>$ sudo k0s kubectl get pods --namespace nginx\nNAME                                READY   STATUS    RESTARTS   AGE\nnginx-deployment-66b6c48dd5-8zq7d   1/1     Running   0          10m\nnginx-deployment-66b6c48dd5-br4jv   1/1     Running   0          10m\nnginx-deployment-66b6c48dd5-sqvhb   1/1     Running   0          10m\n</code></pre>"},{"location":"networking/","title":"Networking","text":""},{"location":"networking/#in-cluster-networking","title":"In-cluster networking","text":"<p>k0s supports two Container Network Interface (CNI) providers out-of-box, Kube-router and Calico. In addition, k0s can support your own CNI configuration.</p>"},{"location":"networking/#notes","title":"Notes","text":"<ul> <li>When deploying k0s with the default settings, all pods on a node can communicate with all pods on all nodes. No configuration changes are needed to get started.</li> <li>Once you initialize the cluster with a network provider the only way to change providers is through a full cluster redeployment.</li> </ul>"},{"location":"networking/#kube-router","title":"Kube-router","text":"<p>Kube-router is built into k0s, and so by default the distribution uses it for network provision. Kube-router uses the standard Linux networking stack and toolset, and you can set up CNI networking without any overlays by using BGP as the main mechanism for in-cluster networking.</p> <ul> <li>Supports armv7 (among many other archs)</li> <li>Uses bit less resources (~15%)</li> <li>Does NOT support dual-stack (IPv4/IPv6) networking</li> <li>Does NOT support Windows nodes</li> </ul>"},{"location":"networking/#calico","title":"Calico","text":"<p>In addition to Kube-router, k0s also offers Calico as an alternative, built-in network provider. Calico is a layer 3 container networking solution that routes packets to pods. It supports, for example, pod-specific network policies that help to secure kubernetes clusters in demanding use cases. Calico uses the vxlan overlay network by default, and you can configure it to support ipip (IP-in-IP).</p> <ul> <li>Does NOT support armv7</li> <li>Uses bit more resources</li> <li>Supports dual-stack (IPv4/IPv6) networking</li> <li>Supports Windows nodes</li> </ul>"},{"location":"networking/#custom-cni-configuration","title":"Custom CNI configuration","text":"<p>You can opt-out of having k0s manage the network setup and choose instead to use any network plugin that adheres to the CNI specification. To do so, configure <code>custom</code> as the network provider in the k0s configurtion file (<code>k0s.yaml</code>). You can do this, for example, by pushing network provider manifests into <code>/var/lib/k0s/manifests</code>, from where k0s controllers will collect them for deployment into the cluster (for more information, refer to Manifest Deployer.</p>"},{"location":"networking/#controller-worker-communication","title":"Controller-Worker communication","text":"<p>One goal of k0s is to allow for the deployment of an isolated control plane, which may prevent the establishment of an IP route between controller nodes and the pod network. Thus, to enable this communication path (which is mandated by conformance tests), k0s deploys Konnectivity service to proxy traffic from the API server (control plane) into the worker nodes. This ensures that we can always fulfill all the Kubernetes API functionalities, but still operate the control plane in total isolation from the workers.</p> <p>Note: To allow Konnectivity agents running on the worker nodes to establish the connection, configure your firewalls for outbound access.</p> <p></p>"},{"location":"networking/#required-ports-and-protocols","title":"Required ports and protocols","text":"Protocol Port Service Direction Notes TCP 2380 etcd peers controller &lt;-&gt; controller TCP 6443 kube-apiserver Worker, CLI =&gt; controller Authenticated Kube API using Kube TLS client certs, ServiceAccount tokens with RBAC TCP 179 kube-router worker &lt;-&gt; worker BGP routing sessions between peers UDP 4789 Calico worker &lt;-&gt; worker Calico VXLAN overlay TCP 10250 kubelet Master, Worker =&gt; Host <code>*</code> Authenticated kubelet API for the master node <code>kube-apiserver</code> (and <code>heapster</code>/<code>metrics-server</code> addons) using TLS client certs TCP 9443 k0s-api controller &lt;-&gt; controller k0s controller join API, TLS with token auth TCP 8132,8133 konnectivity server worker &lt;-&gt; controller Konnectivity is used as \"reverse\" tunnel between kube-apiserver and worker kubelets"},{"location":"raspberry-pi4/","title":"Create a Raspberry Pi 4 Cluster","text":"<p>You can deploy the k0s distribution of Kubernetes to a cluster comprised of Raspberry Pi 4 Computers with Ubuntu 20.04 LTS as the operating system.</p>"},{"location":"raspberry-pi4/#prerequisites","title":"Prerequisites","text":"<p>Install the following tools on your local system:</p> <ul> <li>Kubectl <code>v1.19.4</code>+</li> <li>Raspberry Pi Imager <code>v1.5</code>+</li> <li>Raspberry Pi 4 Model B Computers with 8GB of RAM and 64GB SD Cards.</li> </ul> <p>Note: If you use lower spec Raspberry Pi machines, it may be necessary to manually edit the example code and k0s configuration.</p>"},{"location":"raspberry-pi4/#install-k0s","title":"Install k0s","text":""},{"location":"raspberry-pi4/#set-up-hardware-and-operating-system","title":"Set up hardware and operating system","text":"<p>Download and install the Ubuntu Server 20.04.1 LTS RASPI 4 Image.</p> <p>Note: In addition to the documentation Ubuntu provides documentation on installing Ubuntu on Raspberry Pi Computers, the Raspberry Pi Foundation offers animaging tool that you can use to write the Ubuntu image to your noe SD cards.</p> <p>Once cloud-init finishes bootstrapping the system, the default login credentials are set to user <code>ubuntu</code> with password <code>ubuntu</code> (which you will be prompted to change on first login).</p>"},{"location":"raspberry-pi4/#network-configurations","title":"Network configurations","text":"<p>Note: For network configurtion purposes, this documentation assumes that all of your computers are connected on the same subnet.</p> <p>Review the k0s required ports documentation to ensure that your network and firewall configurations allow necessary traffic for the cluster.</p> <p>Review the Ubuntu Server Networking Configuration Documentation to ensure that all systems have a static IP address on the network, or that the network is providing a static DHCP lease for the nodes.</p>"},{"location":"raspberry-pi4/#openssh","title":"OpenSSH","text":"<p>Ubuntu Server deploys and enables OpenSSH by default. Confirm, though, that for whichever user you will deploy the cluster with on the build system, their SSH Key is copied to each node's root user. Before you start, the configuration should be such that the current user can run:</p> <pre><code>ssh root@${HOST}\n</code></pre> <p>Where <code>${HOST}</code> is any node and the login can succeed with no further prompts.</p>"},{"location":"raspberry-pi4/#set-up-nodes","title":"Set up Nodes","text":"<p>Every node (whether control plane or not) requires additional configuration in preparation for k0s deployment.</p>"},{"location":"raspberry-pi4/#cgroup-configuration","title":"CGroup Configuration","text":"<ol> <li> <p>Ensure that the following packages are installed on each node:</p> <pre><code>apt-get install cgroup-lite cgroup-tools cgroupfs-mount\n</code></pre> </li> <li> <p>Enable the <code>memory</code> cgroup in the Kernel by adding it to the Kernel command line.</p> </li> <li> <p>Open the file <code>/boot/firmware/cmdline.txt</code> (responsible for managing the Kernel parameters), and confirm that the following parameters exist (and add them as necessary):</p> <pre><code>cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1\n</code></pre> </li> <li> <p>Be sure to reboot each node to ensure the <code>memory</code> cgroup is loaded.</p> </li> </ol>"},{"location":"raspberry-pi4/#swap-optional","title":"Swap (Optional)","text":"<p>While swap is technically optional, enable it to ease memory pressure.</p> <ol> <li> <p>To create a swapfile:</p> <pre><code>fallocate -l 2G /swapfile &amp;&amp; \\\nchmod 0600 /swapfile &amp;&amp; \\\nmkswap /swapfile &amp;&amp; \\\nswapon -a\n</code></pre> </li> <li> <p>Ensure that the usage of swap is not too agressive by setting the <code>sudo sysctl vm.swappiness=10</code> (the default is generally higher) and configuring it to be persistent in <code>/etc/sysctl.d/*</code>.</p> </li> <li> <p>Ensure that your swap is mounted after reboots by confirming that the following line exists in your <code>/etc/fstab</code> configuration:</p> <pre><code>/swapfile         none           swap sw       0 0\n</code></pre> </li> </ol>"},{"location":"raspberry-pi4/#kernel-modules","title":"Kernel Modules","text":"<p>Ensure the loading of the <code>overlay</code>, <code>nf_conntrack</code> and <code>br_netfilter</code> modules:</p> <pre><code>modprobe overlay\nmodprobe nf_conntrack\nmodprobe br_netfilter\n</code></pre> <p>In addition, add each of these modules to your <code>/etc/modules-load.d/modules.conf</code> file to ensure they persist following reboot.</p>"},{"location":"raspberry-pi4/#download-k0s","title":"Download k0s","text":"<p>Download a k0s release. For example:</p> <pre><code>wget -O /tmp/k0s https://github.com/k0sproject/k0s/releases/download/v0.9.1/k0s-v0.9.1-arm64 # replace version number!\nsudo install /tmp/k0s /usr/local/bin/k0s\n</code></pre> <p>-- or --</p> <p>Use the k0s download script (as one command) to download the latest stable k0s and make it executable in <code>/usr/bin/k0s</code>.</p> <pre><code>curl -sSLf https://get.k0s.sh | sudo sh\n</code></pre> <p>At this point you can run <code>k0s</code>:</p> <pre><code>$ k0s version\nv0.9.1\n</code></pre>"},{"location":"raspberry-pi4/#deploy-kubernetes","title":"Deploy Kubernetes","text":"<p>Each node can now serve as a control plane node or worker node.</p>"},{"location":"raspberry-pi4/#control-plane-node","title":"Control Plane Node","text":"<p>Use a non-ha control plane with a single node.</p>"},{"location":"raspberry-pi4/#systemd-service-controller","title":"Systemd Service (controller)","text":"<ol> <li> <p>Create a systemd service:</p> <pre><code>sudo k0s install controller\n</code></pre> </li> <li> <p>Start the service:</p> <pre><code>sudo k0s start\n</code></pre> </li> <li> <p>Run <code>sudo k0s status</code> or <code>systemctl status k0scontroller</code> to verify the service status.</p> </li> </ol>"},{"location":"raspberry-pi4/#worker-tokens","title":"Worker Tokens","text":"<p>For each worker node that you expect to have, create a join token:</p> <pre><code>k0s token create --role worker\n</code></pre> <p>Save the join token for subsequent steps.</p>"},{"location":"raspberry-pi4/#worker","title":"Worker","text":"<p>You must deploy and start a worker service for each worker nodes for which you created join tokens.</p>"},{"location":"raspberry-pi4/#systemd-service-worker","title":"Systemd Service (worker)","text":"<ol> <li> <p>Create the join token file for the worker (where <code>TOKEN_CONTENT</code> is one of the join tokens created in the control plane setup):</p> <pre><code>mkdir -p /var/lib/k0s/\necho TOKEN_CONTENT &gt; /var/lib/k0s/join-token\n</code></pre> </li> <li> <p>Deploy the systemd service for the worker:</p> <pre><code>sudo k0s install worker --token-file /var/lib/k0s/join-token\n</code></pre> </li> <li> <p>Start the service:</p> <pre><code>sudo k0s start\n</code></pre> </li> <li> <p>Run <code>sudo k0s status</code> or <code>systemctl status k0sworker</code> to verify the service status.</p> </li> </ol>"},{"location":"raspberry-pi4/#connect-to-your-cluster","title":"Connect To Your Cluster","text":"<p>Generate a <code>kubeconfig</code> for the cluster and begin managing it with <code>kubectl</code> (where <code>CONTROL_PLANE_NODE</code> is the control plane node address):</p> <pre><code>ssh root@CONTROL_PLANE_NODE k0s kubeconfig create --groups \"system:masters\" k0s &gt; config.yaml\nexport KUBECONFIG=$(pwd)/config.yaml\nkubectl create clusterrolebinding k0s-admin-binding --clusterrole=admin --user=k0s\n</code></pre> <p>You can now access and use the cluster:</p> <pre><code>$ kubectl get nodes,deployments,pods -A\nNAME         STATUS   ROLES    AGE     VERSION\nnode/k8s-4   Ready    &lt;none&gt;   5m9s    v1.20.1-k0s1\nnode/k8s-5   Ready    &lt;none&gt;   5m      v1.20.1-k0s1\nnode/k8s-6   Ready    &lt;none&gt;   4m45s   v1.20.1-k0s1\n\nNAMESPACE     NAME                                      READY   UP-TO-DATE   AVAILABLE   AGE\nkube-system   deployment.apps/calico-kube-controllers   1/1     1            1           12m\nkube-system   deployment.apps/coredns                   1/1     1            1           12m\n\nNAMESPACE     NAME                                           READY   STATUS        RESTARTS   AGE\nkube-system   pod/calico-kube-controllers-5f6546844f-rjdkz   1/1     Running       0          12m\nkube-system   pod/calico-node-j475n                          1/1     Running       0          5m9s\nkube-system   pod/calico-node-lnfrf                          1/1     Running       0          4m45s\nkube-system   pod/calico-node-pzp7x                          1/1     Running       0          5m\nkube-system   pod/coredns-5c98d7d4d8-bg9pl                   1/1     Running       0          12m\nkube-system   pod/konnectivity-agent-548hp                   1/1     Running       0          4m45s\nkube-system   pod/konnectivity-agent-66cr8                   1/1     Running       0          4m49s\nkube-system   pod/konnectivity-agent-lxt9z                   1/1     Running       0          4m58s\nkube-system   pod/kube-proxy-ct6bg                           1/1     Running       0          5m\nkube-system   pod/kube-proxy-hg8t2                           1/1     Running       0          4m45s\nkube-system   pod/kube-proxy-vghs9                           1/1     Running       0          5m9s\n</code></pre>"},{"location":"reset/","title":"Uninstall/Reset","text":"<p>k0s can be uninstalled locally with <code>k0s reset</code> command and remotely with <code>k0sctl reset</code> command. They remove all k0s-related files from the host.</p> <p><code>reset</code> operates under the assumption that k0s is installed as a service on the host.</p>"},{"location":"reset/#uninstall-a-k0s-node-locally","title":"Uninstall a k0s node locally","text":"<p>To prevent accidental triggering, <code>k0s reset</code> will not run if the k0s service is running, so you must first stop the service:</p> <ol> <li> <p>Stop the service:</p> <pre><code>sudo k0s stop\n</code></pre> </li> <li> <p>Invoke the <code>reset</code> command:</p> <pre><code>$ sudo k0s reset\nINFO[2021-06-29 13:08:39] * containers steps\nINFO[2021-06-29 13:08:44] successfully removed k0s containers!\nINFO[2021-06-29 13:08:44] no config file given, using defaults\nINFO[2021-06-29 13:08:44] * remove k0s users step:\nINFO[2021-06-29 13:08:44] no config file given, using defaults\nINFO[2021-06-29 13:08:44] * uninstall service step\nINFO[2021-06-29 13:08:44] Uninstalling the k0s service\nINFO[2021-06-29 13:08:45] * remove directories step\nINFO[2021-06-29 13:08:45] * CNI leftovers cleanup step\nINFO k0s cleanup operations done. To ensure a full reset, a node reboot is recommended.\n</code></pre> </li> </ol>"},{"location":"reset/#uninstall-a-k0s-cluster-using-k0sctl","title":"Uninstall a k0s cluster using k0sctl","text":"<p>k0sctl can be used to connect each node and remove all k0s-related files and processes from the hosts.</p> <ol> <li>Invoke <code>k0sctl reset</code> command:<pre><code>$ k0sctl reset --config k0sctl.yaml\nk0sctl v0.9.0 Copyright 2021, k0sctl authors.\n\n? Going to reset all of the hosts, which will destroy all configuration and data, Are you sure? Yes\nINFO ==&gt; Running phase: Connect to hosts \nINFO [ssh] 13.53.43.63:22: connected              \nINFO [ssh] 13.53.218.149:22: connected            INFO ==&gt; Running phase: Detect host operating systems \nINFO [ssh] 13.53.43.63:22: is running Ubuntu 20.04.2 LTS \nINFO [ssh] 13.53.218.149:22: is running Ubuntu 20.04.2 LTS INFO ==&gt; Running phase: Prepare hosts    INFO ==&gt; Running phase: Gather k0s facts \nINFO [ssh] 13.53.43.63:22: found existing configuration \nINFO [ssh] 13.53.43.63:22: is running k0s controller version 1.21.7+k0s.0\nINFO [ssh] 13.53.218.149:22: is running k0s worker version 1.21.7+k0s.0\nINFO [ssh] 13.53.43.63:22: checking if worker  has joined INFO ==&gt; Running phase: Reset hosts      \nINFO [ssh] 13.53.43.63:22: stopping k0s           \nINFO [ssh] 13.53.218.149:22: stopping k0s         \nINFO [ssh] 13.53.218.149:22: running k0s reset    \nINFO [ssh] 13.53.43.63:22: running k0s reset      INFO ==&gt; Running phase: Disconnect from hosts INFO ==&gt; Finished in 8s                  </code></pre> </li> </ol>"},{"location":"runtime/","title":"Runtime","text":"<p>k0s uses containerd as the default Container Runtime Interface (CRI) and runc as the default low-level runtime. In most cases they don't require any configuration changes. However, if custom configuration is needed, this page provides some examples.</p> <p></p>"},{"location":"runtime/#containerd-configuration","title":"containerd configuration","text":"<p>To make changes to containerd configuration you must first generate a default containerd configuration, with the default values set to <code>/etc/k0s/containerd.toml</code>:</p> <pre><code>containerd config default &gt; /etc/k0s/containerd.toml\n</code></pre> <p><code>k0s</code> runs containerd with the following default values:</p> <pre><code>/var/lib/k0s/bin/containerd \\\n--root=/var/lib/k0s/containerd \\\n--state=/var/lib/k0s/run/containerd \\\n--address=/var/lib/k0s/run/containerd.sock \\\n--config=/etc/k0s/containerd.toml\n</code></pre> <p>Next, add the following default values to the configuration file:</p> <pre><code>version = 2\nroot = \"/var/lib/k0s/containerd\"\nstate = \"/var/lib/k0s/run/containerd\"\n...\n\n[grpc]\naddress = \"/var/lib/k0s/run/containerd.sock\"\n</code></pre> <p>Finally, if you want to change CRI look into:</p> <pre><code>  [plugins.\"io.containerd.runtime.v1.linux\"]\nshim = \"containerd-shim\"\nruntime = \"runc\"\n</code></pre>"},{"location":"runtime/#using-gvisor","title":"Using gVisor","text":"<p>gVisor is an application kernel, written in Go, that implements a substantial portion of the Linux system call interface. It provides an additional layer of isolation between running applications and the host operating system.</p> <ol> <li> <p>Install the needed gVisor binaries into the host.</p> <pre><code>(\nset -e\n  URL=https://storage.googleapis.com/gvisor/releases/release/latest\n  wget ${URL}/runsc ${URL}/runsc.sha512 \\\n${URL}/gvisor-containerd-shim ${URL}/gvisor-containerd-shim.sha512 \\\n${URL}/containerd-shim-runsc-v1 ${URL}/containerd-shim-runsc-v1.sha512\n  sha512sum -c runsc.sha512 \\\n-c gvisor-containerd-shim.sha512 \\\n-c containerd-shim-runsc-v1.sha512\n  rm -f *.sha512\n  chmod a+rx runsc gvisor-containerd-shim containerd-shim-runsc-v1\n  sudo mv runsc gvisor-containerd-shim containerd-shim-runsc-v1 /usr/local/bin\n)\n</code></pre> <p>Refer to the gVisor install docs for more information.</p> </li> <li> <p>Prepare the config for <code>k0s</code> managed containerD, to utilize gVisor as additional runtime:</p> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/k0s/containerd.toml\ndisabled_plugins = [\"restart\"]\n[plugins.linux]\n  shim_debug = true\n[plugins.cri.containerd.runtimes.runsc]\n  runtime_type = \"io.containerd.runsc.v1\"\nEOF\n</code></pre> </li> <li> <p>Start and join the worker into the cluster, as normal:</p> <pre><code>k0s worker $token\n</code></pre> </li> <li> <p>Register containerd to the Kubernetes side to make gVisor runtime usable for workloads (by default, containerd uses normal runc as the runtime):</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: node.k8s.io/v1beta1\nkind: RuntimeClass\nmetadata:\n  name: gvisor\nhandler: runsc\nEOF\n</code></pre> <p>At this point, you can use gVisor runtime for your workloads:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: nginx-gvisor\nspec:\nruntimeClassName: gvisor\ncontainers:\n- name: nginx\nimage: nginx\n</code></pre> </li> <li> <p>(Optional) Verify tht the created nginx pod is running under gVisor runtime:</p> <pre><code># kubectl exec nginx-gvisor -- dmesg | grep -i gvisor\n[    0.000000] Starting gVisor...\n</code></pre> </li> </ol>"},{"location":"runtime/#using-nvidia-container-runtime","title":"Using <code>nvidia-container-runtime</code>","text":"<p>By default, CRI is set to runC. As such, you must configure Nvidia GPU support by replacing <code>runc</code> with <code>nvidia-container-runtime</code>:</p> <pre><code>[plugins.\"io.containerd.runtime.v1.linux\"]\nshim = \"containerd-shim\"\nruntime = \"nvidia-container-runtime\"\n</code></pre> <p>Note Detailed instruction on how to run <code>nvidia-container-runtime</code> on your node is available here.</p> <p>After editing the configuration, restart <code>k0s</code> to get containerd using the newly configured runtime.</p>"},{"location":"runtime/#using-custom-cri-runtime","title":"Using custom CRI runtime","text":"<p>Warning: You can use your own CRI runtime with k0s (for example, <code>docker</code>). However, k0s will not start or manage the runtime, and configuration is solely your responsibility.</p> <p>Use the option <code>--cri-socket</code> to run a k0s worker with a custom CRI runtime. the option takes input in the form of <code>&lt;type&gt;:&lt;socket_path&gt;</code> (for <code>type</code>, use <code>docker</code> for a pure Docker setup and <code>remote</code> for anything else).</p> <p>To run k0s with a pre-existing Docker setup, run the worker with <code>k0s worker --cri-socket docker:unix:///var/run/docker.sock &lt;token&gt;</code>.</p> <p>When <code>docker</code> is used as a runtime, k0s configures kubelet to create the dockershim socket at <code>/var/run/dockershim.sock</code>.</p>"},{"location":"shell-completion/","title":"Enabling Shell Completion","text":"<p>Generate the k0s completion script using the <code>k0s completion &lt;shell_name&gt;</code> command, for Bash, Zsh, fish, or PowerShell.</p> <p>Sourcing the completion script in your shell enables k0s autocompletion.</p>"},{"location":"shell-completion/#bash","title":"Bash","text":"<pre><code>echo 'source &lt;(k0s completion bash)' &gt;&gt;~/.bashrc\n</code></pre> <p>To load completions for each session, execute once:</p> <pre><code>k0s completion bash &gt; /etc/bash_completion.d/k0s\n</code></pre>"},{"location":"shell-completion/#zsh","title":"Zsh","text":"<p>If shell completion is not already enabled in Zsh environment you will need to enable it:</p> <pre><code>echo \"autoload -U compinit; compinit\" &gt;&gt; ~/.zshrc\n</code></pre> <p>To load completions for each session, execute once:</p> <pre><code>k0s completion zsh &gt; \"${fpath[1]}/_k0s\"\n</code></pre> <p>Note: You must start a new shell for the setup to take effect.</p>"},{"location":"shell-completion/#fish","title":"Fish","text":"<pre><code>k0s completion fish | source\n</code></pre> <p>To load completions for each session, execute once:</p> <pre><code>k0s completion fish &gt; ~/.config/fish/completions/k0s.fish\n</code></pre>"},{"location":"storage/","title":"Storage (CSI)","text":"<p>k0s supports a wide range of different storage options. There are no \"selected\" storage in k0s. Instead, all Kubernetes storage solutions are supported and users can easily select the storage that fits best for their needs.</p> <p>When the storage solution implements Container Storage Interface (CSI), containers can communicate with the storage for creation and configuration of persistent volumes. This makes it easy to dynamically provision the requested volumes. It also expands the supported storage solutions from the previous generation, in-tree volume plugins. More information about the CSI concept is described on the Kubernetes Blog.</p> <p></p>"},{"location":"storage/#example-storage-solutions","title":"Example storage solutions","text":"<p>Different Kubernetes storage solutions are explained in the official Kubernetes storage documentation. All of them can be used with k0s. Here are some popular ones:</p> <ul> <li>Rook-Ceph (Open Source)</li> <li>OpenEBS (Open Source)</li> <li>MinIO (Open Source)</li> <li>Gluster (Open Source)</li> <li>Longhorn (Open Source)</li> <li>Amazon EBS</li> <li>Google Persistent Disk</li> <li>Azure Disk</li> <li>Portworx</li> </ul> <p>If you are looking for a fault-tolerant storage with data replication, you can find a k0s tutorial for configuring Ceph storage with Rook in here.</p> <p>If you are looking for a bit more simple solution and use a folder from the node local disk, you can take a look at OpenEBS. With OpenEBS, you can either create a simple local storage or a highly available distributed storage.</p>"},{"location":"system-requirements/","title":"System requirements","text":"<p>Verify that your environment meets the system requirements for k0s.</p>"},{"location":"system-requirements/#hardware","title":"Hardware","text":"<p>The minimum hardware requirements for k0s detailed below are approximations and thus results may vary.</p> Role Virtual CPU (vCPU) Memory (RAM) Controller node 1 vCPU (2 recommended) 1 GB (2 recommended) Worker node 1 vCPU (2 recommended) 0.5 GB (1 recommended) Controller + worker 1 vCPU (2 recommended) 1 GB (2 recommended) <p>Note: Use an SSD for optimal storage performance (cluster latency and throughput are sensitive to storage).</p> <p>The specific storage consumption for k0s is as follows:</p> Role Storage (k0s part) Controller node ~0.5 GB Worker node ~1.3 GB Controller + worker ~1.7 GB <p>Note: The operating system and application requirements must be considered in addition to the k0s part.</p>"},{"location":"system-requirements/#host-operating-system","title":"Host operating system","text":"<ul> <li>Linux (kernel v3.10 or later)</li> <li>Windows Server 2019</li> </ul>"},{"location":"system-requirements/#architecture","title":"Architecture","text":"<ul> <li>x86-64</li> <li>ARM64</li> <li>ARMv7</li> </ul>"},{"location":"system-requirements/#networking","title":"Networking","text":"<p>For information on the ports that k0s needs to function, refer to networking.</p>"},{"location":"troubleshooting/","title":"Common Pitfalls","text":"<p>There are few common cases we've seen where k0s fails to run properly.</p>"},{"location":"troubleshooting/#coredns-in-crashloop","title":"CoreDNS in crashloop","text":"<p>The most common case we've encountered so far has been CoreDNS getting into crashloop on the node(s).</p> <p>With kubectl you see something like this:</p> <pre><code>$ kubectl get pod --all-namespaces\nNAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE\nkube-system   calico-kube-controllers-5f6546844f-25px6   1/1     Running   0          167m\nkube-system   calico-node-fwjx5                          1/1     Running   0          164m\nkube-system   calico-node-t4tx5                          1/1     Running   0          164m\nkube-system   calico-node-whwsg                          1/1     Running   0          164m\nkube-system   coredns-5c98d7d4d8-tfs4q                   1/1     Error     17         167m\nkube-system   konnectivity-agent-9jkfd                   1/1     Running   0          164m\nkube-system   konnectivity-agent-bvhdb                   1/1     Running   0          164m\nkube-system   konnectivity-agent-r6mzj                   1/1     Running   0          164m\nkube-system   kube-proxy-kr2r9                           1/1     Running   0          164m\nkube-system   kube-proxy-tbljr                           1/1     Running   0          164m\nkube-system   kube-proxy-xbw7p                           1/1     Running   0          164m\nkube-system   metrics-server-7d4bcb75dd-pqkrs            1/1     Running   0          167m\n</code></pre> <p>When you check the logs, it'll show something like this:</p> <pre><code>$ kubectl -n kube-system logs coredns-5c98d7d4d8-tfs4q\nplugin/loop: Loop (127.0.0.1:55953 -&gt; :1053) detected for zone \".\", see https://coredns.io/plugins/loop#troubleshooting. Query: \"HINFO 4547991504243258144.3688648895315093531.\"\n</code></pre> <p>This is most often caused by systemd-resolved stub (or something similar) running locally and CoreDNS detects a possible loop with DNS queries.</p> <p>The easiest but most crude way to workaround is to disable the systemd-resolved stub and revert the hosts <code>/etc/resolv.conf</code> to original</p> <p>Read more at CoreDNS troubleshooting docs.</p>"},{"location":"troubleshooting/#k0s-controller-fails-on-arm-boxes","title":"<code>k0s controller</code> fails on ARM boxes","text":"<p>In the logs you probably see ETCD not starting up properly.</p> <p>Etcd is not fully supported on ARM architecture, thus you need to run <code>k0s controller</code> and thus also etcd process with env <code>ETCD_UNSUPPORTED_ARCH=arm64</code>.</p> <p>As Etcd is not fully supported on ARM architecture it also means that k0s controlplane with etcd itself is not fully supported on ARM either.</p>"},{"location":"troubleshooting/#pods-pending-when-using-cloud-providers","title":"Pods pending when using cloud providers","text":"<p>Once we enable cloud provider support on kubelet on worker nodes, kubelet will automatically add a taint <code>node.cloudprovider.kubernetes.io/uninitialized</code> for the node. This tain will prevent normal workloads to be scheduled on the node until the cloud provider controller actually runs second initialization on the node and removes the taint. This means that these nodes are not available for scheduling until the cloud provider controller is actually successfully running on the cluster.</p> <p>For troubleshooting your specific cloud provider see its documentation.</p>"},{"location":"troubleshooting/#k0s-not-working-with-read-only-usr","title":"k0s not working with read only <code>/usr</code>","text":"<p>By default k0s does not run on nodes where <code>/usr</code> is read only.</p> <p>This can be fixed by changing the default path for <code>volumePluginDir</code> in your k0s config. You will need to change to values, one for the kubelet itself, and one for Calico.</p> <p>Here is a snippet of an example config with the default values changed:</p> <pre><code>spec:\ncontrollerManager:\nextraArgs:\nflex-volume-plugin-dir: \"/etc/kubernetes/kubelet-plugins/volume/exec\"\nnetwork:\ncalico:\nflexVolumeDriverPath: /etc/k0s/kubelet-plugins/volume/exec/nodeagent~uds\nworkerProfiles:\n- name: coreos\nvalues:\nvolumePluginDir: /etc/k0s/kubelet-plugins/volume/exec/\n</code></pre> <p>With this config you can start your controller as usual. Any workers will need to be started with</p> <pre><code>k0s worker --profile coreos [TOKEN]\n</code></pre>"},{"location":"troubleshooting/#profiling","title":"Profiling","text":"<p>We drop any debug related information and symbols from the compiled binary by utilzing <code>-w -s</code> linker flags.</p> <p>To keep those symbols use <code>DEBUG</code> env variable:</p> <pre><code>DEBUG=true make k0s\n</code></pre> <p>Any value not equal to the \"false\" would work.</p> <p>To add custom linker flags use <code>LDFLAGS</code> variable.</p> <pre><code>LD_FLAGS=\"--custom-flag=value\" make k0s\n</code></pre>"},{"location":"upgrade/","title":"Upgrade","text":"<p>The k0s upgrade is a simple process due to its single binary distribution. The k0s single binary file includes all the necessary parts for the upgrade and essentially the upgrade process is to replace that file and restart the service.</p> <p>This tutorial explains two different approaches for k0s upgrade:</p> <ul> <li>Upgrade a k0s node locally</li> <li>Upgrade a k0s cluster using k0sctl</li> </ul>"},{"location":"upgrade/#upgrade-a-k0s-node-locally","title":"Upgrade a k0s node locally","text":"<p>If your k0s cluster has been deployed with k0sctl, then k0sctl provides the easiest upgrade method. In that case jump to the next chapter. However, if you have deployed k0s without k0sctl, then follow the upgrade method explained in this chapter.</p> <p>Before starting the upgrade, consider moving your applications to another node if you want to avoid downtime. This can be done by draining a worker node. Remember to uncordon the worker node afterwards to tell Kubernetes that it can resume scheduling new pods onto the node.</p> <p>The upgrade process is started by stopping the currently running k0s service.</p> <pre><code>sudo k0s stop\n</code></pre> <p>Now you can replace the old k0s binary file. The easiest way is to use the download script. It will download the latest k0s binary and replace the old binary with it. You can also do this manually without the download script.</p> <pre><code>curl -sSLf https://get.k0s.sh | sudo sh\n</code></pre> <p>Then you can start the service (with the upgraded k0s) and your upgrade is done.</p> <pre><code>sudo k0s start\n</code></pre>"},{"location":"upgrade/#upgrade-a-k0s-cluster-using-k0sctl","title":"Upgrade a k0s cluster using k0sctl","text":"<p>The upgrading of k0s clusters using k0sctl occurs not through a particular command (there is no <code>upgrade</code> sub-command in k0sctl) but by way of the configuration file. The configuration file describes the desired state of the cluster, and when you pass the description to the <code>k0sctl apply</code> command a discovery of the current state is performed and the system does whatever is necessary to bring the cluster to the desired state (for example, perform an upgrade).</p>"},{"location":"upgrade/#k0sctl-cluster-upgrade-process","title":"k0sctl cluster upgrade process","text":"<p>The following operations occur during a k0sctl upgrade:</p> <ol> <li> <p>Upgrade of each controller, one at a time. There is no downtime if multiple controllers are configured.</p> </li> <li> <p>Upgrade of workers, in batches of 10%.</p> </li> <li> <p>Draining of workers, which allows the workload to move to other nodes prior to the actual upgrade of the worker node components. (To skip the drain process, use the <code>--no-drain</code> option.)</p> </li> <li> <p>The upgrade process continues once the upgraded nodes return to Ready state.</p> </li> </ol> <p>You can configure the desired cluster version in the k0sctl configuration by setting the value of <code>spec.k0s.version</code>:</p> <pre><code>spec:\nk0s:\nversion: 1.21.7+k0s.0\n</code></pre> <p>If you do not specify a version, k0sctl checks online for the latest version and defaults to it.</p> <pre><code>$ k0sctl apply\n...\n...\nINFO[0001] ==&gt; Running phase: Upgrade controllers\nINFO[0001] [ssh] 10.0.0.23:22: starting upgrade\nINFO[0001] [ssh] 10.0.0.23:22: Running with legacy service name, migrating...\nINFO[0011] [ssh] 10.0.0.23:22: waiting for the k0s service to start\nINFO[0016] ==&gt; Running phase: Upgrade workers\nINFO[0016] Upgrading 1 workers in parallel\nINFO[0016] [ssh] 10.0.0.17:22: upgrade starting\nINFO[0027] [ssh] 10.0.0.17:22: waiting for node to become ready again\nINFO[0027] [ssh] 10.0.0.17:22: upgrade successful\nINFO[0027] ==&gt; Running phase: Disconnect from hosts\nINFO[0027] ==&gt; Finished in 27s\nINFO[0027] k0s cluster version 1.21.7+k0s.0 is now installed\nINFO[0027] Tip: To access the cluster you can now fetch the admin kubeconfig using:\nINFO[0027]      k0sctl kubeconfig\n</code></pre>"},{"location":"user-management/","title":"User Management","text":""},{"location":"user-management/#adding-a-cluster-user","title":"Adding a Cluster User","text":"<p>Run the kubeconfig create command on the controller to add a user to the cluster. The command outputs a kubeconfig for the user, to use for authentication.</p> <pre><code>k0s kubeconfig create [username]\n</code></pre>"},{"location":"user-management/#enabling-access-to-cluster-resources","title":"Enabling Access to Cluster Resources","text":"<p>Create the user with the <code>system:masters</code> group to grant the user access to the cluster:</p> <pre><code>k0s kubeconfig create --groups \"system:masters\" testUser &gt; k0s.config\n</code></pre> <p>Create a <code>roleBinding</code> to grant the user access to the resources:</p> <pre><code>k0s kubectl create clusterrolebinding --kubeconfig k0s.config testUser-admin-binding --clusterrole=admin --user=testUser\n</code></pre>"},{"location":"worker-node-config/","title":"Configuration options for worker nodes","text":"<p>Although the <code>k0s worker</code> command does not take in any special yaml configuration, there are still methods for configuring the workers to run various components.</p>"},{"location":"worker-node-config/#node-labels","title":"Node labels","text":"<p>The <code>k0s worker</code> command accepts the <code>--labels</code> flag, with which you can make the newly joined worker node the register itself, in the Kubernetes API, with the given set of labels.</p> <p>For example, running the worker with <code>k0s worker --token-file k0s.token --labels=\"k0sproject.io/foo=bar,k0sproject.io/other=xyz\"</code> results in:</p> <pre><code>$ kubectl get node --show-labels\nNAME      STATUS     ROLES    AGE   VERSION        LABELS\nworker0   NotReady   &lt;none&gt;   10s   v1.20.2-k0s1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,k0sproject.io/foo=bar,k0sproject.io/other=xyz,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker0,kubernetes.io/os=linux\n</code></pre> <p>Note: Setting the labels is only effective on the first registration of the node. Changing the labels thereafter has no effect.</p>"},{"location":"worker-node-config/#kubelet-args","title":"Kubelet args","text":"<p>The <code>k0s worker</code> command accepts a generic flag to pass in any set of arguments for kubelet process.</p> <p>For example, running <code>k0s worker --token-file=k0s.token --kubelet-extra-args=\"--node-ip=1.2.3.4 --address=0.0.0.0\"</code> passes in the given flags to kubelet as-is. As such, you must confirm that any flags you are passing in are properly formatted and valued as k0s will not validate those flags.</p>"},{"location":"cli/","title":"Index","text":""},{"location":"cli/#k0s","title":"k0s","text":"<p>k0s - Zero Friction Kubernetes</p>"},{"location":"cli/#synopsis","title":"Synopsis","text":"<p>k0s - The zero friction Kubernetes - https://k0sproject.io</p>"},{"location":"cli/#options","title":"Options","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n--data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n--debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n-h, --help                     help for k0s\n  -l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s api - Run the controller api</li> <li>k0s completion - Generate completion script</li> <li>k0s controller - Run controller</li> <li>k0s default-config - Output the default k0s configuration yaml to stdout</li> <li>k0s docs - Generate Markdown docs for the k0s binary</li> <li>k0s etcd - Manage etcd cluster</li> <li>k0s install - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo)</li> <li>k0s start - Start the k0s service after it has been installed using <code>k0s install</code>. Must be run as root (or with sudo)</li> <li>k0s stop - Stop the k0s service after it has been installed using <code>k0s install</code>. Must be run as root (or with sudo)</li> <li>k0s kubeconfig - Create a kubeconfig file for a specified user</li> <li>k0s status - Helper command for get general information about k0s</li> <li>k0s token - Manage join tokens</li> <li>k0s validate - Helper command for validating the config file</li> <li>k0s version - Print the k0s version</li> <li>k0s worker - Run worker</li> </ul>"},{"location":"cli/k0s/","title":"K0s","text":""},{"location":"cli/k0s/#k0s","title":"k0s","text":"<p>k0s - Zero Friction Kubernetes</p>"},{"location":"cli/k0s/#synopsis","title":"Synopsis","text":"<p>k0s - The zero friction Kubernetes - https://k0sproject.io</p>"},{"location":"cli/k0s/#options","title":"Options","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n--data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n--debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n-h, --help                     help for k0s\n  -l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s api - Run the controller api</li> <li>k0s completion - Generate completion script</li> <li>k0s controller - Run controller</li> <li>k0s default-config - Output the default k0s configuration yaml to stdout</li> <li>k0s docs - Generate Markdown docs for the k0s binary</li> <li>k0s etcd - Manage etcd cluster</li> <li>k0s install - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo)</li> <li>k0s start - Start the k0s service after it has been installed using <code>k0s install</code>. Must be run as root (or with sudo)</li> <li>k0s stop - Stop the k0s service after it has been installed using <code>k0s install</code>. Must be run as root (or with sudo)</li> <li>k0s kubeconfig - Create a kubeconfig file for a specified user</li> <li>k0s status - Helper command for get general information about k0s</li> <li>k0s token - Manage join tokens</li> <li>k0s validate - Helper command for validating the config file</li> <li>k0s version - Print the k0s version</li> <li>k0s worker - Run worker</li> </ul>"},{"location":"cli/k0s_api/","title":"K0s api","text":""},{"location":"cli/k0s_api/#k0s-api","title":"k0s api","text":"<p>Run the controller api</p> <pre><code>k0s api [flags]\n</code></pre>"},{"location":"cli/k0s_api/#options","title":"Options","text":"<pre><code>  -h, --help   help for api\n</code></pre>"},{"location":"cli/k0s_api/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n--data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n--debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n-l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_api/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s - k0s - Zero Friction Kubernetes</li> </ul>"},{"location":"cli/k0s_completion/","title":"K0s completion","text":""},{"location":"cli/k0s_completion/#k0s-completion","title":"k0s completion","text":"<p>Generate completion script</p>"},{"location":"cli/k0s_completion/#synopsis","title":"Synopsis","text":"<p>To load completions:</p>"},{"location":"cli/k0s_completion/#bash","title":"Bash","text":"<pre><code>source &lt;(k0s completion bash)\n</code></pre> <p>To load completions for each session, execute once:</p> <pre><code>k0s completion bash &gt; /etc/bash_completion.d/k0s\n</code></pre>"},{"location":"cli/k0s_completion/#zsh","title":"Zsh","text":"<p>If shell completion is not already enabled in your environment you will need to enable it. You can execute the following once:</p> <pre><code>echo \"autoload -U compinit; compinit\" &gt;&gt; ~/.zshrc\n</code></pre> <p>To load completions for each session, execute once:</p> <pre><code>k0s completion zsh &gt; \"${fpath[1]}/_k0s\"\n</code></pre> <p>You will need to start a new shell for this setup to take effect.</p>"},{"location":"cli/k0s_completion/#fish","title":"Fish","text":"<pre><code>k0s completion fish | source\n</code></pre> <p>To load completions for each session, execute once:</p> <pre><code>k0s completion fish &gt; ~/.config/fish/completions/k0s.fish\n</code></pre>"},{"location":"cli/k0s_completion/#usage","title":"Usage","text":"<pre><code>k0s completion [bash|zsh|fish|powershell]\n</code></pre>"},{"location":"cli/k0s_completion/#options","title":"Options","text":"<pre><code>  -h, --help   help for completion\n</code></pre>"},{"location":"cli/k0s_completion/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n--data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n--debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n-l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_completion/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s - k0s - Zero Friction Kubernetes</li> </ul>"},{"location":"cli/k0s_controller/","title":"K0s controller","text":""},{"location":"cli/k0s_controller/#k0s-controller","title":"k0s controller","text":"<p>Run controller</p> <pre><code>k0s controller [join-token] [flags]\n</code></pre>"},{"location":"cli/k0s_controller/#examples","title":"Examples","text":"<p>Command to associate master nodes using a CLI argument:</p> <pre><code>k0s controller [join-token]\n</code></pre> <p>or a CLI flag:</p> <pre><code>k0s controller --token-file [path_to_file]\n</code></pre>"},{"location":"cli/k0s_controller/#options","title":"Options","text":"<pre><code>      --cri-socket string                              contrainer runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket]\n--enable-worker                                  enable worker (default false)\n-h, --help                                           help for controller\n      --profile string                                 worker profile to use on the node (default \"default\")\n--token-file string                              Path to the file containing join-token.\n      --enable-k0s-cloud-provider                      enables the k0s-cloud-provider (default false)\n--k0s-cloud-provider-port int                    the port that k0s-cloud-provider binds on (default 10258)\n--k0s-cloud-provider-update-frequency duration   the frequency of k0s-cloud-provider node updates (default 2m0s)\n</code></pre>"},{"location":"cli/k0s_controller/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n--data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n--debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n-l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_controller/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s - k0s - Zero Friction Kubernetes</li> </ul>"},{"location":"cli/k0s_default-config/","title":"K0s default config","text":""},{"location":"cli/k0s_default-config/#k0s-default-config","title":"k0s default-config","text":"<p>Output the default k0s configuration yaml to stdout</p> <pre><code>k0s default-config [flags]\n</code></pre>"},{"location":"cli/k0s_default-config/#options","title":"Options","text":"<pre><code>  -h, --help   help for default-config\n</code></pre>"},{"location":"cli/k0s_default-config/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n--data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n--debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n-l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_default-config/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s - k0s - Zero Friction Kubernetes</li> </ul>"},{"location":"cli/k0s_docs/","title":"K0s docs","text":""},{"location":"cli/k0s_docs/#k0s-docs","title":"k0s docs","text":"<p>Generate Markdown docs for the k0s binary</p> <pre><code>k0s docs [flags]\n</code></pre>"},{"location":"cli/k0s_docs/#options","title":"Options","text":"<pre><code>  -h, --help   help for docs\n</code></pre>"},{"location":"cli/k0s_docs/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n--data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n--debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n-l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_docs/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s - k0s - Zero Friction Kubernetes</li> </ul>"},{"location":"cli/k0s_etcd/","title":"K0s etcd","text":""},{"location":"cli/k0s_etcd/#k0s-etcd","title":"k0s etcd","text":"<p>Manage etcd cluster</p>"},{"location":"cli/k0s_etcd/#options","title":"Options","text":"<pre><code>  -h, --help   help for etcd\n</code></pre>"},{"location":"cli/k0s_etcd/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n--data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n--debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n-l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_etcd/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s - k0s - Zero Friction Kubernetes</li> <li>k0s etcd leave - Sign off a given etc node from etcd cluster</li> <li>k0s etcd member-list - Returns etcd cluster members list</li> </ul>"},{"location":"cli/k0s_etcd_leave/","title":"K0s etcd leave","text":""},{"location":"cli/k0s_etcd_leave/#k0s-etcd-leave","title":"k0s etcd leave","text":"<p>Sign off a given etc node from etcd cluster</p> <pre><code>k0s etcd leave [flags]\n</code></pre>"},{"location":"cli/k0s_etcd_leave/#options","title":"Options","text":"<pre><code>  -h, --help                  help for leave\n      --peer-address string   etcd peer address\n</code></pre>"},{"location":"cli/k0s_etcd_leave/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n--data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n--debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n-l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_etcd_leave/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s etcd - Manage etcd cluster</li> </ul>"},{"location":"cli/k0s_etcd_member-list/","title":"K0s etcd member list","text":""},{"location":"cli/k0s_etcd_member-list/#k0s-etcd-member-list","title":"k0s etcd member-list","text":"<p>Returns etcd cluster members list</p> <pre><code>k0s etcd member-list [flags]\n</code></pre>"},{"location":"cli/k0s_etcd_member-list/#options","title":"Options","text":"<pre><code>  -h, --help   help for member-list\n</code></pre>"},{"location":"cli/k0s_etcd_member-list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n--data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n--debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n-l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_etcd_member-list/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s etcd - Manage etcd cluster</li> </ul>"},{"location":"cli/k0s_install/","title":"K0s install","text":""},{"location":"cli/k0s_install/#k0s-install","title":"k0s install","text":"<p>Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo).</p>"},{"location":"cli/k0s_install/#options","title":"Options","text":"<pre><code>  -h, --help   help for install\n</code></pre>"},{"location":"cli/k0s_install/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n--data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n--debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n-l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_install/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s - k0s - Zero Friction Kubernetes</li> <li>k0s install controller - Helper command for setting up k0s as controller node on a brand-new system. Must be run as root (or with sudo)</li> <li>k0s install worker - Helper command for setting up k0s as a worker node on a brand-new system. Must be run as root (or with sudo)</li> <li>k0s start - Start the k0s service after it has been installed using <code>k0s install</code>. Must be run as root (or with sudo)</li> <li>k0s stop - Stop the k0s service after it has been installed using <code>k0s install</code>. Must be run as root (or with sudo)</li> </ul>"},{"location":"cli/k0s_install_controller/","title":"K0s install controller","text":""},{"location":"cli/k0s_install_controller/#k0s-install-controller","title":"k0s install controller","text":"<p>Helper command for setting up k0s as controller node on a brand-new system. Must be run as root (or with sudo)</p> <pre><code>k0s install controller [flags]\n</code></pre>"},{"location":"cli/k0s_install_controller/#examples","title":"Examples","text":"<p>All default values of controller command will be passed to the service stub unless overriden.</p> <p>With controller subcommand you can setup a single node cluster by running:</p> <pre><code>sudo k0s install controller --enable-worker\n</code></pre>"},{"location":"cli/k0s_install_controller/#options","title":"Options","text":"<pre><code>      --cri-socket string   contrainer runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket]\n--enable-worker       enable worker (default false)\n-h, --help                help for controller\n      --profile string      worker profile to use on the node (default \"default\")\n--token-file string   Path to the file containing join-token.\n</code></pre>"},{"location":"cli/k0s_install_controller/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n--data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n--debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n-l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_install_controller/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s install - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo)</li> </ul>"},{"location":"cli/k0s_install_worker/","title":"K0s install worker","text":""},{"location":"cli/k0s_install_worker/#k0s-install-worker","title":"k0s install worker","text":"<p>Helper command for setting up k0s as a worker node on a brand-new system. Must be run as root (or with sudo)</p> <pre><code>k0s install worker [flags]\n</code></pre>"},{"location":"cli/k0s_install_worker/#synopsis","title":"Synopsis","text":"<p>Worker subcommand allows you to pass in all available worker parameters.</p> <p>All default values of worker command will be passed to the service stub unless overriden.</p> <p>Windows flags like \"--api-server\", \"--cidr-range\" and \"--cluster-dns\" will be ignored since install command doesn't yet support Windows services</p>"},{"location":"cli/k0s_install_worker/#options","title":"Options","text":"<pre><code>      --api-server string       HACK: api-server for the windows worker node\n      --cidr-range string       HACK: cidr range for the windows worker node (default \"10.96.0.0/12\")\n--cluster-dns string      HACK: cluster dns for the windows worker node (default \"10.96.0.10\")\n--cri-socket string       contrainer runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket]\n--enable-cloud-provider   Whether or not to enable cloud provider support in kubelet\n  -h, --help                    help for worker\n      --profile string          worker profile to use on the node (default \"default\")\n--token-file string       Path to the file containing token.\n</code></pre>"},{"location":"cli/k0s_install_worker/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n--data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n--debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n-l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_install_worker/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s install - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo)</li> </ul>"},{"location":"cli/k0s_kubeconfig/","title":"K0s kubeconfig","text":""},{"location":"cli/k0s_kubeconfig/#k0s-kubeconfig","title":"k0s kubeconfig","text":"<p>Create a kubeconfig file for a specified user</p> <pre><code>k0s kubeconfig [command] [flags]\n</code></pre>"},{"location":"cli/k0s_kubeconfig/#options","title":"Options","text":"<pre><code>  -h, --help   help for kubeconfig\n</code></pre>"},{"location":"cli/k0s_kubeconfig/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n--data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n--debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n-l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_kubeconfig/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s - k0s - Zero Friction Kubernetes</li> <li>k0s kubeconfig admin - Display Admin's Kubeconfig file</li> <li>k0s kubeconfig create - Create a kubeconfig for a user</li> </ul>"},{"location":"cli/k0s_kubeconfig_admin/","title":"K0s kubeconfig admin","text":""},{"location":"cli/k0s_kubeconfig_admin/#k0s-kubeconfig-admin","title":"k0s kubeconfig admin","text":"<p>Display Admin's Kubeconfig file</p>"},{"location":"cli/k0s_kubeconfig_admin/#synopsis","title":"Synopsis","text":"<p>Print kubeconfig for the Admin user to stdout</p> <pre><code>k0s kubeconfig admin [command] [flags]\n</code></pre>"},{"location":"cli/k0s_kubeconfig_admin/#examples","title":"Examples","text":"<pre><code>k0s kubeconfig admin &gt; ~/.kube/config\nexport KUBECONFIG=~/.kube/config\nkubectl get nodes\n</code></pre>"},{"location":"cli/k0s_kubeconfig_admin/#options","title":"Options","text":"<pre><code>  -h, --help   help for admin\n</code></pre>"},{"location":"cli/k0s_kubeconfig_admin/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n--data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n--debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n-l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_kubeconfig_admin/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s kubeconfig - Create a kubeconfig file for a specified user</li> </ul>"},{"location":"cli/k0s_kubeconfig_create/","title":"K0s kubeconfig create","text":""},{"location":"cli/k0s_kubeconfig_create/#k0s-kubeconfig-create","title":"k0s kubeconfig create","text":"<p>Create a kubeconfig for a user</p>"},{"location":"cli/k0s_kubeconfig_create/#synopsis","title":"Synopsis","text":"<p>Create a kubeconfig with a signed certificate and public key for a given user (and optionally user groups) Note: A certificate once signed cannot be revoked for a particular user</p> <pre><code>k0s kubeconfig create [username] [flags]\n</code></pre>"},{"location":"cli/k0s_kubeconfig_create/#examples","title":"Examples","text":"<p>Command to create a kubeconfig for a user:</p> <pre><code>k0s kubeconfig create [username]\n</code></pre> <p>optionally add groups:</p> <pre><code>k0s kubeconfig create [username] --groups [groups]\n</code></pre>"},{"location":"cli/k0s_kubeconfig_create/#options","title":"Options","text":"<pre><code>      --groups string   Specify groups\n  -h, --help            help for create\n</code></pre>"},{"location":"cli/k0s_kubeconfig_create/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n--data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n--debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n-l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_kubeconfig_create/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s kubeconfig - Create a kubeconfig file for a specified user</li> </ul>"},{"location":"cli/k0s_start/","title":"K0s start","text":""},{"location":"cli/k0s_start/#k0s-start","title":"k0s start","text":"<p>Start the k0s service after it has been installed using <code>k0s install</code>. Must be run as root (or with sudo),</p>"},{"location":"cli/k0s_start/#options","title":"Options","text":"<pre><code>  -h, --help   help for start\n</code></pre>"},{"location":"cli/k0s_start/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -d, --debug                    Debug logging (default: false)\n</code></pre>"},{"location":"cli/k0s_start/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s stop - Stop the k0s service after it has been installed using <code>k0s install</code>. Must be run as root (or with sudo)</li> <li>k0s install - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo)</li> <li>k0s install controller - Helper command for setting up k0s as controller node on a brand-new system. Must be run as root (or with sudo)</li> <li>k0s install worker - Helper command for setting up k0s as a worker node on a brand-new system. Must be run as root (or with sudo)</li> </ul>"},{"location":"cli/k0s_status/","title":"K0s status","text":""},{"location":"cli/k0s_status/#k0s-status","title":"k0s status","text":"<p>Helper command for get general information about k0s</p> <pre><code>k0s status [flags]\n</code></pre>"},{"location":"cli/k0s_status/#synopsis","title":"Synopsis","text":"<p>The command will return information about system init, PID, k0s role, kubeconfig and similar.</p>"},{"location":"cli/k0s_status/#options","title":"Options","text":"<pre><code>  -h, --help         help for status\n  -o, --out string   sets type of output to json or yaml\n</code></pre>"},{"location":"cli/k0s_status/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n--data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n--debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n-l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_status/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s - k0s - Zero Friction Kubernetes</li> </ul>"},{"location":"cli/k0s_stop/","title":"K0s stop","text":""},{"location":"cli/k0s_stop/#k0s-stop","title":"k0s stop","text":"<p>Stop the k0s service after it has been installed using <code>k0s install</code>. Must be run as root (or with sudo).</p>"},{"location":"cli/k0s_stop/#options","title":"Options","text":"<pre><code>  -h, --help   help for start\n</code></pre>"},{"location":"cli/k0s_stop/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -d, --debug                    Debug logging (default: false)\n</code></pre>"},{"location":"cli/k0s_stop/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s start - Start the k0s service after it has been installed using <code>k0s install</code>. Must be run as root (or with sudo)</li> <li>k0s install - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo)</li> <li>k0s install controller - Helper command for setting up k0s as controller node on a brand-new system. Must be run as root (or with sudo)</li> <li>k0s install worker - Helper command for setting up k0s as a worker node on a brand-new system. Must be run as root (or with sudo)</li> </ul>"},{"location":"cli/k0s_token/","title":"K0s token","text":""},{"location":"cli/k0s_token/#k0s-token","title":"k0s token","text":"<p>Manage join tokens</p> <pre><code>k0s token [flags]\n</code></pre>"},{"location":"cli/k0s_token/#options","title":"Options","text":"<pre><code>  -h, --help                help for token\n      --kubeconfig string   path to kubeconfig file [$KUBECONFIG]\n</code></pre>"},{"location":"cli/k0s_token/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n--data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n--debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n-l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_token/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s - k0s - Zero Friction Kubernetes</li> <li>k0s token create - Create join token</li> </ul>"},{"location":"cli/k0s_token_create/","title":"K0s token create","text":""},{"location":"cli/k0s_token_create/#k0s-token-create","title":"k0s token create","text":"<p>Create join token</p> <pre><code>k0s token create [flags]\n</code></pre>"},{"location":"cli/k0s_token_create/#options","title":"Options","text":"<pre><code>      --expiry string   set duration time for token (default \"0\")\n-h, --help            help for create\n      --role string     Either worker or controller (default \"worker\")\n--wait            wait forever (default false)\n</code></pre>"},{"location":"cli/k0s_token_create/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n--data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n--debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n-l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_token_create/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s token - Manage join tokens</li> </ul>"},{"location":"cli/k0s_validate/","title":"K0s validate","text":""},{"location":"cli/k0s_validate/#k0s-validate","title":"k0s validate","text":"<p>Helper command for validating the config file</p>"},{"location":"cli/k0s_validate/#options","title":"Options","text":"<pre><code>  -h, --help   help for validate\n</code></pre>"},{"location":"cli/k0s_validate/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n--data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n--debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n-l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_validate/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s - k0s - Zero Friction Kubernetes</li> <li>k0s validate config - Helper command for validating the config file</li> </ul>"},{"location":"cli/k0s_validate_config/","title":"K0s validate config","text":""},{"location":"cli/k0s_validate_config/#k0s-validate-config","title":"k0s validate config","text":"<p>Helper command for validating the config file</p> <pre><code>k0s validate config [flags]\n</code></pre>"},{"location":"cli/k0s_validate_config/#examples","title":"Examples","text":"<pre><code> k0s validate config --config path_to_config.yaml\n</code></pre>"},{"location":"cli/k0s_validate_config/#options","title":"Options","text":"<pre><code>  -h, --help   help for config\n</code></pre>"},{"location":"cli/k0s_validate_config/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n--data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n--debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n-l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_validate_config/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s validate - Helper command for validating the config file</li> </ul>"},{"location":"cli/k0s_version/","title":"K0s version","text":""},{"location":"cli/k0s_version/#k0s-version","title":"k0s version","text":"<p>Print the k0s version</p> <pre><code>k0s version [flags]\n</code></pre>"},{"location":"cli/k0s_version/#options","title":"Options","text":"<pre><code>  -h, --help   help for version\n</code></pre>"},{"location":"cli/k0s_version/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n--data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n--debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n-l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_version/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s - k0s - Zero Friction Kubernetes</li> </ul>"},{"location":"cli/k0s_worker/","title":"K0s worker","text":""},{"location":"cli/k0s_worker/#k0s-worker","title":"k0s worker","text":"<p>Run worker</p> <pre><code>k0s worker [join-token] [flags]\n</code></pre>"},{"location":"cli/k0s_worker/#examples","title":"Examples","text":"<p>Command to add worker node to the master node using a CLI argument:</p> <pre><code>k0s worker [token]\n</code></pre> <p>or a CLI flag:</p> <pre><code>k0s worker --token-file [path_to_file]\n</code></pre>"},{"location":"cli/k0s_worker/#options","title":"Options","text":"<pre><code>      --api-server string       HACK: api-server for the windows worker node\n      --cidr-range string       HACK: cidr range for the windows worker node (default \"10.96.0.0/12\")\n--cluster-dns string      HACK: cluster dns for the windows worker node (default \"10.96.0.10\")\n--cri-socket string       contrainer runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket]\n--enable-cloud-provider   Whether or not to enable cloud provider support in kubelet\n  -h, --help                    help for worker\n      --profile string          worker profile to use on the node (default \"default\")\n--token-file string       Path to the file containing token.\n</code></pre>"},{"location":"cli/k0s_worker/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n--data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n--debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n-l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_worker/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s - k0s - Zero Friction Kubernetes</li> </ul>"},{"location":"contributors/CODE_OF_CONDUCT/","title":"k0s Community Code of Conduct","text":"<p>k0s follows the CNCF Code of Conduct.</p>"},{"location":"contributors/github_workflow/","title":"Github Workflow","text":"<p>This guide assumes you have already cloned the upstream repo to your system via git clone, or via <code>go get github.com/k0sproject/k0s</code>.</p>"},{"location":"contributors/github_workflow/#fork-the-project","title":"Fork The Project","text":"<ol> <li>Go to http://github.com/k0sproject/k0s</li> <li>On the top, right-hand side, click on \"fork\" and select your username for the fork destination.</li> </ol>"},{"location":"contributors/github_workflow/#adding-the-forked-remote","title":"Adding the Forked Remote","text":"<pre><code>export GITHUB_USER={ your github's username }\n</code></pre> <pre><code>cd $WORKDIR/k0s\ngit remote add $GITHUB_USER git@github.com:${GITHUB_USER}/k0s.git\n\n# Prevent push to Upstream\ngit remote set-url --push origin no_push\n\n# Set your fork remote as a default push target\ngit push --set-upstream $GITHUB_USER main\n</code></pre> <p>Your remotes should look something like this:</p> <pre><code>$ git remote -v\norigin  https://github.com/k0sproject/k0s (fetch)\norigin  no_push (push)\nmy_fork git@github.com:{ github_username }/k0s.git (fetch)\nmy_fork git@github.com:{ github_username }/k0s.git (push)\n</code></pre>"},{"location":"contributors/github_workflow/#create-rebase-your-feature-branch","title":"Create &amp; Rebase Your Feature Branch","text":"<p>Create a feature branch and switch to it:</p> <pre><code>git checkout -b my_feature_branch\n</code></pre> <p>Rebase your branch:</p> <pre><code>$ git fetch origin\n$ git rebase origin/main\nCurrent branch my_feature_branch is up to date.\n</code></pre> <p>Please don't use <code>git pull</code> instead of the above <code>fetch / rebase</code>. <code>git pull</code> does a merge, which leaves merge commits. These make the commit history messy and violate the principle that commits ought to be individually understandable and useful.</p>"},{"location":"contributors/github_workflow/#commit-push","title":"Commit &amp; Push","text":"<p>Commit and sign your changes:</p> <pre><code>git commit --signoff\n</code></pre> <p>The commit message should have a short title as first line, an empty line and then a longer description that explains why the change was made, unless it is obvious.</p> <p>You can go back and edit/build/test some more, then <code>commit --amend</code> in a few cycles.</p> <p>When ready, push your changes to your fork's repository:</p> <pre><code>git push --set-upstream my_fork my_feature_branch\n</code></pre>"},{"location":"contributors/github_workflow/#open-a-pull-request","title":"Open a Pull Request","text":"<p>Github Docs</p>"},{"location":"contributors/github_workflow/#get-a-code-review","title":"Get a code review","text":"<p>Once your pull request has been opened it will be assigned to one or more reviewers, and will go through a series of smoke tests.</p> <p>Commit changes made in response to review comments should be added to the same branch on your fork.</p> <p>Very small PRs are easy to review. Very large PRs are very difficult to review.</p>"},{"location":"contributors/github_workflow/#squashing-commits","title":"Squashing Commits","text":"<p>Commits on your branch should represent meaningful milestones or units of work. Small commits that contain typo fixes, rebases, review feedbacks, etc should be squashed.</p> <p>To do that, it's best to perform an interactive rebase:</p>"},{"location":"contributors/github_workflow/#example","title":"Example","text":"<p>Rebase your feature branch against upstream main branch:</p> <pre><code>git rebase -i origin/main\n</code></pre> <p>If your PR has 3 commits, output would be similar to this:</p> <pre><code>pick f7f3f6d Changed some code\npick 310154e fixed some typos\npick a5f4a0d made some review changes\n\n# Rebase 710f0f8..a5f4a0d onto 710f0f8\n#\n# Commands:\n# p, pick &lt;commit&gt; = use commit\n# r, reword &lt;commit&gt; = use commit, but edit the commit message\n# e, edit &lt;commit&gt; = use commit, but stop for amending\n# s, squash &lt;commit&gt; = use commit, but meld into previous commit\n# f, fixup &lt;commit&gt; = like \"squash\", but discard this commit's log message\n# x, exec &lt;command&gt; = run command (the rest of the line) using shell\n# b, break = stop here (continue rebase later with 'git rebase --continue')\n# d, drop &lt;commit&gt; = remove commit\n# l, label &lt;label&gt; = label current HEAD with a name\n# t, reset &lt;label&gt; = reset HEAD to a label\n# m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;]\n# .       create a merge commit using the original merge commit's\n# .       message (or the oneline, if no original merge commit was\n# .       specified). Use -c &lt;commit&gt; to reword the commit message.\n#\n# These lines can be re-ordered; they are executed from top to bottom.\n#\n# However, if you remove everything, the rebase will be aborted.\n#\n# Note that empty commits are commented out\n</code></pre> <p>Use a command line text editor to change the word <code>pick</code> to <code>f</code> of <code>fixup</code> for the commits you want to squash, then save your changes and continue the rebase:</p> <p>Per the output above, you can see that:</p> <pre><code>fixup &lt;commit&gt; = like \"squash\", but discard this commit's log message\n</code></pre> <p>Which means that when rebased, the commit message \"fixed some typos\" will be removed, and squashed with the parent commit.</p>"},{"location":"contributors/github_workflow/#push-your-final-changes","title":"Push Your Final Changes","text":"<p>Once done, you can push the final commits to your branch:</p> <pre><code>git push --force\n</code></pre> <p>You can run multiple iteration of <code>rebase</code>/<code>push -f</code>, if needed.</p>"},{"location":"contributors/overview/","title":"Contributing to k0s","text":"<p>Thank you for taking the time to make a contribution to k0s. The following document is a set of guidelines and instructions for contributing to k0s.</p> <p>When contributing to this repository, please consider first discussing the change you wish to make by opening an issue.</p>"},{"location":"contributors/overview/#code-of-conduct","title":"Code of Conduct","text":"<p>Our code of conduct can be found in the link below. Please follow it in all your interactions with the project.</p> <ul> <li>Code Of Conduct</li> </ul>"},{"location":"contributors/overview/#github-workflow","title":"Github Workflow","text":"<p>We Use Github Flow, so all code changes are tracked via Pull Requests. A detailed guide on the recommended workflow can be found below:</p> <ul> <li>Github Workflow</li> </ul>"},{"location":"contributors/overview/#code-testing","title":"Code Testing","text":"<p>All submitted PRs go through a set of tests and reviews. You can run most of these tests before a PR is submitted. In fact, we recommend it, because it will save on many possible review iterations and automated tests. The testing guidelines can be found here:</p> <ul> <li>Contributor's Guide to Testing</li> </ul>"},{"location":"contributors/overview/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed as followed:</p> <ul> <li>All content residing under the \"docs/\" directory of this repository is licensed under \"Creative Commons Attribution Share Alike 4.0 International\" (CC-BY-SA-4.0). See docs/LICENCE for details.</li> <li>Content outside of the above mentioned directories or restrictions above is available under the \"Apache License 2.0\".</li> </ul>"},{"location":"contributors/overview/#community","title":"Community","text":"<p>Some of you might have noticed we have official community blog hosted on Medium. If you are not yet following us, we'd like to invite you to do so now! Make sure to follow us on Twitter as well \ud83d\ude0a We have also agreed to share Slack with Lens IDE team. They are close friends of the k0s crew, and many of the Kubernetes users are using Lens anyway, so it felt very natural for us to share their Slack.</p> <p>You\u2019ll find us on dedicated k0s channel. Please join the k0s Slack channel to hear the latest news, discussions and provide your feedback!</p>"},{"location":"contributors/testing/","title":"Testing","text":""},{"location":"contributors/testing/#testing-your-code","title":"Testing Your Code","text":"<p>k0s uses github actions to run automated tests on any PR, before merging. However, a PR will not be reviewed before all tests are green, so to save time and prevent your PR from going stale, it is best to test it before submitting the PR.</p>"},{"location":"contributors/testing/#run-local-verifications","title":"Run Local Verifications","text":"<p>Please run the following style and formatting commands and fix/check-in any changes:</p> <ol> <li> <p>Linting</p> <p>We use golangci-lint for style verification.   In the repository's root directory, simply run:</p> <pre><code>make lint\n</code></pre> </li> <li> <p>Go fmt</p> <pre><code>go fmt ./...\n</code></pre> </li> <li> <p>Pre-submit Flight Checks</p> <p>In the repository root directory, make sure that:</p> <p>* <code>make build</code> runs successfully.   * <code>make check-basic</code> runs successfully.   * <code>make check-unit</code> has no errors.   * <code>make check-hacontrolplane</code> runs successfully.</p> <p>Please note that this last test is prone to \"flakiness\", so it might fail on occasion. If it fails constantly, take a deeper look at your code to find the source of the problem.</p> <p>If you find that all tests passed, you may open a pull request upstream.</p> </li> </ol>"},{"location":"contributors/testing/#opening-a-pull-request","title":"Opening A Pull Request","text":""},{"location":"contributors/testing/#draft-mode","title":"Draft Mode","text":"<p>You may open a pull request in draft mode. All automated tests will still run against the PR, but the PR will not be assigned for review. Once a PR is ready for review, transition it from Draft mode, and code owners will be notified.</p>"},{"location":"contributors/testing/#conformance-testing","title":"Conformance Testing","text":"<p>Once a PR has been reviewed and all other tests have passed, a code owner will run a full end-to-end conformance test against the PR. This is usually the last step before merging.</p>"},{"location":"contributors/testing/#pre-requisites-for-pr-merge","title":"Pre-Requisites for PR Merge","text":"<p>In order for a PR to be merged, the following conditions should exist:</p> <ol> <li>The PR has passed all the automated tests (style, build &amp; conformance tests).</li> <li>PR commits have been signed with the <code>--signoff</code> option.</li> <li>PR was reviewed and approved by a code owner.</li> <li>PR is rebased against upstream's main branch.</li> </ol>"},{"location":"examples/ambassador-ingress/","title":"Install Ambassador Gateway on k0s","text":"<p>You can configure k0s with the Ambassador API Gateway and a MetalLB service loadbalancer. To do this you leverage Helm's extensible bootstrapping functionality to add the correct extensions to the <code>k0s.yaml</code> file during cluster configuration.</p>"},{"location":"examples/ambassador-ingress/#use-docker-for-non-native-k0s-platforms","title":"Use Docker for non-native k0s platforms","text":"<p>With Docker you can run k0s on platforms that the distribution does not natively support (refer to Run k0s in Docker). Skip this section if you are on a platform that k0s natively supports.</p> <p>As you need to create a custom configuration file to install Ambassador Gateway, you will first need to map that file into the k0s container and to expose the ports Ambassador needs for outside access.</p> <ol> <li> <p>Run k0s under Docker:</p> <pre><code>docker run -d --name k0s --hostname k0s --privileged -v /var/lib/k0s -p 6443:6443 docker.io/k0sproject/k0s:latest\n</code></pre> </li> <li> <p>Export the default k0s configuration file:</p> <pre><code>docker exec k0s k0s default-config &gt; k0s.yaml\n</code></pre> </li> <li> <p>Export the cluster config, so you can access it using kubectl:</p> <pre><code>docker exec k0s cat /var/lib/k0s/pki/admin.conf &gt; k0s-cluster.conf\nexport KUBECONFIG=$KUBECONFIG:&lt;absolute path to k0s-cluster.conf&gt;\n</code></pre> </li> </ol>"},{"location":"examples/ambassador-ingress/#configure-k0syaml-for-ambassador-gateway","title":"Configure <code>k0s.yaml</code> for Ambassador Gateway","text":"<ol> <li> <p>Open the <code>k0s.yml</code> file and append the following extensions at the end:</p> <pre><code>extensions:\nhelm:\nrepositories:\n- name: datawire\nurl: https://www.getambassador.io\n- name: bitnami\nurl: https://charts.bitnami.com/bitnami\ncharts:\n- name: ambassador\nchartname: datawire/ambassador\nversion: \"6.5.13\"\nnamespace: ambassador\nvalues: |2\nservice:\nexternalIPs:\n- 172.17.0.2\n- name: metallb\nchartname: bitnami/metallb\nversion: \"1.0.1\"\nnamespace: default\nvalues: |2\nconfigInline:\naddress-pools:\n- name: generic-cluster-pool\nprotocol: layer2\naddresses:\n- 172.17.0.2\n</code></pre> <p>Note: It may be necessary to replace the 172.17.0.2 IP with your local IP address.</p> <p>This action adds both Ambassador and MetalLB (required for LoadBalancers) with the corresponding repositories and (minimal) configurations. Be aware that the provided example illustrates the use of your local network and that you will want to provide a range of IPs for MetalLB that are addressable on your LAN to access these services from anywhere on your network.</p> </li> <li> <p>Stop/remove your k0s container:</p> <pre><code>docker stop k0s\ndocker rm k0s\n</code></pre> </li> <li> <p>Retart your k0s container, this time with additional ports and the above config file mapped into it:</p> <pre><code>docker run --name k0s --hostname k0s --privileged -v /var/lib/k0s -v &lt;path to k0s.yaml file&gt;:/k0s.yaml -p 6443:6443 -p 80:80 -p 443:443 -p 8080:8080 docker.io/k0sproject/k0s:latest\n</code></pre> <p>After some time, you will be able to list the Ambassador Services:</p> <pre><code>kubectl get services -n ambassador\n</code></pre> <p>Output:</p> <pre><code>NAME                          TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE\nambassador-1611224811         LoadBalancer   10.99.84.151    172.17.0.2    80:30327/TCP,443:30355/TCP   2m11s\nambassador-1611224811-admin   ClusterIP      10.96.79.130    &lt;none&gt;        8877/TCP                     2m11s\nambassador-1611224811-redis   ClusterIP      10.110.33.229   &lt;none&gt;        6379/TCP                     2m11s\n</code></pre> </li> <li> <p>Install the Ambassador edgectl tool and run the login command:</p> <pre><code>edgectl login --namespace=ambassador localhost\n</code></pre> <p>Your browser will open and deeliver you to the Ambassador Console.</p> </li> </ol>"},{"location":"examples/ambassador-ingress/#deploy-map-a-service","title":"Deploy / Map a Service","text":"<ol> <li> <p>Create a YAML file for the service (for example purposes, create a Swagger Petstore service using a petstore.YAML file):</p> <pre><code>---\napiVersion: v1\nkind: Service\nmetadata:\nname: petstore\nnamespace: ambassador\nspec:\nports:\n- name: http\nport: 80\ntargetPort: 8080\nselector:\napp: petstore\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: petstore\nnamespace: ambassador\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: petstore\nstrategy:\ntype: RollingUpdate\ntemplate:\nmetadata:\nlabels:\napp: petstore\nspec:\ncontainers:\n- name: petstore-backend\nimage: docker.io/swaggerapi/petstore3:unstable\nports:\n- name: http\ncontainerPort: 8080\n---\napiVersion: getambassador.io/v2\nkind:  Mapping\nmetadata:\nname: petstore\nnamespace: ambassador\nspec:\nprefix: /petstore/\nservice: petstore\n</code></pre> </li> <li> <p>Apply the YAML file:</p> <pre><code>kubectl apply -f petstore.yaml\n</code></pre> <p>Output:</p> <pre><code>service/petstore created\ndeployment.apps/petstore created\nmapping.getambassador.io/petstore created\n</code></pre> </li> <li> <p>Validate that the service is running.</p> <p>In the terminal using curl:</p> <pre><code>curl -k 'https://localhost/petstore/api/v3/pet/findByStatus?status=available'\n</code></pre> <p>Output:</p> <pre><code>[{\"id\":1,\"category\":{\"id\":2,\"name\":\"Cats\"},\"name\":\"Cat 1\",\"photoUrls\":[\"url1\",\"url2\"],\"tags\":[{\"id\":1,\"name\":\"tag1\"},{\"id\":2,\"name\":\"tag2\"}],\"status\":\"available\"},{\"id\":2,\"category\":{\"id\":2,\"name\":\"Cats\"},\"name\":\"Cat 2\",\"photoUrls\":[\"url1\",\"url2\"],\"tags\":[{\"id\":1,\"name\":\"tag2\"},{\"id\":2,\"name\":\"tag3\"}],\"status\":\"available\"},{\"id\":4,\"category\":{\"id\":1,\"name\":\"Dogs\"},\"name\":\"Dog 1\",\"photoUrls\":[\"url1\",\"url2\"],\"tags\":[{\"id\":1,\"name\":\"tag1\"},{\"id\":2,\"name\":\"tag2\"}],\"status\":\"available\"},{\"id\":7,\"category\":{\"id\":4,\"name\":\"Lions\"},\"name\":\"Lion 1\",\"photoUrls\":[\"url1\",\"url2\"],\"tags\":[{\"id\":1,\"name\":\"tag1\"},{\"id\":2,\"name\":\"tag2\"}],\"status\":\"available\"},{\"id\":8,\"category\":{\"id\":4,\"name\":\"Lions\"},\"name\":\"Lion 2\",\"photoUrls\":[\"url1\",\"url2\"],\"tags\":[{\"id\":1,\"name\":\"tag2\"},{\"id\":2,\"name\":\"tag3\"}],\"status\":\"available\"},{\"id\":9,\"category\":{\"id\":4,\"name\":\"Lions\"},\"name\":\"Lion 3\",\"photoUrls\":[\"url1\",\"url2\"],\"tags\":[{\"id\":1,\"name\":\"tag3\"},{\"id\":2,\"name\":\"tag4\"}],\"status\":\"available\"},{\"id\":10,\"category\":{\"id\":3,\"name\":\"Rabbits\"},\"name\":\"Rabbit 1\",\"photoUrls\":[\"url1\",\"url2\"],\"tags\":[{\"id\":1,\"name\":\"tag3\"},{\"id\":2,\"name\":\"tag4\"}],\"status\":\"available\"}]\n</code></pre> <p>Or by way of your browser:</p> <p>Open https://localhost/petstore/ in your browser and change the URL in the field at the top of the page to https://localhost/petstore/api/v3/openapi.json (as it is mapped to the /petstore prefix) and click Explore.</p> </li> <li> <p>Navigate to the Mappings area in the Ambassador Console to view the corresponding PetStore mapping as configured.</p> </li> </ol>"},{"location":"examples/ansible-playbook/","title":"Creating a cluster with an Ansible Playbook","text":"<p>Ansible is a popular infrastructure-as-code tool that can use to automate tasks for the purpose of achieving the desired state in a system. With Ansible (and the k0s-Ansible playbook) you can quickly install a multi-node Kubernetes Cluster.</p> <p>Note: Before using Ansible to create a cluster, you should have a general understanding of Ansible (refer to the official Ansible User Guide.</p>"},{"location":"examples/ansible-playbook/#prerequisites","title":"Prerequisites","text":"<p>You will require the following tools to install k0s on local virtual machines:</p> Tool Detail <code>multipass</code> A lightweight VM manager that uses KVM on Linux, Hyper-V on Windows, and hypervisor.framework on macOS. Installation information <code>ansible</code> An infrastructure as code tool. Installation Guide <code>kubectl</code> Command line tool for running commands against Kubernetes clusters.  Kubernetes Install Tools"},{"location":"examples/ansible-playbook/#create-the-cluster","title":"Create the cluster","text":"<ol> <li> <p>Download k0s-ansible</p> <p>Clone the k0s-ansible repository on your local machine:</p> <pre><code>git clone https://github.com/movd/k0s-ansible.git\ncd k0s-ansible\n</code></pre> </li> <li> <p>Create virtual machines</p> <p>Note: Though multipass is the VM manager in use here, there is no interdependence.</p> <p>Create a number of virtual machines. For the automation to work, each instance must have passwordless SSH access. To achieve this, provision each instance with a cloud-init manifest that imports your current users' public SSH key and into a user <code>k0s</code> (refer to the bash script below).</p> <p>This creates 7 virtual machines:</p> <pre><code>$ ./tools/multipass_create_instances.sh 7\nCreate cloud-init to import ssh key...\n[1/7] Creating instance k0s-1 with multipass...\nLaunched: k0s-1\n[2/7] Creating instance k0s-2 with multipass...\nLaunched: k0s-2\n[3/7] Creating instance k0s-3 with multipass...\nLaunched: k0s-3\n[4/7] Creating instance k0s-4 with multipass...\nLaunched: k0s-4\n[5/7] Creating instance k0s-5 with multipass...\nLaunched: k0s-5\n[6/7] Creating instance k0s-6 with multipass...\nLaunched: k0s-6\n[7/7] Creating instance k0s-7 with multipass...\nLaunched: k0s-7\nName State IPv4 Image\nk0s-1 Running 192.168.64.32 Ubuntu 20.04 LTS\nk0s-2 Running 192.168.64.33 Ubuntu 20.04 LTS\nk0s-3 Running 192.168.64.56 Ubuntu 20.04 LTS\nk0s-4 Running 192.168.64.57 Ubuntu 20.04 LTS\nk0s-5 Running 192.168.64.58 Ubuntu 20.04 LTS\nk0s-6 Running 192.168.64.60 Ubuntu 20.04 LTS\nk0s-7 Running 192.168.64.61 Ubuntu 20.04 LTS\n</code></pre> </li> <li> <p>Create Ansible inventory</p> <p>1. Copy the sample to create the inventory directory:</p> <pre><code>  ```shell\n  cp -rfp inventory/sample inventory/multipass\n  ```\n</code></pre> <p>2. Create the inventory.</p> <pre><code>  Assign the virtual machines to the different host groups, as required by the playbook logic.\n\n  | Host group            | Detail                                    |\n  |:----------------------|:------------------------------------------|\n  | `initial_controller`  | Must contain a single node that creates the worker and controller tokens needed by the other nodes|\n  | `controller`          | Can contain nodes that, together with the host from `initial_controller`, form a highly available isolated control plane |\n  | `worker`              | Must contain at least one node, to allow for the deployment of Kubernetes objects |\n</code></pre> <p>3. Fill in <code>inventory/multipass/inventory.yml</code>. This can be done by direct entry using the metadata provided by <code>multipass list,</code>, or you can use the following Python script <code>multipass_generate_inventory.py</code>:</p> <pre><code>  ```shell\n  $ ./tools/multipass_generate_inventory.py\n  Designate first three instances as control plane\n  Created Ansible Inventory at: /Users/dev/k0s-ansible/tools/inventory.yml\n  $ cp tools/inventory.yml inventory/multipass/inventory.yml\n  ```\n\n  Your `inventory/multipass/inventory.yml` should resemble the example below:\n\n  ```yaml\n  ---\n  all:\n    children:\n      initial_controller:\n        hosts:\n          k0s-1:\n      controller:\n        hosts:\n          k0s-2:\n          k0s-3:\n      worker:\n        hosts:\n          k0s-4:\n          k0s-5:\n          k0s-6:\n          k0s-7:\n    hosts:\n      k0s-1:\n        ansible_host: 192.168.64.32\n      k0s-2:\n        ansible_host: 192.168.64.33\n      k0s-3:\n        ansible_host: 192.168.64.56\n      k0s-4:\n        ansible_host: 192.168.64.57\n      k0s-5:\n        ansible_host: 192.168.64.58\n      k0s-6:\n        ansible_host: 192.168.64.60\n      k0s-7:\n        ansible_host: 192.168.64.61\n    vars:\n      ansible_user: k0s\n  ```\n</code></pre> </li> <li> <p>Test the virtual machine connections</p> <p>Run the following command to test the connection to your hosts:</p> <pre><code>$ ansible -i inventory/multipass/inventory.yml -m ping\nk0s-4 | SUCCESS =&gt; {\n\"ansible_facts\": {\n\"discovered_interpreter_python\": \"/usr/bin/python3\"\n},\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n...\n</code></pre> <p>If the test result indicates success, you can proceed.</p> </li> <li> <p>Provision the cluster with Ansible</p> <p>Applying the playbook, k0s download and be set up on all nodes, tokens will be exchanged, and a kubeconfig will be dumped to your local deployment environment.</p> <pre><code>$ ansible-playbook site.yml -i inventory/multipass/inventory.yml\nTASK [k0s/initial_controller : print kubeconfig command] *******************************************************\nTuesday 22 December 2020  17:43:20 +0100 (0:00:00.257)       0:00:41.287 ******\nok: [k0s-1] =&gt; {\n\"msg\": \"To use Cluster: export KUBECONFIG=/Users/dev/k0s-ansible/inventory/multipass/artifacts/k0s-kubeconfig.yml\"\n}\n...\nPLAY RECAP *****************************************************************************************************\nk0s-1                      : ok=21   changed=11   unreachable=0    failed=0    skipped=1    rescued=0    ignored=0\nk0s-2                      : ok=10   changed=5    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0\nk0s-3                      : ok=10   changed=5    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0\nk0s-4                      : ok=9    changed=5    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0\nk0s-5                      : ok=9    changed=5    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0\nk0s-6                      : ok=9    changed=5    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0\nk0s-7                      : ok=9    changed=5    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0\n\nTuesday 22 December 2020  17:43:36 +0100 (0:00:01.204)       0:00:57.478 ******\n===============================================================================\nprereq : Install apt packages -------------------------------------------------------------------------- 22.70s\nk0s/controller : Wait for k8s apiserver ----------------------------------------------------------------- 4.30s\nk0s/initial_controller : Create worker join token ------------------------------------------------------- 3.38s\nk0s/initial_controller : Wait for k8s apiserver --------------------------------------------------------- 3.36s\ndownload : Download k0s binary k0s-v0.9.0-rc1-amd64 ----------------------------------------------------- 3.11s\nGathering Facts ----------------------------------------------------------------------------------------- 2.85s\nGathering Facts ----------------------------------------------------------------------------------------- 1.95s\nprereq : Create k0s Directories ------------------------------------------------------------------------- 1.53s\nk0s/worker : Enable and check k0s service --------------------------------------------------------------- 1.20s\nprereq : Write the k0s config file ---------------------------------------------------------------------- 1.09s\nk0s/initial_controller : Enable and check k0s service --------------------------------------------------- 0.94s\nk0s/controller : Enable and check k0s service ----------------------------------------------------------- 0.73s\nGathering Facts ----------------------------------------------------------------------------------------- 0.71s\nGathering Facts ----------------------------------------------------------------------------------------- 0.66s\nGathering Facts ----------------------------------------------------------------------------------------- 0.64s\nk0s/worker : Write the k0s token file on worker --------------------------------------------------------- 0.64s\nk0s/worker : Copy k0s service file ---------------------------------------------------------------------- 0.53s\nk0s/controller : Write the k0s token file on controller ------------------------------------------------- 0.41s\nk0s/controller : Copy k0s service file ------------------------------------------------------------------ 0.40s\nk0s/initial_controller : Copy k0s service file ---------------------------------------------------------- 0.36s\n</code></pre> </li> </ol>"},{"location":"examples/ansible-playbook/#use-the-cluster-with-kubectl","title":"Use the cluster with kubectl","text":"<p>A kubeconfig was copied to your local machine while the playbook was running which you can use to gain access to your new Kubernetes cluster:</p> <pre><code>$ export KUBECONFIG=/Users/dev/k0s-ansible/inventory/multipass/artifacts/k0s-kubeconfig.yml\n$ kubectl cluster-info\nKubernetes control plane is running at https://192.168.64.32:6443\nCoreDNS is running at https://192.168.64.32:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\nMetrics-server is running at https://192.168.64.32:6443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\n\n$ kubectl get nodes -o wide\nNAME    STATUS     ROLES    AGE   VERSION        INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME\nk0s-4   Ready      &lt;none&gt;   21s   v1.20.1-k0s1   192.168.64.57   &lt;none&gt;        Ubuntu 20.04.1 LTS   5.4.0-54-generic   containerd://1.4.3\nk0s-5   Ready      &lt;none&gt;   21s   v1.20.1-k0s1   192.168.64.58   &lt;none&gt;        Ubuntu 20.04.1 LTS   5.4.0-54-generic   containerd://1.4.3\nk0s-6   NotReady   &lt;none&gt;   21s   v1.20.1-k0s1   192.168.64.60   &lt;none&gt;        Ubuntu 20.04.1 LTS   5.4.0-54-generic   containerd://1.4.3\nk0s-7   NotReady   &lt;none&gt;   21s   v1.20.1-k0s1   192.168.64.61   &lt;none&gt;        Ubuntu 20.04.1 LTS   5.4.0-54-generic   containerd://1.4.3\n</code></pre> <p>Note: The first three control plane nodes will not display, as the control plane is fully isolated. To check on the distributed etcd cluster, you can use ssh to securely log a controller node, or you can run the following ad-hoc command:</p> <pre><code>$ ansible k0s-1 -a \"k0s etcd member-list -c /etc/k0s/k0s.yaml\" -i inventory/multipass/inventory.yml | tail -1 | jq\n{\n\"level\": \"info\",\n  \"members\": {\n\"k0s-1\": \"https://192.168.64.32:2380\",\n    \"k0s-2\": \"https://192.168.64.33:2380\",\n    \"k0s-3\": \"https://192.168.64.56:2380\"\n},\n  \"msg\": \"done\",\n  \"time\": \"2020-12-23T00:21:22+01:00\"\n}\n</code></pre> <p>Once all worker nodes are at <code>Ready</code> state you can use the cluster. You can test the cluster state by creating a simple nginx deployment.</p> <pre><code>$ kubectl create deployment nginx --image=gcr.io/google-containers/nginx --replicas=5\ndeployment.apps/nginx created\n\n$ kubectl expose deployment nginx --target-port=80 --port=8100\nservice/nginx exposed\n\n$ kubectl run hello-k0s --image=quay.io/prometheus/busybox --rm -it --restart=Never --command -- wget -qO- nginx:8100\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx on Debian!&lt;/title&gt;\n...\npod \"hello-k0s\" deleted\n</code></pre> <p>Note: k0s users are the developers of k0s-ansible. Please send your feedback, bug reports, and pull requests to github.com/movd/k0s-ansible._</p>"},{"location":"examples/rook-ceph/","title":"Installing Ceph Storage with Rook","text":"<p>In this tutorial you'll create a Ceph storage for k0s. Ceph is a highly scalable, distributed storage solution. It offers object, block, and file storage, and it's designed to run on any common hardware. Ceph implements data replication into multiple volumes that makes it fault-tolerant. Another clear advantage of Ceph in Kubernetes is the dynamic provisioning. This means that applications just need to request the storage (persistent volume claim) and Ceph will automatically provision the requested storage without a manual creation of the persistent volume each time.</p> <p>Unfortunately, the Ceph deployment as such can be considered a bit complex. To make the deployment easier, we'll use Rook operator. Rook is a CNCF project and it's dedicated to storage orchestration. Rook supports several storage solutions, but in this tutorial we will use it to manage Ceph.</p> <p>This tutorial uses three worker nodes and one controller. It's possible to use less nodes, but using three worker nodes makes it a good example for deploying a high-available storage cluster. We use external storage partitions, which are assigned to the worker nodes to be used by Ceph.</p> <p>After the Ceph deployment we'll deploy a sample application (MongoDB) to use the storage in practice.</p> <p></p>"},{"location":"examples/rook-ceph/#prerequisites","title":"Prerequisites","text":"<ul> <li>Linux OS</li> <li>GitHub access</li> <li>AWS account</li> <li>Terraform</li> </ul>"},{"location":"examples/rook-ceph/#deployment-steps","title":"Deployment steps","text":""},{"location":"examples/rook-ceph/#1-preparations","title":"1. Preparations","text":"<p>In this example we'll use Terraform to create four Ubuntu VMs on AWS. Using Terraform makes the VM deployment fast and repeatable. You can avoid manually setting up everything in the AWS GUI. Moreover, when you have finished with the tutorial, it's very easy to tear down the VMs with Terraform (with one command). However, you can set up the nodes in many different ways and it doesn't make a difference in the following steps.</p> <p>We will use k0sctl to create the k0s cluster. k0sctl repo also includes a ready-made Terraform configuration to create the VMs on AWS. We'll use that. Let's start be cloning the k0sctl repo.</p> <pre><code>git clone git@github.com:k0sproject/k0sctl.git\n</code></pre> <p>Take a look at the Terraform files</p> <pre><code>cd k0sctl/examples/aws-tf\nls -l\n</code></pre> <p>Open <code>variables.tf</code> and set the number of controller and worker nodes like this:</p> <pre><code>variable \"cluster_name\" {\ntype    = string\ndefault = \"k0sctl\"\n}\n\nvariable \"controller_count\" {\ntype    = number\ndefault = 1\n}\n\nvariable \"worker_count\" {\ntype    = number\ndefault = 3\n}\n\nvariable \"cluster_flavor\" {\ntype    = string\ndefault = \"t3.small\"\n}\n</code></pre> <p>You can also configure a different name to your cluster and change the default VM type. <code>t3.small</code> (2 vCPUs, 2 GB RAM) runs just fine for this tutorial.</p>"},{"location":"examples/rook-ceph/#2-create-the-vms","title":"2. Create the VMs","text":"<p>For AWS, you need an account. Terraform will use the following environment variable: <code>AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_SESSION_TOKEN</code>. You can easily copy-paste them from the AWS portal. For more information, see the AWS documentation.</p> <p></p> <p>When the environment variables are set, you can proceed with Terraform and deploy the VMs.</p> <pre><code>terraform init\nterraform apply\n</code></pre> <p>If you decide to create the VMs manually using AWS GUI, you need to disable source / destination checking. This needs to be disbled always for multi-node Kubernetes clusters in order to get the node-to-node communication working due to Network Address Translation. For Terraform this is already taken care of in the default configuration.</p>"},{"location":"examples/rook-ceph/#3-create-and-attach-the-volumes","title":"3. Create and attach the volumes","text":"<p>Ceph requires one of the following storage options for storing the data:</p> <ul> <li>Raw devices (no partitions or formatted filesystems)</li> <li>Raw partitions (no formatted filesystem)</li> <li>PVs available from a storage class in block mode</li> </ul> <p>We will be using raw partititions (AWS EBS volumes), which can be easily attached to the worker node VMs. They are automatically detected by Ceph with its default configuration.</p> <p>Deploy AWS EBS volumes, one for each worker node. You can manually create three EBS volumes (for example 10 GB each) using the AWS GUI and attach those to your worker nodes. Formatting shouldn't be done. Instead, Ceph handles that part automatically.</p> <p>After you have attached the EBS volumes to the worker nodes, log in to one of the workers and check the available block devices:</p> <pre><code>$ lsblk -f\nNAME        FSTYPE   LABEL           UUID                                 FSAVAIL FSUSE% MOUNTPOINT\nloop0       squashfs                                                            0   100% /snap/amazon-ssm-agent/3552\nloop1       squashfs                                                            0   100% /snap/core18/1997\nloop2       squashfs                                                            0   100% /snap/snapd/11588\nloop3       squashfs                                                            0   100% /snap/lxd/19647\nnvme0n1\n\u2514\u2500nvme0n1p1 ext4     cloudimg-rootfs e8070c31-bfee-4314-a151-d1332dc23486    5.1G    33% /\nnvme1n1\n</code></pre> <p>The last line (nvme1n1) in this example printout corresponds to the attached EBS volume. Note that it doesn't have any filesystem (FSTYPE is empty). This meets the Ceph storage requirements and you are good to proceed.</p>"},{"location":"examples/rook-ceph/#4-install-k0s-using-k0sctl","title":"4. Install k0s using k0sctl","text":"<p>You can use terraform to automatically output a config file for k0sctl with the ip addresses and access details.</p> <pre><code>terraform output -raw k0s_cluster &gt; k0sctl.yaml\n</code></pre> <p>After that deploying k0s becomes very easy with the ready-made configuration.</p> <pre><code>k0sctl apply --config k0sctl.yaml\n</code></pre> <p>It might take around 2-3 minutes for k0sctl to connect each node, install k0s and connect the nodes together to form a cluster.</p>"},{"location":"examples/rook-ceph/#5-access-k0s-cluster","title":"5. Access k0s cluster","text":"<p>To access your new cluster remotely, you can use k0sctl to fetch kubeconfig and use that with kubectl or Lens.</p> <pre><code>k0sctl kubeconfig --config k0sctl.yaml &gt; kubeconfig\nexport KUBECONFIG=$PWD/kubeconfig\nkubectl get nodes\n</code></pre> <p>The other option is to login to your controller node and use the k0s in-built kubectl to access the cluster. Then you don't need to worry about kubeconfig (k0s takes care of that automatically).</p> <pre><code>ssh -i aws.pem &lt;username&gt;@&lt;ip-address&gt;\nsudo k0s kubectl get nodes\n</code></pre>"},{"location":"examples/rook-ceph/#6-deploy-rook","title":"6. Deploy Rook","text":"<p>To get started with Rook, let's first clone the Rook GitHub repo:</p> <pre><code>git clone --single-branch --branch v1.6.0 https://github.com/rook/rook.git\ncd rook/cluster/examples/kubernetes/ceph\n</code></pre> <p>We will use mostly the default Rook configuration. However, k0s kubelet drectory must be configured in <code>operator.yaml</code> like this</p> <pre><code>ROOK_CSI_KUBELET_DIR_PATH: \"/var/lib/k0s/kubelet\"\n</code></pre> <p>To create the resources, which are needed by the Rook\u2019s Ceph operator, run</p> <pre><code>kubectl apply -f crds.yaml -f common.yaml -f operator.yaml\n</code></pre> <p>Now you should see the operator running. Check them with</p> <pre><code>kubectl get pods -n rook-ceph\n</code></pre>"},{"location":"examples/rook-ceph/#7-deploy-ceph-cluster","title":"7. Deploy Ceph Cluster","text":"<p>Then you can proceed to create a Ceph cluster. Ceph will use the three EBS volumes attached to the worker nodes:</p> <pre><code>kubectl apply -f cluster.yaml\n</code></pre> <p>It takes some minutes to prepare the volumes and create the cluster. Once this is completed you should see the following output:</p> <pre><code>$ kubectl get pods -n rook-ceph\n\nNAME                                                         READY   STATUS      RESTARTS   AGE\ncsi-cephfsplugin-nhxc8                                       3/3     Running     0          2m48s\ncsi-cephfsplugin-provisioner-db45f85f5-ldhjp                 6/6     Running     0          2m48s\ncsi-cephfsplugin-provisioner-db45f85f5-sxfm8                 6/6     Running     0          2m48s\ncsi-cephfsplugin-tj2bh                                       3/3     Running     0          2m48s\ncsi-cephfsplugin-z2rrl                                       3/3     Running     0          2m48s\ncsi-rbdplugin-5q7gq                                          3/3     Running     0          2m49s\ncsi-rbdplugin-8sfpd                                          3/3     Running     0          2m49s\ncsi-rbdplugin-f2xdz                                          3/3     Running     0          2m49s\ncsi-rbdplugin-provisioner-d85cbdb48-g6vck                    6/6     Running     0          2m49s\ncsi-rbdplugin-provisioner-d85cbdb48-zpmvr                    6/6     Running     0          2m49s\nrook-ceph-crashcollector-ip-172-31-0-76-64cb4c7775-m55x2     1/1     Running     0          45s\nrook-ceph-crashcollector-ip-172-31-13-183-654b46588d-djqsd   1/1     Running     0          2m57s\nrook-ceph-crashcollector-ip-172-31-15-5-67b68698f-gcjb7      1/1     Running     0          2m46s\nrook-ceph-mgr-a-5ffc65c874-8pxgv                             1/1     Running     0          58s\nrook-ceph-mon-a-ffcd85c5f-z89tb                              1/1     Running     0          2m59s\nrook-ceph-mon-b-fc8f59464-lgczk                              1/1     Running     0          2m46s\nrook-ceph-mon-c-69bd87b558-kl4nl                             1/1     Running     0          91s\nrook-ceph-operator-54cf7487d4-pl66p                          1/1     Running     0          4m57s\nrook-ceph-osd-0-dd4fd8f6-g6s9m                               1/1     Running     0          48s\nrook-ceph-osd-1-7c478c49c4-gkqml                             1/1     Running     0          47s\nrook-ceph-osd-2-5b887995fd-26492                             1/1     Running     0          46s\nrook-ceph-osd-prepare-ip-172-31-0-76-6b5fw                   0/1     Completed   0          28s\nrook-ceph-osd-prepare-ip-172-31-13-183-cnkf9                 0/1     Completed   0          25s\nrook-ceph-osd-prepare-ip-172-31-15-5-qc6pt                   0/1     Completed   0          23s\n</code></pre>"},{"location":"examples/rook-ceph/#8-configure-ceph-block-storage","title":"8. Configure Ceph block storage","text":"<p>Before Ceph can provide storage to your cluster, you need to create a ReplicaPool and a StorageClass. In this example, we use the default configuration to create the block storage.</p> <pre><code>kubectl apply -f ./csi/rbd/storageclass.yaml\n</code></pre>"},{"location":"examples/rook-ceph/#9-request-storage","title":"9. Request storage","text":"<p>Create a new manifest file <code>mongo-pvc.yaml</code> with the following content:</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: mongo-pvc\nspec:\nstorageClassName: rook-ceph-block\naccessModes:\n- ReadWriteOnce\nresources:\nrequests:\nstorage: 2Gi\n</code></pre> <p>This will create Persistent Volume Claim (PVC) to request a 2 GB block storage from Ceph. Provioning will be done dynamically. You can define the block size freely as long as it fits to the available storage size.</p> <pre><code>kubectl apply -f mongo-pvc.yaml\n</code></pre> <p>You can now check the status of your PVC:</p> <pre><code>kubectl get pvc\n</code></pre> <p>When the PVC gets the requested volume reserved (bound), it should look like this:</p> <pre><code>$ kubectl get pvc\n\nNAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE\nmongo-pvc   Bound    pvc-08337736-65dd-49d2-938c-8197a8871739   3Gi        RWO            rook-ceph-block   6s\n</code></pre>"},{"location":"examples/rook-ceph/#10-deploy-an-example-application","title":"10. Deploy an example application","text":"<p>Let's deploy a Mongo database to verify the Ceph storage. Create a new file <code>mongo.yaml</code> with the following content:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: mongo\nspec:\nselector:\nmatchLabels:\napp: mongo\ntemplate:\nmetadata:\nlabels:\napp: mongo\nspec:\ncontainers:\n- image: mongo:4.0\nname: mongo\nports:\n- containerPort: 27017\nname: mongo\nvolumeMounts:\n- name: mongo-persistent-storage\nmountPath: /data/db\nvolumes:\n- name: mongo-persistent-storage\npersistentVolumeClaim:\nclaimName: mongo-pvc\n</code></pre> <p>Deploy the database:</p> <pre><code>kubectl apply -f mongo.yaml\n</code></pre>"},{"location":"examples/rook-ceph/#11-access-the-application","title":"11. Access the application","text":"<p>Open the MongoDB shell using the mongo pod:</p> <pre><code>$ kubectl get pods\nNAME                    READY   STATUS    RESTARTS   AGE\nmongo-b87cbd5cc-4wx8t   1/1     Running   0          76s\n\n$ kubectl exec -it mongo-b87cbd5cc-4wx8t -- mongo\n</code></pre> <p>Create a DB and insert some data:</p> <pre><code>&gt; use testDB\nswitched to db testDB\n&gt; db.testDB.insertOne( {name: \"abc\", number: 123  })\n{\n  \"acknowledged\" : true,\n  \"insertedId\" : ObjectId(\"60815690a709d344f83b651d\")\n}\n&gt; db.testDB.insertOne( {name: \"bcd\", number: 234  })\n{\n  \"acknowledged\" : true,\n  \"insertedId\" : ObjectId(\"6081569da709d344f83b651e\")\n}\n</code></pre> <p>Read the data:</p> <pre><code>&gt; db.getCollection(\"testDB\").find()\n{ \"_id\" : ObjectId(\"60815690a709d344f83b651d\"), \"name\" : \"abc\", \"number\" : 123 }\n{ \"_id\" : ObjectId(\"6081569da709d344f83b651e\"), \"name\" : \"bcd\", \"number\" : 234 }\n&gt;\n</code></pre> <p>You can also try to restart the mongo pod or restart the worker nodes to verity that the storage is persistent.</p>"},{"location":"examples/rook-ceph/#12-clean-up","title":"12. Clean-up","text":"<p>You can use Terraform to take down the VMs:</p> <pre><code>terraform destroy\n</code></pre> <p>Remember to delete the EBS volumes separately.</p>"},{"location":"examples/rook-ceph/#conclusions","title":"Conclusions","text":"<p>You have now created a replicated Ceph storage for k0s. All you data is stored to multiple disks at the same time so you have a fault-tolerant solution. You also have enabled dynamic provisioning. Your applications can request the available storage without a manual creation of the persistent volumes each time.</p> <p>This was just one example to deploy distributed storage to k0s cluster using an operator. You can easily use different Kubernetes storage solutions with k0s.</p>"},{"location":"examples/traefik-ingress/","title":"Install the Traefik Ingress Controller on k0s","text":"<p>You can configure k0s with the Traefik ingress controller, a MetalLB service loadbalancer, and deploy the Traefik Dashboard using a service sample. To do this you leverage Helm's extensible bootstrapping functionality to add the correct extensions to the <code>k0s.yaml</code> file during cluster configuration.</p>"},{"location":"examples/traefik-ingress/#1-configure-k0syaml","title":"1. Configure k0s.yaml","text":"<p>Configure k0s to install Traefik and MetalLB during cluster bootstrapping by adding their Helm charts as extensions in the k0s configuration file (<code>k0s.yaml</code>).</p> <p>Note:</p> <p>A good practice is to have a small range of IP addresses that are addressable on your network, preferably outside the assignment pool your DHCP server allocates (though any valid IP range should work locally on your machine). Providing an addressable range allows you to access your load balancer and Ingress services from anywhere on your local network.</p> <pre><code>extensions:\nhelm:\nrepositories:\n- name: traefik\nurl: https://helm.traefik.io/traefik\n- name: bitnami\nurl: https://charts.bitnami.com/bitnami\ncharts:\n- name: traefik\nchartname: traefik/traefik\nversion: \"9.11.0\"\nnamespace: default\n- name: metallb\nchartname: bitnami/metallb\nversion: \"1.0.1\"\nnamespace: default\nvalues: |2\nconfigInline:\naddress-pools:\n- name: generic-cluster-pool\nprotocol: layer2\naddresses:\n- 192.168.0.5-192.168.0.10\n</code></pre>"},{"location":"examples/traefik-ingress/#2-retrieve-the-load-balancer-ip","title":"2. Retrieve the Load Balancer IP","text":"<p>After you start your cluster, run <code>kubectl get all</code> to confirm the deployment of Traefik and MetalLB. The command should return a response with the <code>metallb</code> and <code>traefik</code> resources, along with a service load balancer that has an assigned <code>EXTERNAL-IP</code>.</p> <pre><code>kubectl get all\n</code></pre> <p>Output:</p> <pre><code>NAME                                                 READY   STATUS    RESTARTS   AGE\npod/metallb-1607085578-controller-864c9757f6-bpx6r   1/1     Running   0          81s\npod/metallb-1607085578-speaker-245c2                 1/1     Running   0          60s\npod/traefik-1607085579-77bbc57699-b2f2t              1/1     Running   0          81s\n\nNAME                         TYPE           CLUSTER-IP       EXTERNAL-IP      PORT(S)                      AGE\nservice/kubernetes           ClusterIP      10.96.0.1        &lt;none&gt;           443/TCP                      96s\nservice/traefik-1607085579   LoadBalancer   10.105.119.102   192.168.0.5      80:32153/TCP,443:30791/TCP   84s\n\nNAME                                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\ndaemonset.apps/metallb-1607085578-speaker   1         1         1       1            1           kubernetes.io/os=linux   87s\n\nNAME                                            READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/metallb-1607085578-controller   1/1     1            1           87s\ndeployment.apps/traefik-1607085579              1/1     1            1           84s\n\nNAME                                                       DESIRED   CURRENT   READY   AGE\nreplicaset.apps/metallb-1607085578-controller-864c9757f6   1         1         1       81s\nreplicaset.apps/traefik-1607085579-77bbc57699              1         1         1       81s\n</code></pre> <p>Take note of the <code>EXTERNAL-IP</code> given to the <code>service/traefik-n</code> load balancer. In this example, <code>192.168.0.5</code> has been assigned and can be used to access services via the Ingress proxy:</p> <pre><code>NAME                         TYPE           CLUSTER-IP       EXTERNAL-IP      PORT(S)                      AGE\nservice/traefik-1607085579   LoadBalancer   10.105.119.102   192.168.0.5      80:32153/TCP,443:30791/TCP   84s\n</code></pre> <p>Receiving a 404 response here is normal, as you've not configured any Ingress resources to respond yet:</p> <pre><code>$ curl http://192.168.0.5\n404 page not found\n</code></pre>"},{"location":"examples/traefik-ingress/#3-deploy-and-access-the-traefik-dashboard","title":"3. Deploy and access the Traefik Dashboard","text":"<p>With an available and addressable load balancer present on your cluster, now you can quickly deploy the Traefik dashboard and access it from anywhere on your LAN (assuming that MetalLB is configured with an addressable range).</p> <ol> <li> <p>Create the Traefik Dashboard IngressRoute in a YAML file:</p> <pre><code>apiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\nname: dashboard\nspec:\nentryPoints:\n- web\nroutes:\n- match: PathPrefix(`/dashboard`) || PathPrefix(`/api`)\nkind: Rule\nservices:\n- name: api@internal\nkind: TraefikService\n</code></pre> </li> <li> <p>Deploy the resource:</p> <pre><code>root@k0s-host \u279c kubectl apply -f traefik-dashboard.yaml\n</code></pre> <p>Output:</p> <pre><code>ingressroute.traefik.containo.us/dashboard created\n</code></pre> <p>At this point you should be able to access the dashboard using the <code>EXTERNAL-IP</code> that you noted above by visiting <code>http://192.168.0.5</code> in your browser:</p> <p></p> </li> <li> <p>Create a simple <code>whoami</code> Deployment, Service, and Ingress manifest:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: whoami-deployment\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: whoami\ntemplate:\nmetadata:\nlabels:\napp: whoami\nspec:\ncontainers:\n- name: whoami-container\nimage: containous/whoami\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: whoami-service\nspec:\nports:\n- name: http\ntargetPort: 80\nport: 80\nselector:\napp: whoami\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\nname: whoami-ingress\nspec:\nrules:\n- http:\npaths:\n- path: /whoami\npathType: Exact\nbackend:\nservice:\nname: whoami-service\nport:\nnumber: 80\n</code></pre> </li> <li> <p>Apply the manifests:</p> <pre><code>root@k0s-host \u279c kubectl apply -f whoami.yaml\n</code></pre> <p>Output:</p> <pre><code>deployment.apps/whoami-deployment created\nservice/whoami-service created\ningress.networking.k8s.io/whoami-ingress created\n</code></pre> </li> <li> <p>Test the ingress and service:</p> <pre><code>curl http://192.168.0.5/whoami\n</code></pre> <p>Output:</p> <pre><code>Hostname: whoami-deployment-85bfbd48f-7l77c\nIP: 127.0.0.1\nIP: ::1\nIP: 10.244.214.198\nIP: fe80::b049:f8ff:fe77:3e64\nRemoteAddr: 10.244.214.196:34858\nGET /whoami HTTP/1.1\nHost: 192.168.0.5\nUser-Agent: curl/7.68.0\nAccept: */*\nAccept-Encoding: gzip\nX-Forwarded-For: 192.168.0.82\nX-Forwarded-Host: 192.168.0.5\nX-Forwarded-Port: 80\nX-Forwarded-Proto: http\nX-Forwarded-Server: traefik-1607085579-77bbc57699-b2f2t\nX-Real-Ip: 192.168.0.82\n</code></pre> </li> </ol>"},{"location":"examples/traefik-ingress/#further-details","title":"Further details","text":"<p>With the Traefik Ingress Controller it is possible to use 3rd party tools, such as ngrok, to go further and expose your load balancer to the world. In doing this you enable dynamic certificate provisioning through Let's Encrypt, using either cert-manager or Traefik's own built-in ACME provider.</p>"},{"location":"internal/host-dependencies/","title":"Host Dependencies","text":"<p>The goal of k0s is to only depend on kernel functionality. However, the kubelet currently has several host dependencies. Some of these may need to be bundled, but we should prefer upstream contributions to remove these dependencies.</p>"},{"location":"internal/host-dependencies/#list-of-hard-dependencies","title":"List of hard dependencies","text":"<ul> <li><code>find</code> -- PR by @ncopa to resolve this: https://github.com/kubernetes/kubernetes/pull/95189</li> <li><code>du</code> -- PR by @ncopa to resolve this: https://github.com/kubernetes/kubernetes/pull/95178 -- note that <code>du</code> dependency remains, but using POSIX-compliant argument</li> <li><code>nice</code></li> <li><code>iptables</code> -- as documented in https://github.com/k0sproject/k0s/issues/176 it is unclear whether <code>iptables</code> is needed. It appears to come from the <code>portmap</code> plugin, but the most robust solution may be to simply bundle <code>iptables</code> with k0s.</li> </ul>"},{"location":"internal/publishing_docs_using_mkdocs/","title":"Publishing Docs","text":"<p>We use mkdocs and mike for publishing docs to docs.k0sproject.io. This guide will provide a simple how-to on how to configure and deploy newly added docs to our website.</p>"},{"location":"internal/publishing_docs_using_mkdocs/#requirements","title":"Requirements","text":"<p>Install mike: https://github.com/jimporter/mike#installation</p>"},{"location":"internal/publishing_docs_using_mkdocs/#adding-a-new-link-to-the-navigation","title":"Adding A New link to the Navigation","text":"<ul> <li>All docs must live under the <code>docs</code> directory (I.E., changes to the main <code>README.md</code> are not reflected in the website).</li> <li>Add a new link under <code>nav</code> in the main mkdocs.yml file:</li> </ul> <pre><code>nav:\n- Overview: README.md\n- Creating A Cluster:\n- Quick Start Guide: create-cluster.md\n- Run in Docker: k0s-in-docker.md\n- Single node set-up: k0s-single-node.md\n- Configuration Reference:\n- Architecture: architecture.md\n- Networking: networking.md\n- Configuration Options: configuration.md\n- Using Cloud Providers: cloud-providers.md\n- Running k0s with Traefik: examples/traefik-ingress.md\n- Running k0s as a service: install.md\n- k0s CLI Help Pages: cli/k0s.md\n- Deploying Manifests: manifests.md\n- FAQ: FAQ.md\n- Troubleshooting: troubleshooting.md\n- Contributing:\n- Overview: contributors/overview.md\n- Workflow: contributors/github_workflow.md\n- Testing: contributors/testing.md\n</code></pre> <ul> <li>Once your changes are pushed to <code>main</code>, the \"Publish Docs\" jos will start running: https://github.com/k0sproject/k0s/actions?query=workflow%3A%22Publish+docs+via+GitHub+Pages%22</li> <li>You should see the deployment outcome in the <code>gh-pages</code> deployment page: https://github.com/k0sproject/k0s/deployments/activity_log?environment=github-pages</li> </ul>"},{"location":"internal/publishing_docs_using_mkdocs/#testing-docs-locally","title":"Testing docs locally","text":"<p>We've got a dockerized setup for easily testing docs in local environment. Simply run <code>docker-compose up</code> in the docs root folder. The docs will be available on <code>localhost:80</code>.</p> <p>Note If you have something already running locally on port <code>80</code> you need to change the mapped port on the <code>docker-compose.yml</code> file.</p>"},{"location":"internal/upgrading-calico/","title":"Upgrading Calico","text":"<p>k0s bundles Kubernetes manifests for Calico. The manifests are retrieved from the official Calico docs.</p> <p>As fetching and modifying the entire multi-thousand line file is error-prone, you may follow these steps to upgrade Calico to the latest version:</p> <ol> <li>run <code>./get-calico.sh</code></li> <li>check the git diff to see if it looks sensible</li> <li>re-apply our manual adjustments (documented below)</li> <li>run <code>make bindata-manifests</code></li> <li>compile, pray, and test</li> <li>commit and create a PR</li> </ol>"},{"location":"internal/upgrading-calico/#manual-adjustments","title":"Manual Adjustments","text":"<p>Note: All manual adjustments should be fairly obvious from the git diff. This section attempts to provide a sanity checklist to go through and make sure we still have those changes applied. The code blocks in this section are our modifications, not the calico originals.</p> <p><code>static/manifests/calico/DaemonSet/calico-node.yaml</code>:</p> <ul> <li>variable-based support for both vxlan and ipip (search for <code>ipip</code> to find):</li> </ul> <pre><code>{{- if eq .Mode \"ipip\" }}\n# Enable IPIP\n- name: CALICO_IPV4POOL_IPIP\nvalue: {{ .Overlay }}\n# Enable or Disable VXLAN on the default IP pool.\n- name: CALICO_IPV4POOL_VXLAN\nvalue: \"Never\"\n{{- else if eq .Mode \"vxlan\" }}\n# Disable IPIP\n- name: CALICO_IPV4POOL_IPIP\nvalue: \"Never\"\n# Enable VXLAN on the default IP pool.\n- name: CALICO_IPV4POOL_VXLAN\nvalue: {{ .Overlay }}\n- name: FELIX_VXLANPORT\nvalue: \"{{ .VxlanPort }}\"\n- name: FELIX_VXLANVNI\nvalue: \"{{ .VxlanVNI }}\"\n{{- end }}\n</code></pre> <ul> <li>iptables auto detect:</li> </ul> <pre><code># Auto detect the iptables backend\n- name: FELIX_IPTABLESBACKEND\nvalue: \"auto\"\n</code></pre> <ul> <li>variable-based WireGuard support:</li> </ul> <pre><code>{{- if .EnableWireguard }}\n- name: FELIX_WIREGUARDENABLED\nvalue: \"true\"\n{{- end }}\n</code></pre> <ul> <li>variable-based cluster CIDR:</li> </ul> <pre><code>- name: CALICO_IPV4POOL_CIDR\nvalue: \"{{ .ClusterCIDR }}\"\n</code></pre> <ul> <li>custom backend and MTU</li> </ul> <pre><code># calico-config.yaml\ncalico_backend: \"{{ .Mode }}\"\nveth_mtu: \"{{ .MTU }}\"\n</code></pre> <ul> <li>remove bgp from <code>CLUSTER_TYPE</code></li> </ul> <pre><code>- name: CLUSTER_TYPE\nvalue: \"k8s\"\n</code></pre> <ul> <li>disable BIRD checks on liveness and readiness as we don't support BGP by removing <code>-bird-ready</code> and <code>-bird-live</code> from the readiness and liveness probes respectively</li> </ul>"},{"location":"internal/upgrading-calico/#container-image-names","title":"Container image names","text":"<p>Instead of hardcoded image names and versions use placeholders to support configuration level settings. Following placeholders are used:</p> <ul> <li><code>CalicoCNIImage</code> for calico/cni</li> <li><code>CalicoNodeImage</code> for calico/node</li> <li><code>CalicoKubeControllersImage</code> for calico/kube-controllers</li> </ul> <p>Also, all containers in manifests were modified to have 'imagePullPolicy' field:</p> <pre><code>imagePullPolicy: {{ .PullPolicy }}\n</code></pre> <p>Example:</p> <pre><code># calico-node.yaml\nimage: {{ .CalicoCNIImage }}\n</code></pre>"}]}