{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>k0s is an all-inclusive Kubernetes distribution with all the required bells and whistles preconfigured to make building a Kubernetes clusters a matter of just copying an executable to every host and running it.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Packaged as a single static binary</li> <li>Self-hosted, isolated control plane</li> <li>Variety of storage backends: etcd, SQLite, MySQL (or any compatible), PostgreSQL</li> <li>Elastic control-plane</li> <li>Vanilla upstream Kubernetes</li> <li>Supports custom container runtimes (containerd is the default)</li> <li>Supports custom Container Network Interface (CNI) plugins (calico is the default)</li> <li>Supports x86_64 and arm64</li> </ul>"},{"location":"#join-the-community","title":"Join the Community","text":"<p>If you'd like to help build k0s, please check out our guide to Contributing and our Code of Conduct.</p>"},{"location":"#demo","title":"Demo","text":""},{"location":"#downloading-k0s","title":"Downloading k0s","text":"<p>Download k0s for linux amd64 and arm64 architectures.</p>"},{"location":"#quick-start","title":"Quick start","text":"<p>Creating A k0s Cluster</p>"},{"location":"CODE_OF_CONDUCT/","title":"K0s Community Code Of Conduct","text":"<p>Please refer to our contributor code of conduct.</p>"},{"location":"FAQ/","title":"Frequently asked questions","text":""},{"location":"FAQ/#how-is-k0s-pronounced","title":"How is k0s pronounced?","text":"<p>kay-zero-ess</p>"},{"location":"FAQ/#how-do-i-run-a-single-node-cluster","title":"How do I run a single node cluster?","text":"<p><code>k0s server --enable-worker</code></p>"},{"location":"FAQ/#how-do-i-connect-to-the-cluster","title":"How do I connect to the cluster?","text":"<p>You find the config in <code>${DATADIR}/pki/admin.conf</code> (default: <code>/var/lib/k0s/pki/admin.conf</code>). Copy this file, and change the <code>localhost</code> entry to the public ip of the controller. Use the modified config to connect with kubectl: <pre><code>export KUBECONFIG=/path/to/admin.conf\nkubectl ...\n</code></pre></p>"},{"location":"FAQ/#why-doesnt-kubectl-get-nodes-list-the-k0s-server","title":"Why doesn't <code>kubectl get nodes</code> list the k0s server?","text":"<p>As a default, the control plane does not run kubelet at all, and will not accept any workloads, so the server will not show up on the node list in kubectl. If you want your server to accept workloads and run pods, you do so with: <code>k0s server --enable-worker</code> (recommended only as test/dev/POC environments).</p>"},{"location":"architecture/","title":"Architecture","text":"<p>Note: As with any young project, things change rapidly. Thus all the details in this architecture documentation may not be always up-to-date, but the high level concepts and patterns should still apply.</p>"},{"location":"architecture/#packaging","title":"Packaging","text":"<p>k0s is packaged as single, self-extracting binary which embeds Kubernetes binaries. This has many benefits: - Everything can be, and is, statically compiled - No OS level deps - No RPMs, dep's, snaps or any other OS specific packaging needed. Single \"package\" for all OSes - We can fully control the versions of each and every dependency</p> <p></p>"},{"location":"architecture/#control-plane","title":"Control plane","text":"<p>k0s as a single binary acts as the process supervisor for all other control plane components. This means there's no container engine or kubelet running on controllers (by default). Which means there is no way for a cluster user to schedule workloads onto controller nodes.</p> <p></p> <p>k0s creates, manages and configures each of the components. k0s runs all control plane components as \"naked\" processes. So on the controller node there's no container engine running.</p>"},{"location":"architecture/#storage","title":"Storage","text":"<p>Typically Kubernetes control plane supports only etcd as the datastore. In addition to etcd, k0s supports many other datastore options. This is achieved by including kine. Kine allows wide variety of backend data stores to be used such as MySQL, PostgreSQL, SQLite and dqlite. See more in storage documentation</p> <p>In case of k0s managed etcd, k0s manages the full lifecycle of the etcd cluster. This means for example that by joining a new controller node with <code>k0s server \"long-join-token\"</code> k0s will automatically adjust the etcd cluster membership info to allow the new member to join the cluster.</p> <p>Note: Currently k0s cannot shrink the etcd cluster. For now user needs to manually remove the etcd member and only after that shutdown the k0s controller on the removed node.</p>"},{"location":"architecture/#worker-plane","title":"Worker plane","text":"<p>Like for the control plane, k0s creates and manages the core worker components as naked processes on the worker node. Currently we support only containerd as the container engine.</p>"},{"location":"cloud-providers/","title":"Using cloud providers","text":"<p>k0s builds Kubernetes components in \"providerless\" mode. This means that there is no cloud providers built into k0s managed Kubernetes components.</p> <p>This means the cloud providers have to be configured \"externally\". The following steps outline how to enable cloud providers support in your k0s cluster.</p> <p>For more information on running Kubernetes with cloud providers see the official documentation.</p>"},{"location":"cloud-providers/#enabling-cloud-provider-support-in-kubelet","title":"Enabling cloud provider support in kubelet","text":"<p>Even when all components are built with \"providerless\" mode, we need to be able to enable cloud provider \"mode\" for kubelet. This is done by running the workers with <code>--enable-cloud-provider=true</code>. This enables <code>--cloud-provider=external</code> on kubelet process.</p>"},{"location":"cloud-providers/#deploying-the-actual-cloud-provider","title":"Deploying the actual cloud provider","text":"<p>From Kubernetes point of view, it does not realy matter how and where the cloud providers controller(s) are running. Of course the easiest way is to deploy them on the cluster itself. </p> <p>To deploy your cloud provider as k0s managed stack you can use the built-in manifest deployer. Simply drop all the needed manifests under e.g. <code>/var/lib/k0s/manifests/aws/</code> directory and k0s will deploy everything.</p> <p>Some cloud providers do need some configuration files to be present on all the nodes or some other pre-requisites. Consult your cloud providers documentation for needed steps.</p>"},{"location":"configuration/","title":"k0s configuration","text":""},{"location":"configuration/#control-plane","title":"Control plane","text":"<p>k0s Control plane can be configured via a YAML config file. By default <code>k0s server</code> command reads a file called <code>k0s.yaml</code> but can be told to read any yaml file via <code>--config</code> option.</p> <p>An example config file with the most common options users should configure:</p> <pre><code>apiVersion: k0s.k0sproject.io/v1beta1\nkind: Cluster\nmetadata:\nname: k0s\nspec:\napi:\naddress: 192.168.68.106\nsans:\n- my-k0s-control.my-domain.com\nnetwork:\npodCIDR: 10.244.0.0/16\nserviceCIDR: 10.96.0.0/12\nextensions:\nhelm:\nrepositories:\n- name: prometheus-community\nurl: https://prometheus-community.github.io/helm-charts\ncharts:\n- name: prometheus-stack\nchartname: prometheus-community/prometheus\nversion: \"11.16.8\"\nnamespace: default\n</code></pre>"},{"location":"configuration/#specapi","title":"<code>spec.api</code>","text":"<ul> <li><code>address</code>: The local address to bind API on. Also used as one of the addresses pushed on the k0s create service certificate on the API. Defaults to first non-local address found on the node.</li> <li><code>sans</code>: List of additional addresses to push to API servers serving certificate</li> </ul>"},{"location":"configuration/#specnetwork","title":"<code>spec.network</code>","text":"<ul> <li><code>podCIDR</code>: Pod network CIDR to be used in the cluster</li> <li><code>serviceCIDR</code>: Network CIDR to be used for cluster VIP services.</li> </ul>"},{"location":"configuration/#extensionshelm","title":"<code>extensions.helm</code>","text":"<p>List of Helm repositories and charts to deploy during cluster bootstrap. This example configures Prometheus from \"stable\" Helms chart repository.</p>"},{"location":"configuration/#configuring-multi-node-controlplane","title":"Configuring multi-node controlplane","text":"<p>When configuring an elastic/HA controlplane one must use same configuration options on each node for the cluster level options. Following options need to match on each node, otherwise the control plane components will end up in very unknown states: - <code>network</code> - <code>storage</code>: Needless to say, one cannot create a clustered controlplane with each node only storing data locally on SQLite.</p>"},{"location":"configuration/#full-config-reference","title":"Full config reference","text":"<p>Note: Many of the options configure things deep down in the \"stack\" on various components. So please make sure you understand what is being configured and whether or not it works in your specific environment.</p> <p>A full config file with defaults generated by the <code>k0s default-config</code> command:</p> <pre><code>apiVersion: k0s.k0sproject.io/v1beta1\nkind: Cluster\nmetadata:\nname: k0s\nspec:\napi:\nexternalAddress: my-lb-address.example.com\naddress: 192.168.68.106\nsans:\n- 192.168.68.106\nextraArgs: {}\ncontrollerManager:\nextraArgs: {}\nscheduler:\nextraArgs: {}\nstorage:\ntype: etcd\netcd:\npeerAddress: 192.168.68.106\nnetwork:\npodCIDR: 10.244.0.0/16\nserviceCIDR: 10.96.0.0/12\nprovider: calico\ncalico:\nmode: vxlan\nvxlanPort: 4789\nvxlanVNI: 4096\nmtu: 1450\nwireguard: false\nflexVolumeDriverPath: /usr/libexec/k0s/kubelet-plugins/volume/exec/nodeagent~uds\npodSecurityPolicy:\ndefaultPolicy: 00-k0s-privileged\nworkerProfiles: []\nimages:\nkonnectivity:\nimage: us.gcr.io/k8s-artifacts-prod/kas-network-proxy/proxy-agent\nversion: v0.0.13\nmetricsserver:\nimage: gcr.io/k8s-staging-metrics-server/metrics-server\nversion: v0.3.7\nkubeproxy:\nimage: k8s.gcr.io/kube-proxy\nversion: v1.20.1\ncoredns:\nimage: docker.io/coredns/coredns\nversion: 1.7.0\ncalico:\ncni:\nimage: calico/cni\nversion: v3.16.2\nflexvolume:\nimage: calico/pod2daemon-flexvol\nversion: v3.16.2\nnode:\nimage: calico/node\nversion: v3.16.2\nkubecontrollers:\nimage: calico/kube-controllers\nversion: v3.16.2\nrepository: \"\"\ntelemetry:\ninterval: 10m0s\nenabled: true\nextensions:\nhelm:\nrepositories:\n- name: stable\nurl: https://charts.helm.sh/stable\n- name: prometheus-community\nurl: https://prometheus-community.github.io/helm-charts\ncharts:\n- name: prometheus-stack\nchartname: prometheus-community/prometheus\nversion: \"11.16.8\"\nvalues: |\nserver:\npodDisruptionBudget:\nenabled: false\nnamespace: default\n</code></pre>"},{"location":"configuration/#specapi_1","title":"<code>spec.api</code>","text":"<ul> <li><code>externalAddress</code>: If k0s controllers are running behind a loadbalancer provide the loadbalancer address here. This will configure all cluster components to connect to this address and also configures this address to be used when joining new nodes into the cluster.</li> <li><code>address</code>: The local address to bind API on. Also used as one of the addresses pushed on the k0s create service certificate on the API. Defaults to first non-local address found on the node.</li> <li><code>sans</code>: List of additional addresses to push to API servers serving certificate</li> <li><code>extraArgs</code>: Map of key-values (strings) for any extra arguments you wish to pass down to Kubernetes api-server process</li> </ul>"},{"location":"configuration/#speccontrollermanager","title":"<code>spec.controllerManager</code>","text":"<ul> <li><code>extraArgs</code>: Map of key-values (strings) for any extra arguments you wish to pass down to Kubernetes controller manager process</li> </ul>"},{"location":"configuration/#specscheduler","title":"<code>spec.scheduler</code>","text":"<ul> <li><code>extraArgs</code>: Map of key-values (strings) for any extra arguments you wish to pass down to Kubernetes scheduler process</li> </ul>"},{"location":"configuration/#specstorage","title":"<code>spec.storage</code>","text":"<ul> <li><code>type</code>: Type of the data store, either <code>etcd</code> or <code>kine</code>.</li> <li><code>etcd.peerAddress</code>: Nodes address to be used for etcd cluster peering.</li> <li><code>kine.dataSource</code>: kine datasource URL.</li> </ul> <p>Using type <code>etcd</code> will make k0s to create and manage an elastic etcd cluster within the controller nodes.</p>"},{"location":"configuration/#specnetwork_1","title":"<code>spec.network</code>","text":"<ul> <li><code>provider</code>: Network provider, either <code>calico</code> or <code>custom</code>. In case of <code>custom</code> user can push any network provider.</li> <li><code>podCIDR</code>: Pod network CIDR to be used in the cluster</li> <li><code>serviceCIDR</code>: Network CIDR to be used for cluster VIP services.</li> </ul>"},{"location":"configuration/#specnetworkcalico","title":"<code>spec.network.calico</code>","text":"<ul> <li><code>mode</code>: <code>vxlan</code> (default) or <code>ipip</code></li> <li><code>vxlanPort</code>: The UDP port to use for VXLAN (default <code>4789</code>)</li> <li><code>vxlanVNI</code>: The virtual network ID to use for VXLAN. (default: <code>4096</code>)</li> <li><code>mtu</code>: MTU to use for overlay network (default <code>1450</code>)</li> <li><code>wireguard</code>: enable wireguard based encryption (default <code>false</code>). Your host system must be wireguard ready. See https://docs.projectcalico.org/security/encrypt-cluster-pod-traffic for details.</li> <li><code>flexVolumeDriverPath</code>: The host path to use for Calicos flex-volume-driver (default: <code>/usr/libexec/k0s/kubelet-plugins/volume/exec/nodeagent~uds</code>). This should only need to be changed if the default path is unwriteable. See https://github.com/projectcalico/calico/issues/2712 for details. This option should ideally be paired with a custom volumePluginDir in the profile used on your worker nodes.</li> </ul>"},{"location":"configuration/#specpodsecuritypolicy","title":"<code>spec.podSecurityPolicy</code>","text":"<p>Configures the default psp to be set. k0s creates two PSPs out of box:</p> <ul> <li><code>00-k0s-privileged</code> (default): no restrictions, always also used for Kubernetes/k0s level system pods</li> <li><code>99-k0s-restricted</code>: no host namespaces or root users allowed, no bind mounts from host</li> </ul> <p>As a user you can of course create any supplemental PSPs and bind them to users / access accounts as you need.</p>"},{"location":"configuration/#specworkerprofiles","title":"<code>spec.workerProfiles</code>","text":"<p>Array of <code>spec.workerProfiles.workerProfile</code> Each element has following properties: - <code>name</code>: string, name, used as profile selector for the worker process - <code>values</code>: mapping object</p> <p>For each profile the control plane will create separate ConfigMap with kubelet-config yaml. Based on the <code>--profile</code> argument given to the <code>k0s worker</code> the corresponding ConfigMap would be used to extract <code>kubelet-config.yaml</code> from. <code>values</code> are recursively merged with default <code>kubelet-config.yaml</code></p> <p>There are a few fields that cannot be overridden:  - <code>clusterDNS</code> - <code>clusterDomain</code> - <code>apiVersion</code> - <code>kind</code></p> <p>Example:</p> <pre><code>  workerProfiles:\n    - name: custom-role\n      values:\n         key: value\n         mapping:\n             innerKey: innerValue\n</code></pre> <p>Custom volumePluginDir:</p> <pre><code>  workerProfiles:\n    - name: custom-role\n      values:\n         volumePluginDir: /var/libexec/k0s/kubelet-plugins/volume/exec\n</code></pre>"},{"location":"configuration/#images","title":"<code>images</code>","text":"<p>Each node under the <code>images</code> key has the same structure <pre><code>images:\n    konnectivity:\n      image: calico/kube-controllers\n      version: v3.16.2\n</code></pre> Following keys are avaiable</p>"},{"location":"configuration/#imageskonnectivity","title":"<code>images.konnectivity</code>","text":""},{"location":"configuration/#imagesmetricsserver","title":"<code>images.metricsserver</code>","text":""},{"location":"configuration/#imageskubeproxy","title":"<code>images.kubeproxy</code>","text":""},{"location":"configuration/#imagescoredns","title":"<code>images.coredns</code>","text":""},{"location":"configuration/#imagescalicocni","title":"<code>images.calico.cni</code>","text":""},{"location":"configuration/#imagescalicoflexvolume","title":"<code>images.calico.flexvolume</code>","text":""},{"location":"configuration/#imagescaliconode","title":"<code>images.calico.node</code>","text":""},{"location":"configuration/#imagescalicokubecontrollers","title":"<code>images.calico.kubecontrollers</code>","text":""},{"location":"configuration/#imagesrepository","title":"<code>images.repository</code>","text":"<p>If <code>images.repository</code> is set and not empty, every image name will be prefixed with the value of <code>images.repository</code></p> <p>Example <pre><code>images:\n  repository: \"my.own.repo\"\n    konnectivity:\n      image: calico/kube-controllers\n      version: v3.16.2\n</code></pre></p> <p>In the runtime the image name will be calculated as <code>my.own.repo/calico/kube-controllers:v3.16.2</code>.</p> <p>This only affects the location where images are getting pulled, omitting an image specification here will not disable the component from being deployed.</p>"},{"location":"configuration/#extensions","title":"Extensions","text":"<p>As stated in the project scope we intent to keep the scope of k0s quite small and not build gazillions of extensions into the product itself.</p> <p>To run k0s easily with your preferred extensions you have two options.</p> <ol> <li>Dump all needed extension manifest under <code>/var/lib/k0s/manifests/my-extension</code>. Read more on this approach here.</li> <li>Define your extensions as Helm charts:</li> </ol> <pre><code>extensions:\n  helm:\n    repositories:\n    - name: stable\n      url: https://charts.helm.sh/stable\n    - name: prometheus-community\n      url: https://prometheus-community.github.io/helm-charts\n    charts:\n    - name: prometheus-stack\n      chartname: prometheus-community/prometheus\n      version: \"11.16.8\"\n      values: |\n        storageSpec:\n          emptyDir:\n            medium: Memory\n      namespace: default\n</code></pre> <p>This way you get a declarative way to configure the cluster and k0s controller manages the setup of the defined extension Helm charts as part of the cluster bootstrap process.</p> <p>Some examples what you could use as extension charts: - Ingress controllers: Nginx ingress, Traefix ingress (tutorial),  - Volume storage providers: OpenEBS, Rook, Longhorn - Monitoring: Prometheus, Grafana</p>"},{"location":"configuration/#telemetry","title":"Telemetry","text":"<p>To build better end user experience we collect and send telemetry data from clusters. It is enabled by default and can be disabled by settings corresponding option as <code>false</code> The default interval is 10 minutes, any valid value for <code>time.Duration</code> string representation can be used as a value. Example <pre><code>telemetry:\n  interval: 2m0s\n  enabled: true\n</code></pre></p>"},{"location":"conformance-testing/","title":"Kubernetes conformance testing for k0s","text":"<p>We run the conformance testing for the last RC build for a release. Follow the instructions as the conformance testing repository.</p> <p>In a nutshell, you need to: - Setup k0s on some VMs/bare metal boxes - Download, if you do not already have, sonobuoy tool - Run the conformance tests with something like <code>sonobuoy run --mode=certified-conformance</code> - Wait for couple hours - Collect results</p>"},{"location":"containerd_config/","title":"containerd configuration","text":"<p>containerd is an industry-standard container runtime.</p> <p>NOTE: In most use cases changes to the containerd configuration will not be required. </p> <p>In order to make changes to containerd configuration first you need to generate a default containerd configuration by running: <pre><code>containerd config default &gt; /etc/k0s/containerd.toml\n</code></pre> This command will set the default values to <code>/etc/k0s/containerd.toml</code>. </p> <p><code>k0s</code> runs containerd with the follwoing default values: <pre><code>/var/lib/k0s/bin/containerd \\\n    --root=/var/lib/k0s/containerd \\\n    --state=/var/lib/k0s/run/containerd \\\n    --address=/var/lib/k0s/run/containerd.sock \\\n    --config=/etc/k0s/containerd.toml\n</code></pre></p> <p>Before proceeding further, add the following default values to the configuration file: <pre><code>version = 2\nroot = \"/var/lib/k0s/containerd\"\nstate = \"/var/lib/k0s/run/containerd\"\n...\n\n[grpc]\n  address = \"/var/lib/k0s/run/containerd.sock\"\n</code></pre></p> <p>Next if you want to change CRI look into this section</p> <p><code>[plugins.\"io.containerd.runtime.v1.linux\"]     shim = \"containerd-shim\"     runtime = \"runc\"</code></p>"},{"location":"containerd_config/#using-gvisor","title":"Using gVisor","text":"<p>gVisor is an application kernel, written in Go, that implements a substantial portion of the Linux system call interface. It provides an additional layer of isolation between running applications and the host operating system.</p> <p>First you must install the needed gVisor binaries into the host. <pre><code>(\nset -e\n  URL=https://storage.googleapis.com/gvisor/releases/release/latest\n  wget ${URL}/runsc ${URL}/runsc.sha512 \\\n${URL}/gvisor-containerd-shim ${URL}/gvisor-containerd-shim.sha512 \\\n${URL}/containerd-shim-runsc-v1 ${URL}/containerd-shim-runsc-v1.sha512\n  sha512sum -c runsc.sha512 \\\n-c gvisor-containerd-shim.sha512 \\\n-c containerd-shim-runsc-v1.sha512\n  rm -f *.sha512\n  chmod a+rx runsc gvisor-containerd-shim containerd-shim-runsc-v1\n  sudo mv runsc gvisor-containerd-shim containerd-shim-runsc-v1 /usr/local/bin\n)\n</code></pre></p> <p>See gVisor install docs</p> <p>Next we need to prepare the config for <code>k0s</code> managed containerD to utilize gVisor as additional runtime: <pre><code>cat &lt;&lt;EOF | sudo tee /etc/k0s/containerd.toml\ndisabled_plugins = [\"restart\"]\n[plugins.linux]\n  shim_debug = true\n[plugins.cri.containerd.runtimes.runsc]\n  runtime_type = \"io.containerd.runsc.v1\"\nEOF\n</code></pre></p> <p>Then we can start and join the worker as normally into the cluster: <pre><code>k0s worker $token\n</code></pre></p> <p>By default containerd uses nromal runc as the runtime. To make gVisor runtime usable for workloads we must register it to Kubernetes side: <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: node.k8s.io/v1beta1\nkind: RuntimeClass\nmetadata:\n  name: gvisor\nhandler: runsc\nEOF\n</code></pre></p> <p>After this we can use it for our workloads: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: nginx-gvisor\nspec:\nruntimeClassName: gvisor\ncontainers:\n- name: nginx\nimage: nginx\n</code></pre></p> <p>We can verify the created nginx pod is actually running under gVisor runtime: <pre><code># kubectl exec nginx-gvisor -- dmesg | grep -i gvisor\n[    0.000000] Starting gVisor...\n</code></pre></p>"},{"location":"containerd_config/#using-custom-nvidia-container-runtime","title":"Using custom <code>nvidia-container-runtime</code>","text":"<p>By default CRI is set tu runC and if you want to configure Nvidia GPU support you will have to replace <code>runc</code> with <code>nvidia-container-runtime</code> as shown below:</p> <pre><code>[plugins.\"io.containerd.runtime.v1.linux\"]\n    shim = \"containerd-shim\"\n    runtime = \"nvidia-container-runtime\"\n</code></pre> <p>Note To run <code>nvidia-container-runtime</code> on your node please look here for detailed instructions.</p> <p>After changes to the configuration, restart <code>k0s</code> and in this case containerd will be using newly configured runtime.</p>"},{"location":"create-cluster/","title":"Creating A cluster with k0s","text":"<p>As k0s binary has everything it needs packaged into a single binary, it makes it super easy to spin up Kubernetes clusters.</p>"},{"location":"create-cluster/#pre-requisites","title":"Pre-requisites","text":"<p>Download k0s binary from releases and push it to all the nodes you wish to connect to the cluster.</p> <p>That's it, really.</p>"},{"location":"create-cluster/#bootstrapping-controller-node","title":"Bootstrapping controller node","text":"<p>Create a configuration file if you wish to tune some of the settings.</p> <pre><code>$ k0s server -c k0s.yaml\n</code></pre> <p>That's it, really. k0s process will act as a \"supervisor\" for all the control plane components. In few seconds you'll have the control plane up-and-running.</p> <p>Naturally, to make k0s boot up the control plane when the node itself reboots you should really make the k0s process to be supervised by systemd or some other init system.</p>"},{"location":"create-cluster/#create-join-token","title":"Create join token","text":"<p>To be able to join workers into the cluster we need a token. The token embeds information with which we can enable mutual trust between the worker and controller(s) and allow the node to join the cluster as worker.</p> <p>To get a token run the following on one of the existing controller nodes: <pre><code>k0s token create --role=worker\n</code></pre></p> <p>This will output a long token which we will use to join the worker. To enhance security, we can also set an expiration time on the tokens by using: <pre><code>k0s token create --role=worker --expiry=\"100h\"\n</code></pre></p>"},{"location":"create-cluster/#joining-workers-to-cluster","title":"Joining worker(s) to cluster","text":"<p>To join the worker we need to run k0s in worker mode with the token from previous step: <pre><code>$ k0s worker \"long-join-token\"\n</code></pre></p> <p>That's it, really.</p> <p>Naturally, to make k0s boot up the worker components when the node itself reboots you should really make the k0s process to be supervised by systemd or some other init system.</p>"},{"location":"create-cluster/#tokens","title":"Tokens","text":"<p>The tokens are actually base64 encoded kubeconfigs. </p> <p>Why: - well defined structure - can be used directly as bootstrap auth configs for kubelet - embeds CA info for mutual trust</p> <p>The actual bearer token embedded in the kubeconfig is a bootstrap token. For controller join token and for worker join token we use different usage attributes so we can make sure we can validate the token role on the controller side.</p>"},{"location":"create-cluster/#join-controller-node","title":"Join controller node","text":"<p>To be able to join a new controller node into the cluster you must be using either etcd or some externalized data store (MySQL or Postgres) via kine. Also make sure the configurations match for the data storage on all controller nodes.</p> <p>To create a join token for the new controller, run the following on existing controller node: <pre><code>k0s token create --role=controller --expiry=1h\n</code></pre></p> <p>On the new controller, run: <pre><code>k0s server \"long-join-token\"\n</code></pre></p>"},{"location":"create-cluster/#adding-a-cluster-user","title":"Adding a Cluster User","text":"<p>To add a user to cluster, use the kubeconfig create command. This will output a kubeconfig for the user, which can be used for authentication.</p> <p>On the controller, run the following to generate a kubeconfig for a user:</p> <pre><code>k0s kubeconfig create [username]\n</code></pre>"},{"location":"create-cluster/#enabling-access-to-cluster-resources","title":"Enabling Access to Cluster Resources","text":"<p>To allow the user access to the cluster, the user needs to be created with the <code>system:masters</code> group: <pre><code>clusterUser=\"testUser\"\nk0s kubeconfig create --groups \"system:masters\" $clusterUser &gt; ~/.kube/config\n</code></pre></p> <p>Create the proper roleBinding, to allow the user access to the resources: <pre><code>kubectl create clusterrolebinding $clusterUser-admin-binding --clusterrole=admin --user=$clusterUser\n</code></pre></p>"},{"location":"create-cluster/#service-and-log-setup","title":"Service and Log Setup","text":"<p>k0s install sub-command was created as a helper command to allow users to easily install k0s as a service. For more information, read here.</p>"},{"location":"create-cluster/#enabling-shell-completion","title":"Enabling Shell Completion","text":"<p>The k0s completion script for Bash, zsh, fish and powershell can be generated with the command <code>k0s completion &lt; shell &gt;</code>. Sourcing the completion script in your shell enables k0s autocompletion.</p>"},{"location":"create-cluster/#bash","title":"Bash","text":"<pre><code>echo 'source &lt;(k0s completion bash)' &gt;&gt;~/.bashrc\n</code></pre> <pre><code># To load completions for each session, execute once:\n$ k0s completion bash &gt; /etc/bash_completion.d/k0s\n</code></pre>"},{"location":"create-cluster/#zsh","title":"Zsh","text":"<p>If shell completion is not already enabled in your environment you will need to enable it.  You can execute the following once: <pre><code>$ echo \"autoload -U compinit; compinit\" &gt;&gt; ~/.zshrc\n</code></pre> <pre><code># To load completions for each session, execute once:\n$ k0s completion zsh &gt; \"${fpath[1]}/_k0s\"\n</code></pre> You will need to start a new shell for this setup to take effect.</p>"},{"location":"create-cluster/#fish","title":"Fish","text":"<p><pre><code>$ k0s completion fish | source\n</code></pre> <pre><code># To load completions for each session, execute once:\n$ k0s completion fish &gt; ~/.config/fish/completions/k0s.fish\n</code></pre></p>"},{"location":"custom-cri-runtime/","title":"Custom CRI runtime","text":"<p>k0s supports users bringing their own CRI runtime (for example, docker). In which case, k0s will not start nor manage the runtime, and it is fully up to the user to configure it properly.</p> <p>To run a k0s worker with a custom CRI runtime use the option <code>--cri-socket</code>.  It takes input in the form of <code>&lt;type&gt;:&lt;socket&gt;</code> where:</p> <ul> <li><code>type</code>: Either <code>remote</code> or <code>docker</code>. Use <code>docker</code> for pure docker setup, <code>remote</code> for anything else.</li> <li><code>socket</code>: Path to the socket, examples: <code>unix:///var/run/docker.sock</code></li> </ul> <p>To run k0s with pre-existing docker setup run the worker with <code>k0s worker --cri-socket docker:unix:///var/run/docker.sock &lt;token&gt;</code>.</p> <p>When <code>docker</code> is used as a runtime, k0s will configure kubelet to create the dockershim socket at <code>/var/run/dockershim.sock</code>.</p>"},{"location":"experimental-windows/","title":"Windows support","text":""},{"location":"experimental-windows/#experimental-status","title":"Experimental status","text":"<p>Windows support feature is under active development and MUST BE considered as highly experemential. </p>"},{"location":"experimental-windows/#build","title":"Build","text":"<p><code>make clean k0s.exe</code></p> <p>This should create k0s.exe with staged kubelet.exe and kube-proxy.exe</p>"},{"location":"experimental-windows/#description","title":"Description","text":"<p>the k0s.exe supervises kubelet.exe and kube-proxy.exe During the first run calico install script created as <code>C:\\bootstrap.ps1</code></p> <p>The bootstrap script downloads the calico binaries, builds pause container and set ups vSwitch settings.</p>"},{"location":"experimental-windows/#running","title":"Running","text":"<p>It is expected to have docker EE installed on the windows node (we need it during the initial calico set up)</p> <pre><code>C:\\&gt;k0s.exe worker --cri-socket=docker:tcp://127.0.0.1:2375 --cidr-range=&lt;cidr_range&gt; --cluster-dns=&lt;clusterdns&gt; --api-server=&lt;k0s api&gt; &lt;token&gt;\n</code></pre> <p>Cluster MUST have at least one linux worker node.</p> <p>Cluster control plane must be inited with proper config (see section below)</p>"},{"location":"experimental-windows/#configuration","title":"Configuration","text":""},{"location":"experimental-windows/#strict-affinity","title":"Strict-affinity","text":"<p>To run windows node we need to have strict affinity enabled.</p> <p>There is a configuration field <code>spec.network.calico.withWindowsNodes</code>, equals false by default. If set to the true, the additional calico related manifest <code>/var/lib/k0s/manifests/calico/calico-IPAMConfig-ipamconfig.yaml</code> would be created with the following values</p> <p><pre><code>---\napiVersion: crd.projectcalico.org/v1\nkind: IPAMConfig\nmetadata:\n  name: default\nspec:\n  strictAffinity: true\n</code></pre> Another way is to use calicoctl manually: <pre><code>calicoctl ipam configure --strictaffinity=true\n</code></pre></p>"},{"location":"experimental-windows/#network-connectivity-in-aws","title":"Network connectivity in AWS","text":"<p>The network interface attached to your EC2 instance MUST have disabled \u201cChange Source/Dest. Check\u201d option. In AWS console option can be found on the Actions menu for a selected network interface.</p>"},{"location":"experimental-windows/#hacks","title":"Hacks","text":"<p>We need to figure out proper way to pass cluster settings from controller plane to worker.</p> <p>While we don't have it, there are CLI arguments: - cidr-range - cluster-dns - api-server </p>"},{"location":"experimental-windows/#some-useful-commands","title":"Some useful commands","text":"<p>Run pod with cmd.exe shell <pre><code>kubectl run win --image=hello-world:nanoserver --command=true -i --attach=true -- cmd.exe\n</code></pre></p> <p>Manifest for pod with IIS web-server <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: iis\nspec:\n  containers:\n  - name: iis\n    image: mcr.microsoft.com/windows/servercore/iis\n    imagePullPolicy: IfNotPresent\n</code></pre></p>"},{"location":"extensions/","title":"Cluster extensions","text":"<p>k0s allows users to use extensions to extend cluster functionality.</p> <p>At the moment the only supported type of extensions is helm based charts.</p> <p>The default configuration has no extensions.</p>"},{"location":"extensions/#helm-based-extensions","title":"Helm based extensions","text":""},{"location":"extensions/#configuration","title":"Configuration","text":"<p>Example.   <code>helm:   repositories:   - name: stable     url: https://charts.helm.sh/stable   - name: prometheus-community     url: https://prometheus-community.github.io/helm-charts   charts:   - name: prometheus-stack     chartname: prometheus-community/prometheus     version: \"11.16.8\"     values: |       storageSpec:         emptyDir:           medium: Memory     namespace: default <pre><code>By using the configuration above, the cluster would:\n\n- add stable and prometheus-community chart repositories\n- install the `prometheus-community/prometheus` chart of the specified version to the `default` namespace.\n\nThe chart installation is implemented by using CRD `helm.k0sproject.io/Chart`.\nFor every given helm extension the cluster creates a Chart CRD instance.\nThe cluster has a controller which monitors for the Chart CRDs, supporting the following operations:\n- install\n- upgrade\n- delete\n\nFor security reasons, the cluster operates only on Chart CRDs instantiated in the `kube-system` namespace, however, the target namespace could be any.\n\n#### CRD definition\n</code></pre> apiVersion: helm.k0sproject.io/v1beta1 kind: Chart metadata:   creationTimestamp: \"2020-11-10T14:17:53Z\"   generation: 2   labels:     k0s.k0sproject.io/stack: helm   name: k0s-addon-chart-test-addon   namespace: kube-system   resourceVersion: \"627\"   selfLink: /apis/helm.k0sproject.io/v1beta1/namespaces/kube-system/charts/k0s-addon-chart-test-addon   uid: ebe59ed4-1ff8-4d41-8e33-005b183651ed spec:   chartName: prometheus-community/prometheus   namespace: default   values: |     storageSpec:       emptyDir:         medium: Memory   version: 11.16.8 status:   appVersion: 2.21.0   namespace: default   releaseName: prometheus-1605017878   revision: 2   updated: 2020-11-10 14:18:08.235656 +0000 UTC m=+41.871656901   version: 11.16.8</code></p> <p>The <code>Chart.spec</code> defines the chart information.</p> <p>The <code>Chart.status</code> keeps the information about the last operation performed by the operator.</p>"},{"location":"install/","title":"Service and Logging","text":"<p>We've created the subcommand <code>k0s install</code> to allow users to easily install k0s as a service, and define its logging.</p> <p>This is an alpha state feature.</p>"},{"location":"install/#caveats","title":"Caveats","text":"<ul> <li>This command is strictly a helper command. It is not meant to provide a fully-automated solution, since you can run k0s in multiple, very different ways.</li> <li>It configures your service set-up as either a worker or a server, and will have different tasks, depending on the role you pick.</li> <li>Supported services: OpenRC &amp; Systemd</li> </ul>"},{"location":"install/#server-setup","title":"Server setup","text":"<p>This is the default mode of operation. When a server role is picked, the installer will do the following:</p> <ul> <li>Create user accounts for the different components (see https://github.com/k0sproject/k0s/blob/main/pkg/apis/v1beta1/system.go#L6)</li> <li>Create a service file (OpenRC/Systemd) and redirects logging to <code>/var/log/k0s.log</code>.</li> <li>If the <code>--debug</code> flag is used, it will also pass this flag along to the service file.</li> <li><code>enable-worker</code> (single-node) setup is not supported. If you would like to run your service in that way, a possible solution would be to run <code>cmd install</code> as worker, and edit the startup command by hand.</li> </ul>"},{"location":"install/#worker-setup","title":"Worker Setup","text":"<ul> <li>A worker cannot run with any other user, other than <code>root</code>, so no special users will be created.</li> <li>The service file will include the <code>--token-file</code> flag, with a value that needs to be manually changed.</li> <li>If the <code>--debug</code> flag is used, it will also pass this flag along to the service file.</li> </ul>"},{"location":"install/#additional-documentation","title":"Additional Documentation","text":"<p>see: k0s install</p>"},{"location":"k0s-in-docker/","title":"k0s in Docker","text":"<p>We publish a k0s container image with every release. By default, we run both controller and worker in the same container to provide an easy local testing \"cluster\".</p> <p>You can run your own k0s-in-docker easily with: <pre><code>docker run -d --name k0s --hostname k0s --privileged -v /var/lib/k0s -p 6443:6443 docker.pkg.github.com/k0sproject/k0s/k0s:&lt;version&gt;\n</code></pre> Just grab the kubeconfig file with <code>docker exec k0s cat /var/lib/k0s/pki/admin.conf</code> and paste e.g. into Lens.</p>"},{"location":"k0s-in-docker/#running-workers","title":"Running workers","text":"<p>If you want to attach multiple workers nodes into the cluster you can run separate containers for each worker.</p> <p>First, we need a join token for the worker: <pre><code>token=$(docker exec -t -i k0s k0s token create --role=worker)\n</code></pre></p> <p>Then join a new worker by running the container with: <pre><code>docker run -d --name k0s-worker1 --hostname k0s-worker1 --privileged -v /var/lib/k0s docker.pkg.github.com/k0sproject/k0s/k0s:&lt;version&gt; k0s worker $token\n</code></pre></p> <p>Repeat for as many workers you need, and have resources for. :)</p>"},{"location":"k0s-in-docker/#docker-compose","title":"Docker Compose","text":"<p>You can also run k0s with Docker Compose: <pre><code>version: \"3.9\"\nservices:\nk0s:\ncontainer_name: k0s\nimage: docker.pkg.github.com/k0sproject/k0s/k0s:&lt;version&gt;\nhostname: k0s\nprivileged: true\nvolumes:\n- \"/var/lib/k0s\"\nports:\n- \"6443:6443\"\nnetwork_mode: \"bridge\"\n</code></pre></p>"},{"location":"k0s-in-docker/#known-limitations","title":"Known limitations","text":""},{"location":"k0s-in-docker/#no-custom-docker-networks","title":"No custom Docker networks","text":"<p>Currently, we cannot run k0s nodes if the containers are configured to use custom networks e.g. with <code>--net my-net</code>. This is caused by the fact that Docker sets up a custom DNS service within the network and that messes up CoreDNS. We know that there are some workarounds possible, but they are bit hackish. And on the other hand, running k0s cluster(s) in bridge network should not cause issues.</p>"},{"location":"k0s-single-node/","title":"k0s Single Node Quick Start","text":"<p>These instructions outline a quick method for running a local k0s master and worker in a single node.</p> <p>NOTE:  This method of running k0s is only recommended for dev, test or POC environments.</p>"},{"location":"k0s-single-node/#prepare-dependencies","title":"Prepare Dependencies","text":""},{"location":"k0s-single-node/#1-download-the-k0s-binary","title":"1. Download the k0s binary","text":"<pre><code>curl -sSLf https://get.k0s.sh | sh\n</code></pre>"},{"location":"k0s-single-node/#2-download-the-kubectl-binary","title":"2. Download the kubectl binary","text":"<pre><code>sudo curl --output /usr/local/sbin/kubectl -L \"https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl\"\n</code></pre>"},{"location":"k0s-single-node/#3-make-both-binaries-executable","title":"3. Make both binaries executable","text":"<pre><code>sudo chmod +x /usr/local/sbin/kubectl\nsudo chmod +x /usr/bin/k0s\n</code></pre>"},{"location":"k0s-single-node/#start-k0s","title":"Start k0s","text":""},{"location":"k0s-single-node/#1-create-the-k0s-config-directory","title":"1. Create the k0s config directory","text":"<pre><code>mkdir -p ${HOME}/.k0s\n</code></pre>"},{"location":"k0s-single-node/#2-generate-a-default-cluster-configuration","title":"2. Generate a default cluster configuration","text":"<pre><code>k0s default-config | tee ${HOME}/.k0s/k0s.yaml\n</code></pre>"},{"location":"k0s-single-node/#3-start-k0s","title":"3. Start k0s","text":"<pre><code>sudo k0s server -c ${HOME}/.k0s/k0s.yaml --enable-worker &amp;\n</code></pre>"},{"location":"k0s-single-node/#use-kubectl-to-access-k0s","title":"Use kubectl to access k0s","text":""},{"location":"k0s-single-node/#1-save-kubeconfig-for-user","title":"1. Save kubeconfig for user","text":"<pre><code>sudo cat /var/lib/k0s/pki/admin.conf | tee ~/.k0s/kubeconfig\n</code></pre>"},{"location":"k0s-single-node/#2-set-the-kubeconfig-environment-variable","title":"2. Set the KUBECONFIG environment variable","text":"<pre><code>export KUBECONFIG=\"${HOME}/.k0s/kubeconfig\"\n</code></pre>"},{"location":"k0s-single-node/#3-monitor-cluster-startup","title":"3. Monitor cluster startup","text":"<pre><code>kubectl get pods --all-namespaces\n</code></pre>"},{"location":"manifests/","title":"Manifest deployer","text":"<p>k0s embeds a manifest deployer on controllers which provides an easy way to deploy manifests automatically.</p> <p>By default k0s reads all manifests under <code>${DATADIR}/manifests</code> (default: <code>/var/lib/k0s/manifests</code>) and ensures their state matches the cluster state. When you remove a manifest file, k0s will automatically prune all the resources associated with it.</p> <p>Each directory that is a direct descendant of <code>${DATADIR}/manifests</code> is considered as its own \"stack\", but nested directories will be excluded from the stack mechanism.</p> <p>Note: k0s uses this mechanism for some of its internal in-cluster components and other resources. Make sure you only touch the manifests not managed by k0s.</p>"},{"location":"manifests/#future","title":"Future","text":"<p>We may in the future support nested directories, but those will not be considered stacks, but rather sub-resources of a parent stacks. Stacks are exclusively top-level.</p>"},{"location":"networking/","title":"k0s Networking","text":""},{"location":"networking/#in-cluster-networking","title":"In-cluster networking","text":"<p>k0s supports currently only Calico as the built-in in-cluster overlay network provider. A user can however opt-out of k0s managing the network setup by using a <code>custom</code> as the network type.</p> <p>Using <code>custom</code> network provider it is expected that the user sets up the networking. This can be achieved e.g. by pushing network provider manifests into <code>/var/lib/k0s/manifests</code> from where k0s controllers will pick them up and deploy into the cluster. More on the automatic manifest handling here.</p>"},{"location":"networking/#controllers-worker-communication","title":"Controller(s) - Worker communication","text":"<p>As one of the goals of k0s is to allow deployment of totally isolated control plane we cannot rely on the fact that there is an IP route between controller nodes and the pod overlay network. To enable this communication path, which is mandated by conformance tests, we use Egress service and konnectivity proxy to proxy the traffic from API server into worker nodes. This ansures that we can always fulfill all the Kubernetes API functionalities but still operate the control plane in total isolation from the workers.</p>"},{"location":"networking/#needed-open-ports-protocols","title":"Needed open ports &amp; protocols","text":"Protocol Port Service Direction Notes TCP 2380 etcd peers controller &lt;-&gt; controller TCP 6443 kube-apiserver Worker, CLI =&gt; controller authenticated kube API using kube TLS client certs, ServiceAccount tokens with RBAC UDP 4789 Calico worker &lt;-&gt; worker Calico VXLAN overlay TCP 10250 kubelet Master, Worker =&gt; Host <code>*</code> authenticated kubelet API for the master node <code>kube-apiserver</code> (and <code>heapster</code>/<code>metrics-server</code> addons) using TLS client certs TCP 9443 k0s-api controller &lt;-&gt; controller k0s controller join API, TLS with token auth TCP 8132,8133 konnectivity server worker &lt;-&gt; controller konnectivity is used as \"reverse\" tunnel between kube-apiserver and worker kubelets"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>There are few common cases we've seen where k0s fails to run properly. </p>"},{"location":"troubleshooting/#coredns-in-crashloop","title":"CoreDNS in crashloop","text":"<p>The most common case we've encountered so far has been CoreDNS getting into crashloop on the node(s).</p> <p>With kubectl you see something like this: <pre><code>$ kubectl get pod --all-namespaces\nNAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE\nkube-system   calico-kube-controllers-5f6546844f-25px6   1/1     Running   0          167m\nkube-system   calico-node-fwjx5                          1/1     Running   0          164m\nkube-system   calico-node-t4tx5                          1/1     Running   0          164m\nkube-system   calico-node-whwsg                          1/1     Running   0          164m\nkube-system   coredns-5c98d7d4d8-tfs4q                   1/1     Error     17         167m\nkube-system   konnectivity-agent-9jkfd                   1/1     Running   0          164m\nkube-system   konnectivity-agent-bvhdb                   1/1     Running   0          164m\nkube-system   konnectivity-agent-r6mzj                   1/1     Running   0          164m\nkube-system   kube-proxy-kr2r9                           1/1     Running   0          164m\nkube-system   kube-proxy-tbljr                           1/1     Running   0          164m\nkube-system   kube-proxy-xbw7p                           1/1     Running   0          164m\nkube-system   metrics-server-7d4bcb75dd-pqkrs            1/1     Running   0          167m\n</code></pre></p> <p>When you check the logs, it'll show something like this: <pre><code>$ kubectl -n kube-system logs coredns-5c98d7d4d8-tfs4q\nplugin/loop: Loop (127.0.0.1:55953 -&gt; :1053) detected for zone \".\", see https://coredns.io/plugins/loop#troubleshooting. Query: \"HINFO 4547991504243258144.3688648895315093531.\"\n</code></pre></p> <p>This is most often caused by systemd-resolved stub (or something similar) running locally and CoreDNS detects a possible loop with DNS queries.</p> <p>The easiest but most crude way to workaround is to disable the systemd-resolved stub and revert the hosts <code>/etc/resolv.conf</code> to original</p> <p>Read more at CoreDNS troubleshooting docs.</p>"},{"location":"troubleshooting/#k0s-server-fails-on-arm-boxes","title":"<code>k0s server</code> fails on ARM boxes","text":"<p>In the logs you probably see ETCD not starting up properly.</p> <p>Etcd is not fully supported on ARM architecture, thus you need to run <code>k0s server</code> and thus also etcd process with env <code>ETCD_UNSUPPORTED_ARCH=arm64</code>.</p> <p>As Etcd is not fully supported on ARM architecture it also means that k0s controlplane with etcd itself is not fully supported on ARM either.</p>"},{"location":"troubleshooting/#pods-pending-when-using-cloud-providers","title":"Pods pending when using cloud providers","text":"<p>Once we enable cloud provider support on kubelet on worker nodes, kubelet will automatically add a taint <code>node.cloudprovider.kubernetes.io/uninitialized</code> for the node. This tain will prevent normal workloads to be scheduled on the node untill the cloud provider controller actually runs second initialization on the node and removes the taint. This means that these nodes are not schedulable untill the cloud provider controller is actually succesfully running on the cluster.</p> <p>For troubleshooting your specific cloud provider see its documentation.</p>"},{"location":"troubleshooting/#k0s-not-working-with-read-only-usr","title":"k0s not working with read only <code>/usr</code>","text":"<p>By default k0s does not run on nodes where <code>/usr</code> is read only.</p> <p>This can be fixed by changing the default path for <code>volumePluginDir</code> in your k0s config. You will need to change to values, one for the kubelet itself, and one for Calico.</p> <p>Here is a snippet of an example config with the default values changed:</p> <pre><code>spec:\ncontrollerManager:\nextraArgs:\nflex-volume-plugin-dir: \"/etc/kubernetes/kubelet-plugins/volume/exec\"\nnetwork:\ncalico:\nflexVolumeDriverPath: /etc/k0s/kubelet-plugins/volume/exec/nodeagent~uds\nworkerProfiles:\n- name: coreos\nvalues:\nvolumePluginDir: /etc/k0s/kubelet-plugins/volume/exec/\n</code></pre> <p>With this config you can start your server as usual. Any workers will need to be started with</p> <p><code>k0s worker --profile coreos [TOKEN]</code></p>"},{"location":"cli/","title":"Index","text":""},{"location":"cli/#k0s","title":"k0s","text":"<p>k0s - Zero Friction Kubernetes</p>"},{"location":"cli/#synopsis","title":"Synopsis","text":"<p>k0s - The zero friction Kubernetes - https://k0sproject.io</p>"},{"location":"cli/#options","title":"Options","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n  -h, --help                     help for k0s\n  -l, --logging stringToString   Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])\n</code></pre>"},{"location":"cli/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s api  - Run the controller api</li> <li>k0s completion    - Generate completion script</li> <li>k0s default-config    - Output the default k0s configuration yaml to stdout</li> <li>k0s docs    - Generate Markdown docs for the k0s binary</li> <li>k0s etcd    - Manage etcd cluster</li> <li>k0s install  - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo)</li> <li>k0s kubeconfig    - Create a kubeconfig file for a specified user</li> <li>k0s server    - Run server</li> <li>k0s token  - Manage join tokens</li> <li>k0s version  - Print the k0s version</li> <li>k0s worker    - Run worker</li> </ul>"},{"location":"cli/k0s/","title":"k0s CLI Help Pages","text":""},{"location":"cli/k0s/#k0s","title":"k0s","text":"<p>k0s - Zero Friction Kubernetes</p>"},{"location":"cli/k0s/#synopsis","title":"Synopsis","text":"<p>k0s - The zero friction Kubernetes - https://k0sproject.io</p>"},{"location":"cli/k0s/#options","title":"Options","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n  -h, --help                     help for k0s\n  -l, --logging stringToString   Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])\n</code></pre>"},{"location":"cli/k0s/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s api  - Run the controller api</li> <li>k0s completion    - Generate completion script</li> <li>k0s default-config    - Output the default k0s configuration yaml to stdout</li> <li>k0s docs    - Generate Markdown docs for the k0s binary</li> <li>k0s etcd    - Manage etcd cluster</li> <li>k0s install  - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo)</li> <li>k0s kubeconfig    - Create a kubeconfig file for a specified user</li> <li>k0s server    - Run server</li> <li>k0s token  - Manage join tokens</li> <li>k0s version  - Print the k0s version</li> <li>k0s worker    - Run worker</li> </ul>"},{"location":"cli/k0s_api/","title":"K0s api","text":""},{"location":"cli/k0s_api/#k0s-api","title":"k0s api","text":"<p>Run the controller api</p> <pre><code>k0s api [flags]\n</code></pre>"},{"location":"cli/k0s_api/#options","title":"Options","text":"<pre><code>  -h, --help   help for api\n</code></pre>"},{"location":"cli/k0s_api/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n  -l, --logging stringToString   Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])\n</code></pre>"},{"location":"cli/k0s_api/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s  - k0s - Zero Friction Kubernetes</li> </ul>"},{"location":"cli/k0s_completion/","title":"K0s completion","text":""},{"location":"cli/k0s_completion/#k0s-completion","title":"k0s completion","text":"<p>Generate completion script</p>"},{"location":"cli/k0s_completion/#synopsis","title":"Synopsis","text":"<p>To load completions:</p> <p>Bash:</p> <p>$ source &lt;(k0s completion bash)</p>"},{"location":"cli/k0s_completion/#to-load-completions-for-each-session-execute-once","title":"To load completions for each session, execute once:","text":"<p>$ k0s completion bash &gt; /etc/bash_completion.d/k0s</p> <p>Zsh:</p>"},{"location":"cli/k0s_completion/#if-shell-completion-is-not-already-enabled-in-your-environment-you-will-need","title":"If shell completion is not already enabled in your environment you will need","text":""},{"location":"cli/k0s_completion/#to-enable-it-you-can-execute-the-following-once","title":"to enable it.  You can execute the following once:","text":"<p>$ echo \"autoload -U compinit; compinit\" &gt;&gt; ~/.zshrc</p>"},{"location":"cli/k0s_completion/#to-load-completions-for-each-session-execute-once_1","title":"To load completions for each session, execute once:","text":"<p>$ k0s completion zsh &gt; \"${fpath[1]}/_k0s\"</p>"},{"location":"cli/k0s_completion/#you-will-need-to-start-a-new-shell-for-this-setup-to-take-effect","title":"You will need to start a new shell for this setup to take effect.","text":"<p>Fish:</p> <p>$ k0s completion fish | source</p>"},{"location":"cli/k0s_completion/#to-load-completions-for-each-session-execute-once_2","title":"To load completions for each session, execute once:","text":"<p>$ k0s completion fish &gt; ~/.config/fish/completions/k0s.fish</p> <pre><code>k0s completion [bash|zsh|fish|powershell]\n</code></pre>"},{"location":"cli/k0s_completion/#options","title":"Options","text":"<pre><code>  -h, --help   help for completion\n</code></pre>"},{"location":"cli/k0s_completion/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n  -l, --logging stringToString   Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])\n</code></pre>"},{"location":"cli/k0s_completion/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s  - k0s - Zero Friction Kubernetes</li> </ul>"},{"location":"cli/k0s_default-config/","title":"K0s default config","text":""},{"location":"cli/k0s_default-config/#k0s-default-config","title":"k0s default-config","text":"<p>Output the default k0s configuration yaml to stdout</p> <pre><code>k0s default-config [flags]\n</code></pre>"},{"location":"cli/k0s_default-config/#options","title":"Options","text":"<pre><code>  -h, --help   help for default-config\n</code></pre>"},{"location":"cli/k0s_default-config/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n  -l, --logging stringToString   Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])\n</code></pre>"},{"location":"cli/k0s_default-config/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s  - k0s - Zero Friction Kubernetes</li> </ul>"},{"location":"cli/k0s_docs/","title":"K0s docs","text":""},{"location":"cli/k0s_docs/#k0s-docs","title":"k0s docs","text":"<p>Generate Markdown docs for the k0s binary</p> <pre><code>k0s docs [flags]\n</code></pre>"},{"location":"cli/k0s_docs/#options","title":"Options","text":"<pre><code>  -h, --help   help for docs\n</code></pre>"},{"location":"cli/k0s_docs/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n  -l, --logging stringToString   Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])\n</code></pre>"},{"location":"cli/k0s_docs/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s  - k0s - Zero Friction Kubernetes</li> </ul>"},{"location":"cli/k0s_etcd/","title":"K0s etcd","text":""},{"location":"cli/k0s_etcd/#k0s-etcd","title":"k0s etcd","text":"<p>Manage etcd cluster</p>"},{"location":"cli/k0s_etcd/#options","title":"Options","text":"<pre><code>  -h, --help   help for etcd\n</code></pre>"},{"location":"cli/k0s_etcd/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n  -l, --logging stringToString   Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])\n</code></pre>"},{"location":"cli/k0s_etcd/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s  - k0s - Zero Friction Kubernetes</li> <li>k0s etcd leave    - Sign off a given etc node from etcd cluster</li> <li>k0s etcd member-list    - Returns etcd cluster members list</li> </ul>"},{"location":"cli/k0s_etcd_leave/","title":"K0s etcd leave","text":""},{"location":"cli/k0s_etcd_leave/#k0s-etcd-leave","title":"k0s etcd leave","text":"<p>Sign off a given etc node from etcd cluster</p> <pre><code>k0s etcd leave [flags]\n</code></pre>"},{"location":"cli/k0s_etcd_leave/#options","title":"Options","text":"<pre><code>  -h, --help                  help for leave\n      --peer-address string   etcd peer address\n</code></pre>"},{"location":"cli/k0s_etcd_leave/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n  -l, --logging stringToString   Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])\n</code></pre>"},{"location":"cli/k0s_etcd_leave/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s etcd    - Manage etcd cluster</li> </ul>"},{"location":"cli/k0s_etcd_member-list/","title":"K0s etcd member list","text":""},{"location":"cli/k0s_etcd_member-list/#k0s-etcd-member-list","title":"k0s etcd member-list","text":"<p>Returns etcd cluster members list</p> <pre><code>k0s etcd member-list [flags]\n</code></pre>"},{"location":"cli/k0s_etcd_member-list/#options","title":"Options","text":"<pre><code>  -h, --help   help for member-list\n</code></pre>"},{"location":"cli/k0s_etcd_member-list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n  -l, --logging stringToString   Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])\n</code></pre>"},{"location":"cli/k0s_etcd_member-list/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s etcd    - Manage etcd cluster</li> </ul>"},{"location":"cli/k0s_install/","title":"K0s install","text":""},{"location":"cli/k0s_install/#k0s-install","title":"k0s install","text":"<p>Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo)</p> <pre><code>k0s install [flags]\n</code></pre>"},{"location":"cli/k0s_install/#options","title":"Options","text":"<pre><code>  -h, --help          help for install\n      --role string   node role (possible values: server or worker. In a single-node setup, a worker role should be used) (default \"server\")\n</code></pre>"},{"location":"cli/k0s_install/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n  -l, --logging stringToString   Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])\n</code></pre>"},{"location":"cli/k0s_install/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s  - k0s - Zero Friction Kubernetes</li> </ul>"},{"location":"cli/k0s_kubeconfig/","title":"K0s kubeconfig","text":""},{"location":"cli/k0s_kubeconfig/#k0s-kubeconfig","title":"k0s kubeconfig","text":"<p>Create a kubeconfig file for a specified user</p> <pre><code>k0s kubeconfig [command] [flags]\n</code></pre>"},{"location":"cli/k0s_kubeconfig/#options","title":"Options","text":"<pre><code>  -h, --help   help for kubeconfig\n</code></pre>"},{"location":"cli/k0s_kubeconfig/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n  -l, --logging stringToString   Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])\n</code></pre>"},{"location":"cli/k0s_kubeconfig/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s  - k0s - Zero Friction Kubernetes</li> <li>k0s kubeconfig admin    - Display Admin's Kubeconfig file</li> <li>k0s kubeconfig create  - Create a kubeconfig for a user</li> </ul>"},{"location":"cli/k0s_kubeconfig_admin/","title":"K0s kubeconfig admin","text":""},{"location":"cli/k0s_kubeconfig_admin/#k0s-kubeconfig-admin","title":"k0s kubeconfig admin","text":"<p>Display Admin's Kubeconfig file</p>"},{"location":"cli/k0s_kubeconfig_admin/#synopsis","title":"Synopsis","text":"<p>Print kubeconfig for the Admin user to stdout</p> <pre><code>k0s kubeconfig admin [command] [flags]\n</code></pre>"},{"location":"cli/k0s_kubeconfig_admin/#examples","title":"Examples","text":"<pre><code>    $ k0s kubeconfig admin &gt; ~/.kube/config\n    $ export KUBECONFIG=~/.kube/config\n    $ kubectl get nodes\n</code></pre>"},{"location":"cli/k0s_kubeconfig_admin/#options","title":"Options","text":"<pre><code>  -h, --help   help for admin\n</code></pre>"},{"location":"cli/k0s_kubeconfig_admin/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n  -l, --logging stringToString   Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])\n</code></pre>"},{"location":"cli/k0s_kubeconfig_admin/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s kubeconfig    - Create a kubeconfig file for a specified user</li> </ul>"},{"location":"cli/k0s_kubeconfig_create/","title":"K0s kubeconfig create","text":""},{"location":"cli/k0s_kubeconfig_create/#k0s-kubeconfig-create","title":"k0s kubeconfig create","text":"<p>Create a kubeconfig for a user</p>"},{"location":"cli/k0s_kubeconfig_create/#synopsis","title":"Synopsis","text":"<p>Create a kubeconfig with a signed certificate and public key for a given user (and optionally user groups) Note: A certificate once signed cannot be revoked for a particular user</p> <pre><code>k0s kubeconfig create [username] [flags]\n</code></pre>"},{"location":"cli/k0s_kubeconfig_create/#examples","title":"Examples","text":"<pre><code>    Command to create a kubeconfig for a user:\n    CLI argument:\n    $ k0s kubeconfig create [username]\n\n    optionally add groups:\n    $ k0s kubeconfig create [username] --groups [groups]\n</code></pre>"},{"location":"cli/k0s_kubeconfig_create/#options","title":"Options","text":"<pre><code>      --groups string   Specify groups\n  -h, --help            help for create\n</code></pre>"},{"location":"cli/k0s_kubeconfig_create/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n  -l, --logging stringToString   Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])\n</code></pre>"},{"location":"cli/k0s_kubeconfig_create/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s kubeconfig    - Create a kubeconfig file for a specified user</li> </ul>"},{"location":"cli/k0s_server/","title":"K0s server","text":""},{"location":"cli/k0s_server/#k0s-server","title":"k0s server","text":"<p>Run server</p> <pre><code>k0s server [join-token] [flags]\n</code></pre>"},{"location":"cli/k0s_server/#examples","title":"Examples","text":"<pre><code>    Command to associate master nodes:\n    CLI argument:\n    $ k0s server [join-token]\n\n    or CLI flag:\n    $ k0s server --token-file [path_to_file]\n    Note: Token can be passed either as a CLI argument or as a flag\n</code></pre>"},{"location":"cli/k0s_server/#options","title":"Options","text":"<pre><code>      --enable-worker       enable worker (default false)\n  -h, --help                help for server\n      --profile string      worker profile to use on the node (default \"default\")\n      --token-file string   Path to the file containing join-token.\n</code></pre>"},{"location":"cli/k0s_server/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n  -l, --logging stringToString   Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])\n</code></pre>"},{"location":"cli/k0s_server/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s  - k0s - Zero Friction Kubernetes</li> </ul>"},{"location":"cli/k0s_token/","title":"K0s token","text":""},{"location":"cli/k0s_token/#k0s-token","title":"k0s token","text":"<p>Manage join tokens</p> <pre><code>k0s token [flags]\n</code></pre>"},{"location":"cli/k0s_token/#options","title":"Options","text":"<pre><code>  -h, --help                help for token\n      --kubeconfig string   path to kubeconfig file [$KUBECONFIG]\n</code></pre>"},{"location":"cli/k0s_token/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n  -l, --logging stringToString   Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])\n</code></pre>"},{"location":"cli/k0s_token/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s  - k0s - Zero Friction Kubernetes</li> <li>k0s token create    - Create join token</li> </ul>"},{"location":"cli/k0s_token_create/","title":"K0s token create","text":""},{"location":"cli/k0s_token_create/#k0s-token-create","title":"k0s token create","text":"<p>Create join token</p> <pre><code>k0s token create [flags]\n</code></pre>"},{"location":"cli/k0s_token_create/#options","title":"Options","text":"<pre><code>      --expiry string   set duration time for token (default \"0\")\n  -h, --help            help for create\n      --role string     Either worker or controller (default \"worker\")\n      --wait            wait forever (default false)\n</code></pre>"},{"location":"cli/k0s_token_create/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n  -l, --logging stringToString   Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])\n</code></pre>"},{"location":"cli/k0s_token_create/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s token  - Manage join tokens</li> </ul>"},{"location":"cli/k0s_version/","title":"K0s version","text":""},{"location":"cli/k0s_version/#k0s-version","title":"k0s version","text":"<p>Print the k0s version</p> <pre><code>k0s version [flags]\n</code></pre>"},{"location":"cli/k0s_version/#options","title":"Options","text":"<pre><code>  -h, --help   help for version\n</code></pre>"},{"location":"cli/k0s_version/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n  -l, --logging stringToString   Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])\n</code></pre>"},{"location":"cli/k0s_version/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s  - k0s - Zero Friction Kubernetes</li> </ul>"},{"location":"cli/k0s_worker/","title":"K0s worker","text":""},{"location":"cli/k0s_worker/#k0s-worker","title":"k0s worker","text":"<p>Run worker</p> <pre><code>k0s worker [join-token] [flags]\n</code></pre>"},{"location":"cli/k0s_worker/#examples","title":"Examples","text":"<pre><code>    Command to add worker node to the master node:\n    CLI argument:\n    $ k0s worker [token]\n\n    or CLI flag:\n    $ k0s worker --token-file [path_to_file]\n    Note: Token can be passed either as a CLI argument or as a flag\n</code></pre>"},{"location":"cli/k0s_worker/#options","title":"Options","text":"<pre><code>      --cri-socket string       contrainer runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket]\n      --enable-cloud-provider   Whether or not to enable cloud provider support in kubelet\n  -h, --help                    help for worker\n      --profile string          worker profile to use on the node (default \"default\")\n      --token-file string       Path to the file containing token.\n</code></pre>"},{"location":"cli/k0s_worker/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n  -l, --logging stringToString   Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])\n</code></pre>"},{"location":"cli/k0s_worker/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s  - k0s - Zero Friction Kubernetes</li> </ul>"},{"location":"contributors/CODE_OF_CONDUCT/","title":"k0s Community Code of Conduct","text":"<p>k0s follows the CNCF Code of Conduct.</p>"},{"location":"contributors/github_workflow/","title":"Github Workflow","text":"<ul> <li>Fork The Project</li> <li>Adding the Forked Remote</li> <li>Create &amp; Rebase Your Feature Branch</li> <li>Commit &amp; Push</li> <li>Open a Pull Request<ul> <li>Get a code review</li> <li>Squash commits</li> <li>Push Your Final Changes</li> </ul> </li> </ul> <p>This guide assumes you have already cloned the upstream repo to your system via git clone, or via <code>go get github.com/k0sproject/k0s</code>.</p>"},{"location":"contributors/github_workflow/#fork-the-project","title":"Fork The Project","text":"<ol> <li>Go to http://github.com/k0sproject/k0s</li> <li>On the top, right-hand side, click on \"fork\" and select your username for the fork destination.</li> </ol>"},{"location":"contributors/github_workflow/#adding-the-forked-remote","title":"Adding the Forked Remote","text":"<p><pre><code>export GITHUB_USER={ your github's username }\n</code></pre> <pre><code>cd $WORKDIR/k0s\ngit remote add $GITHUB_USER git@github.com:${GITHUB_USER}/k0s.git\n\n# Prevent push to Upstream\ngit remote set-url --push origin no_push\n\n# Set your fork remote as a default push target\ngit push --set-upstream $GITHUB_USER main\n</code></pre></p> <p>Your remotes should look something like this: <pre><code>\u279c git remote -v\norigin  https://github.com/k0sproject/k0s (fetch)\norigin  no_push (push)\nmy_fork git@github.com:{ github_username }/k0s.git (fetch)\nmy_fork git@github.com:{ github_username }/k0s.git (push)\n</code></pre></p>"},{"location":"contributors/github_workflow/#create-rebase-your-feature-branch","title":"Create &amp; Rebase Your Feature Branch","text":"<p>Create a feature branch: <pre><code>git branch -b my_feature_branch\n</code></pre> Rebase your branch: <pre><code>git fetch origin\n\ngit rebase origin/main\nCurrent branch my_feature_branch is up to date.\n</code></pre> Please don't use <code>git pull</code> instead of the above <code>fetch / rebase</code>. <code>git pull</code> does a merge, which leaves merge commits. These make the commit history messy and violate the principle that commits ought to be individually understandable and useful.</p>"},{"location":"contributors/github_workflow/#commit-push","title":"Commit &amp; Push","text":"<p>Commit and sign your changes: <pre><code>git commit -m \"my commit title\" --signoff\n</code></pre> You can go back and edit/build/test some more, then <code>commit --amend</code> in a few cycles.</p> <p>When ready, push your changes to your fork's repository: <pre><code>git push --set-upstream my_fork my_feature_branch\n</code></pre></p>"},{"location":"contributors/github_workflow/#open-a-pull-request","title":"Open a Pull Request","text":"<p>Github Docs</p>"},{"location":"contributors/github_workflow/#get-a-code-review","title":"Get a code review","text":"<p>Once your pull request has been opened it will be assigned to one or more reviewers, and will go through a series of smoke tests.</p> <p>Commit changes made in response to review comments should be added to the same branch on your fork.</p> <p>Very small PRs are easy to review. Very large PRs are very difficult to review.</p>"},{"location":"contributors/github_workflow/#squashing-commits","title":"Squashing Commits","text":"<p>Commits on your branch should represent meaningful milestones or units of work. Small commits that contain typo fixes, rebases, review feedbacks, etc should be squashed.</p> <p>To do that, it's best to perform an interactive rebase:</p>"},{"location":"contributors/github_workflow/#example","title":"Example","text":"<p>If you PR has 3 commits, count backwards from your last commit using <code>HEAD~3</code>: <pre><code>git rebase -i HEAD~3\n</code></pre> Output would be similar to this: <pre><code>pick f7f3f6d Changed some code\npick 310154e fixed some typos\npick a5f4a0d made some review changes\n\n# Rebase 710f0f8..a5f4a0d onto 710f0f8\n#\n# Commands:\n# p, pick &lt;commit&gt; = use commit\n# r, reword &lt;commit&gt; = use commit, but edit the commit message\n# e, edit &lt;commit&gt; = use commit, but stop for amending\n# s, squash &lt;commit&gt; = use commit, but meld into previous commit\n# f, fixup &lt;commit&gt; = like \"squash\", but discard this commit's log message\n# x, exec &lt;command&gt; = run command (the rest of the line) using shell\n# b, break = stop here (continue rebase later with 'git rebase --continue')\n# d, drop &lt;commit&gt; = remove commit\n# l, label &lt;label&gt; = label current HEAD with a name\n# t, reset &lt;label&gt; = reset HEAD to a label\n# m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;]\n# .       create a merge commit using the original merge commit's\n# .       message (or the oneline, if no original merge commit was\n# .       specified). Use -c &lt;commit&gt; to reword the commit message.\n#\n# These lines can be re-ordered; they are executed from top to bottom.\n#\n# However, if you remove everything, the rebase will be aborted.\n#\n# Note that empty commits are commented out\n</code></pre> Use a command line text editor to change the word <code>pick</code> to <code>fixup</code> for the commits you want to squash, then save your changes and continue the rebase:</p> <p>Per the output above, you can see that: <pre><code>fixup &lt;commit&gt; = like \"squash\", but discard this commit's log message\n</code></pre> Which means that when rebased, the commit message \"fixed some typos\" will be removed, and squashed with the parent commit.</p>"},{"location":"contributors/github_workflow/#push-your-final-changes","title":"Push Your Final Changes","text":"<p>Once done, you can push the final commits to your branch: <pre><code>git push --force\n</code></pre> You can run multiple iteration of <code>rebase</code>/<code>push -f</code>, if needed.</p>"},{"location":"contributors/overview/","title":"Contributing to k0s","text":"<p>Thank you for taking the time to make a contribution to k0s. The following document is a set of guidelines and instructions for contributing to k0s.</p> <p>When contributing to this repository, please consider first discussing the change you wish to make by opening an issue.</p>"},{"location":"contributors/overview/#code-of-conduct","title":"Code of Conduct","text":"<p>Our code of conduct can be found in the link below. Please follow it in all your interactions with the project. - Code Of Conduct </p>"},{"location":"contributors/overview/#github-workflow","title":"Github Workflow","text":"<p>We Use Github Flow, so all code changes are tracked via Pull Requests. A detailed guide on the recommended workflow can be found below: - Github Workflow</p>"},{"location":"contributors/overview/#code-testing","title":"Code Testing","text":"<p>All submitted PRs go through a set of tests and reviews. You can run most of these tests before a PR is submitted. In fact, we recommend it, because it will save on many possible review iterations and automated tests. The testing guidelines can be found here: - Contributor's Guide to Testing</p>"},{"location":"contributors/overview/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed as followed:</p> <ul> <li>All content residing under the \"docs/\" directory of this repository is licensed under \"Creative Commons Attribution Share Alike 4.0 International\" (CC-BY-SA-4.0). See docs/LICENCE for details.</li> <li>Content outside of the above mentioned directories or restrictions above is available under the \"Apache License 2.0\".</li> </ul>"},{"location":"contributors/overview/#community","title":"Community","text":"<p>Some of you might have noticed we have official community blog hosted on Medium. If you are not yet following us, we'd like to invite you to do so now!  Make sure to follow us on Twitter as well \ud83d\ude0a We have also agreed to share Slack with Lens IDE team. They are close friends of the k0s crew, and many of the Kubernetes users are using Lens anyway, so it felt very natural for us to share their Slack. </p> <p>You\u2019ll find us on dedicated k0s channel. Please join the k0s Slack channel to hear the latest news, discussions and provide your feedback!</p>"},{"location":"contributors/testing/","title":"Testing","text":""},{"location":"contributors/testing/#testing-your-code","title":"Testing Your Code","text":"<p>k0s uses github actions to run automated tests on any PR, before merging. However, a PR will not be reviewed before all tests are green, so to save time and prevent your PR from going stale, it is best to test it before submitting the PR.</p>"},{"location":"contributors/testing/#run-local-verifications","title":"Run Local Verifications","text":"<p>Please run the following style and formatting commands and fix/check-in any changes:</p>"},{"location":"contributors/testing/#1-linting","title":"1. Linting","text":"<p>We use golangci-lint for style verification. In the repository's root directory, simply run: <pre><code>make lint\n</code></pre></p>"},{"location":"contributors/testing/#2-go-fmt","title":"2. Go fmt","text":"<pre><code>go fmt ./...\n</code></pre>"},{"location":"contributors/testing/#3-pre-submit-flight-checks","title":"3. Pre-submit Flight Checks","text":"<p>In the repository root directory, make sure that:</p> <ul> <li><code>make build</code> runs successfully.</li> <li><code>make check-basic</code> runs successfully.</li> <li><code>make check-unit</code> has no errors.</li> <li><code>make check-hacontrolplane</code> runs successfully. </li> </ul> <p>Please note that this last test is prone to \"flakiness\", so it might fail on occasion. If it fails constantly, take a deeper look at your code to find the source of the problem.</p> <p>If you find that all tests passed, you may open a pull request upstream.</p>"},{"location":"contributors/testing/#opening-a-pull-request","title":"Opening A Pull Request","text":""},{"location":"contributors/testing/#draft-mode","title":"Draft Mode","text":"<p>You may open a pull request in draft mode. All automated tests will still run against the PR, but the PR will not be assigned for review. Once a PR is ready for review, transition it from Draft mode, and code owners will be notified.</p>"},{"location":"contributors/testing/#conformance-testing","title":"Conformance Testing","text":"<p>Once a PR has been reviewed and all other tests have passed, a code owner will run a full end-to-end conformance test against the PR. This is usually the last step before merging.</p>"},{"location":"contributors/testing/#pre-requisites-for-pr-merge","title":"Pre-Requisites for PR Merge","text":"<p>In order for a PR to be merged, the following conditions should exist: 1. The PR has passed all the automated tests (style, build &amp; conformance tests). 2. PR commits have been signed with the <code>--signoff</code> option. 3. PR was reviewed and approved by a code owner. 4. PR is rebased against upstream's main branch.</p>"},{"location":"examples/traefik-ingress/","title":"Installing the Traefik Ingress Controller on k0s","text":"<p>In this tutorial, you'll learn how to configure k0s with the Traefik ingress controller, a MetalLB service loadbalancer, and deploy the Traefik Dashboard along with a service example. Utilizing the extensible bootstrapping functionality with Helm,  it's as simple as adding the right extensions to the <code>k0s.yaml</code> file  when configuring your cluster.</p>"},{"location":"examples/traefik-ingress/#configuring-k0syaml","title":"Configuring k0s.yaml","text":"<p>Modify your <code>k0s.yaml</code> file to include the Traefik and MetalLB helm charts as extensions, and these will install during the cluster's bootstrap.</p> <p>Note: You may want to have a small range of IP addresses that are addressable on your network, preferably outside the assignment pool allocated by your DHCP server. Providing an addressable range should allow you to access your LoadBalancer and Ingress services from anywhere on your local network. However, any valid IP range should work locally on your machine.</p> <pre><code>extensions:\nhelm:\nrepositories:\n- name: traefik\nurl: https://helm.traefik.io/traefik\n- name: bitnami\nurl: https://charts.bitnami.com/bitnami\ncharts:\n- name: traefik\nchartname: traefik/traefik\nversion: \"9.11.0\"\nnamespace: default\n- name: metallb\nchartname: bitnami/metallb\nversion: \"1.0.1\"\nnamespace: default\nvalues: |2\nconfigInline:\naddress-pools:\n- name: generic-cluster-pool\nprotocol: layer2\naddresses:\n- 192.168.0.5-192.168.0.10\n</code></pre> <p>Providing a range of IPs for MetalLB that are addressable on your LAN is suggested  if you want to access LoadBalancer and Ingress services from anywhere on your local network.</p>"},{"location":"examples/traefik-ingress/#retrieving-the-load-balancer-ip","title":"Retrieving the Load Balancer IP","text":"<p>Once you've started your cluster, you should confirm the deployment of Traefik and MetalLB. Executing a <code>kubectl get all</code> should include a response with the <code>metallb</code> and <code>traefik</code> resources,  along with a service loadbalancer that has an <code>EXTERNAL-IP</code> assigned to it.  See the example below:</p> <pre><code>root@k0s-host \u279c kubectl get all\nNAME                                                 READY   STATUS    RESTARTS   AGE\npod/metallb-1607085578-controller-864c9757f6-bpx6r   1/1     Running   0          81s\npod/metallb-1607085578-speaker-245c2                 1/1     Running   0          60s\npod/traefik-1607085579-77bbc57699-b2f2t              1/1     Running   0          81s\n\nNAME                         TYPE           CLUSTER-IP       EXTERNAL-IP      PORT(S)                      AGE\nservice/kubernetes           ClusterIP      10.96.0.1        &lt;none&gt;           443/TCP                      96s\nservice/traefik-1607085579   LoadBalancer   10.105.119.102   192.168.0.5      80:32153/TCP,443:30791/TCP   84s\n\nNAME                                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\ndaemonset.apps/metallb-1607085578-speaker   1         1         1       1            1           kubernetes.io/os=linux   87s\n\nNAME                                            READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/metallb-1607085578-controller   1/1     1            1           87s\ndeployment.apps/traefik-1607085579              1/1     1            1           84s\n\nNAME                                                       DESIRED   CURRENT   READY   AGE\nreplicaset.apps/metallb-1607085578-controller-864c9757f6   1         1         1       81s\nreplicaset.apps/traefik-1607085579-77bbc57699              1         1         1       81s\n</code></pre> <p>Take note of the <code>EXTERNAL-IP</code> given to the <code>service/traefik-n</code> LoadBalancer. In this example, <code>192.168.0.5</code> has been assigned and can be used to access services via the Ingress proxy:</p> <pre><code>NAME                         TYPE           CLUSTER-IP       EXTERNAL-IP      PORT(S)                      AGE\nservice/traefik-1607085579   LoadBalancer   10.105.119.102   192.168.0.5      80:32153/TCP,443:30791/TCP   84s\n# Recieving a 404 response here is normal, as you've not configured any Ingress resources to respond yet\nroot@k0s-host \u279c curl http://192.168.0.5\n404 page not found\n</code></pre>"},{"location":"examples/traefik-ingress/#deploy-and-access-the-traefik-dashboard","title":"Deploy and access the Traefik Dashboard","text":"<p>Now that you have an available and addressable load balancer on your cluster,  you can quickly deploy the Traefik dashboard and access it from anywhere on your local network  (provided that you configured MetalLB with an addressable range).</p> <p>Create the Traefik Dashboard IngressRoute  in a YAML file:</p> <pre><code>apiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\nname: dashboard\nspec:\nentryPoints:\n- web\nroutes:\n- match: PathPrefix(`/dashboard`) || PathPrefix(`/api`)\nkind: Rule\nservices:\n- name: api@internal\nkind: TraefikService\n</code></pre> <p>Next, deploy the resource:</p> <pre><code>root@k0s-host \u279c kubectl apply -f traefik-dashboard.yaml\ningressroute.traefik.containo.us/dashboard created\n</code></pre> <p>Once deployed, you should be able to access the dashboard using the <code>EXTERNAL-IP</code>  that you noted above by visiting <code>http://192.168.0.5</code> in your browser:</p> <p></p> <p>Now, create a simple <code>whoami</code> Deployment, Service,  and Ingress manifest:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: whoami-deployment\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: whoami\ntemplate:\nmetadata:\nlabels:\napp: whoami\nspec:\ncontainers:\n- name: whoami-container\nimage: containous/whoami\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: whoami-service\nspec:\nports:\n- name: http\ntargetPort: 80\nport: 80\nselector:\napp: whoami\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\nname: whoami-ingress\nspec:\nrules:\n- http:\npaths:\n- path: /whoami\npathType: Exact\nbackend:\nservice:\nname: whoami-service\nport:\nnumber: 80\n</code></pre> <p>Once you've created this, apply and test it:</p> <pre><code># apply the manifests\nroot@k0s-host \u279c kubectl apply -f whoami.yaml\ndeployment.apps/whoami-deployment created\nservice/whoami-service created\ningress.networking.k8s.io/whoami-ingress created\n# test the ingress and service\nroot@k0s-host \u279c curl http://192.168.0.5/whoami\nHostname: whoami-deployment-85bfbd48f-7l77c\nIP: 127.0.0.1\nIP: ::1\nIP: 10.244.214.198\nIP: fe80::b049:f8ff:fe77:3e64\nRemoteAddr: 10.244.214.196:34858\nGET /whoami HTTP/1.1\nHost: 192.168.0.5\nUser-Agent: curl/7.68.0\nAccept: */*\nAccept-Encoding: gzip\nX-Forwarded-For: 192.168.0.82\nX-Forwarded-Host: 192.168.0.5\nX-Forwarded-Port: 80\nX-Forwarded-Proto: http\nX-Forwarded-Server: traefik-1607085579-77bbc57699-b2f2t\nX-Real-Ip: 192.168.0.82\n</code></pre>"},{"location":"examples/traefik-ingress/#summary","title":"Summary","text":"<p>From here, it's possible to use 3rd party tools, such as ngrok, to go further and expose your LoadBalancer to the world. Doing so then enables dynamic certificate provisioning through Let's Encrypt utilizing either cert-manager or Traefik's own built-in ACME provider. This guide should have given you a general idea of getting started with Ingress on k0s and exposing your applications and services quickly.</p>"},{"location":"internal/host-dependencies/","title":"Host Dependencies","text":"<p>The goal of k0s is to only depend on kernel functionality. However, the kubelet currently has several host dependencies. Some of these may need to be bundled, but we should prefer upstream contributions to remove these dependencies.</p>"},{"location":"internal/host-dependencies/#list-of-hard-dependencies","title":"List of hard dependencies","text":"<ul> <li><code>find</code> -- PR by @ncopa to resolve this: https://github.com/kubernetes/kubernetes/pull/95189</li> <li><code>du</code> -- PR by @ncopa to resolve this: https://github.com/kubernetes/kubernetes/pull/95178 -- note that <code>du</code> dependency remains, but using POSIX-compliant argument </li> <li><code>nice</code></li> <li><code>iptables</code> -- as documented in https://github.com/k0sproject/k0s/issues/176 it is unclear whether <code>iptables</code> is needed. It appears to come from the <code>portmap</code> plugin, but the most robust solution may be to simply bundle <code>iptables</code> with k0s.</li> </ul>"},{"location":"internal/publishing_docs_using_mkdocs/","title":"Publishing Docs","text":"<p>We use mkdocs and mike for publishing docs to docs.k0sproject.io. This guide will provide a simple how-to on how to configure and deploy newly added docs to our website.</p>"},{"location":"internal/publishing_docs_using_mkdocs/#requirements","title":"Requirements","text":"<p>Install mike: https://github.com/jimporter/mike#installation</p>"},{"location":"internal/publishing_docs_using_mkdocs/#adding-a-new-link-to-the-navigation","title":"Adding A New link to the Navigation","text":"<ul> <li>All docs must live under the <code>docs</code> directory (I.E., changes to the main <code>README.md</code> are not reflected in the website).</li> <li>Add a new link under <code>nav</code> in the main mkdocs.yml file:     <code>nav:     - Overview: README.md     - Creating A Cluster:         - Quick Start Guide: create-cluster.md         - Run in Docker: k0s-in-docker.md         - Single node set-up: k0s-single-node.md     - Configuration Reference:         - Architecture: architecture.md         - Networking: networking.md         - Configuration Options: configuration.md         - Configuring Containerd: containerd_config.md         - Using A Custom CRI: custom-cri-runtime.md         - Using Cloud Providers: cloud-providers.md         - Running k0s with Traefik: examples/traefik-ingress.md         - Running k0s as a service: install.md         - k0s CLI Help Pages: cli/k0s.md     - Deploying Manifests: manifests.md     - FAQ: FAQ.md     - Troubleshooting: troubleshooting.md     - Contributing:         - Overview: contributors/overview.md         - Workflow: contributors/github_workflow.md         - Testing: contributors/testing.md</code></li> <li>Test your deployment locally, using <code>mike</code>:     <pre><code>version=\"v0.9.0\" # example\nmike deploy ${version}\n</code></pre><pre><code>mike set-default ${version}\n</code></pre> <p><pre><code>mike serve\n</code></pre>   Your local version should be served under: http://localhost:8000.</p> </li> </ul> <ul> <li>Once your changes are pushed to <code>main</code>, the \"Publish Docs\" jos will start running: https://github.com/k0sproject/k0s/actions?query=workflow%3A%22Publish+docs+via+GitHub+Pages%22</li> <li>You should see the deployment outcome in the <code>gh-pages</code> deployment page: https://github.com/k0sproject/k0s/deployments/activity_log?environment=github-pages    </li> </ul>"},{"location":"internal/upgrading-calico/","title":"Upgrading Calico","text":"<p>k0s bundles Kubernetes manifests for Calico. The manifests are retrieved from the official Calico docs.</p> <p>As fetching and modifying the entire multi-thousand line file is error-prone, you may follow these steps to upgrade Calico to the latest version:</p> <ol> <li>run <code>./get-calico.sh</code></li> <li>check the git diff to see if it looks sensible</li> <li>re-apply our manual adjustments (documented below)</li> <li>run <code>make bindata-manifests</code></li> <li>compile, pray, and test</li> <li>commit and create a PR</li> </ol>"},{"location":"internal/upgrading-calico/#manual-adjustments","title":"Manual Adjustments","text":"<p>Note: All manual adjustments should be fairly obvious from the git diff. This section attempts to provide a sanity checklist to go through and make sure we still have those changes applied. The code blocks in this section are our modifications, not the calico originals.</p> <p><code>static/manifests/calico/DaemonSet/calico-node.yaml</code>:</p> <ul> <li>variable-based support for both vxlan and ipip (search for <code>ipip</code> to find): <pre><code>{{- if eq .Mode \"ipip\" }}\n# Enable IPIP\n- name: CALICO_IPV4POOL_IPIP\n  value: \"Always\"\n# Enable or Disable VXLAN on the default IP pool.\n- name: CALICO_IPV4POOL_VXLAN\n  value: \"Never\"\n{{- else if eq .Mode \"vxlan\" }}\n# Disable IPIP\n- name: CALICO_IPV4POOL_IPIP\n  value: \"Never\"\n# Enable VXLAN on the default IP pool.\n- name: CALICO_IPV4POOL_VXLAN\n  value: \"Always\"\n- name: FELIX_VXLANPORT\n  value: \"{{ .VxlanPort }}\"\n- name: FELIX_VXLANVNI\n  value: \"{{ .VxlanVNI }}\"\n{{- end }}\n</code></pre></li> <li>iptables auto detect: <pre><code># Auto detect the iptables backend\n- name: FELIX_IPTABLESBACKEND\n  value: \"auto\"\n</code></pre></li> <li>variable-based WireGuard support: <pre><code>{{- if .EnableWireguard }}\n- name: FELIX_WIREGUARDENABLED\n  value: \"true\"\n{{- end }}\n</code></pre></li> <li>variable-based cluster CIDR: <pre><code>- name: CALICO_IPV4POOL_CIDR\n  value: \"{{ .ClusterCIDR }}\"\n</code></pre></li> <li>custom backend and MTU <pre><code># calico-config.yaml\ncalico_backend: \"{{ .Mode }}\"\nveth_mtu: \"{{ .MTU }}\"\n</code></pre></li> <li>remove bgp from <code>CLUSTER_TYPE</code> <pre><code>- name: CLUSTER_TYPE\n  value: \"k8s\"\n</code></pre></li> <li>disable BIRD checks on liveness and readiness as we don't support BGP by removing <code>-bird-ready</code> and <code>-bird-live</code> from the readiness and liveness probes respectively</li> </ul>"},{"location":"internal/upgrading-calico/#container-image-names","title":"Container image names","text":"<p>Instead of hardcoded image names and versions use placeholders to support configuration level settings. Following placeholders are used:</p> <ul> <li><code>CalicoCNIImage</code> for calico/cni</li> <li><code>CalicoFlexVolumeImage</code> for calico/pod2daemon-flexvol</li> <li><code>CalicoNodeImage</code> for calico/node</li> <li><code>CalicoKubeControllersImage</code> for calico/kube-controllers</li> </ul> <p>Example:  <pre><code># calico-node.yaml\nimage: {{ .CalicoCNIImage }}\n</code></pre></p>"}]}