{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>k0s is an all-inclusive Kubernetes distribution with all the required bells and whistles preconfigured to make building a Kubernetes clusters a matter of just copying an executable to every host and running it.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Packaged as a single static binary</li> <li>Self-hosted, isolated control plane</li> <li>Variety of storage backends: etcd, SQLite, MySQL (or any compatible), PostgreSQL</li> <li>Elastic control-plane</li> <li>Vanilla upstream Kubernetes</li> <li>Supports custom container runtimes (containerd is the default)</li> <li>Supports custom Container Network Interface (CNI) plugins (calico is the default)</li> <li>Supports x86_64 and arm64</li> </ul>"},{"location":"#join-the-community","title":"Join the Community","text":"<p>If you'd like to help build k0s, please check out our guide to Contributing and our Code of Conduct.</p>"},{"location":"#demo","title":"Demo","text":""},{"location":"#downloading-k0s","title":"Downloading k0s","text":"<p>Download k0s for linux amd64 and arm64 architectures.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Creating A k0s Cluster</p>"},{"location":"CODE_OF_CONDUCT/","title":"k0s Community Code Of Conduct","text":"<p>Please refer to our contributor code of conduct.</p>"},{"location":"FAQ/","title":"Frequently asked questions","text":""},{"location":"FAQ/#how-is-k0s-pronounced","title":"How is k0s pronounced?","text":"<p>kay-zero-ess</p>"},{"location":"FAQ/#how-do-i-run-a-single-node-cluster","title":"How do I run a single node cluster?","text":"<p>The cluster can be started with: `k0s controller --single</p> <p>See also the Getting Started tutorial.</p>"},{"location":"FAQ/#how-do-i-connect-to-the-cluster","title":"How do I connect to the cluster?","text":"<p>You find the config in <code>${DATADIR}/pki/admin.conf</code> (default: <code>/var/lib/k0s/pki/admin.conf</code>). Copy this file, and change the <code>localhost</code> entry to the public ip of the controller. Use the modified config to connect with kubectl: <pre><code>export KUBECONFIG=/path/to/admin.conf\nkubectl ...\n</code></pre></p>"},{"location":"FAQ/#why-doesnt-kubectl-get-nodes-list-the-k0s-controllers","title":"Why doesn't <code>kubectl get nodes</code> list the k0s controllers?","text":"<p>As a default, the control plane does not run kubelet at all, and will not accept any workloads, so the controller will not show up on the node list in kubectl. If you want your controller to accept workloads and run pods, you do so with: <code>k0s controller --enable-worker</code> (recommended only as test/dev/POC environments).</p>"},{"location":"FAQ/#is-k0sproject-really-open-source","title":"Is k0sproject really open source?","text":"<p>Yes, k0sproject is 100% open source. The source code is under Apache 2 and the documentation is under the Creative Commons License. Mirantis, Inc. is the main contributor and sponsor for this OSS project: building all the binaries from upstream, performing necessary security scans and calculating checksums so that it's easy and safe to use. The use of these ready-made binaries are subject to Mirantis EULA and the binaries include only open source software.</p>"},{"location":"airgap-install/","title":"Airgap install","text":"<p>In this tutorial we are going to cover k0s deployment in the environment with restricted internet access.  As usually in the k0sproject we aim to give you the best user experience with the least possible amount of friction.</p>"},{"location":"airgap-install/#prerequisites-for-exporting-images-bundle-from-running-cluster","title":"Prerequisites for exporting images bundle from running cluster","text":"<p>Working cluster with at least one controller (this cluster will be used to build images bundle). Please, refer to the getting started guide. You also need to have containerd CLI management tool <code>ctr</code> installed on the worker machine. Please refer to the ContainerD getting-started guide.</p>"},{"location":"airgap-install/#steps","title":"Steps","text":""},{"location":"airgap-install/#1-create-oci-bundle","title":"1. Create OCI bundle","text":"<p>k0s supports only uncompressed image bundles.</p>"},{"location":"airgap-install/#11-using-docker","title":"1.1 Using Docker","text":"<p>Use following commands to build OCI bundle by utilizing your docker environment.  <pre><code>## Pull images\n# k0s airgap list-images | xargs -I{} docker pull {}\n\n## Create bundle\n# docker image save $(k0s airgap list-images | xargs) -o bundle_file\n</code></pre></p>"},{"location":"airgap-install/#12-using-previously-set-up-k0s-worker","title":"1.2 Using previously set up k0s worker","text":"<p>To build OCI bundle with images we can utilize the fact that containerd pulls all images during the k0s worker normal bootstrap. Use following commands on the machine with previously installed k0s worker:</p> <pre><code>## The command k0s airgap list-images prints images used by the current setup\n## It respects the k0s.yaml values\n\n# export IMAGES=`k0s airgap list-images | xargs`\n# ctr --namespace k8s.io --address /run/k0s/containerd.sock images export bundle_file $IMAGES \n</code></pre> <p>Pay attention to the <code>address</code> and <code>namespace</code> arguments given to the <code>ctr</code> tool.</p>"},{"location":"airgap-install/#2-sync-bundle-file-with-airgapped-machine","title":"2. Sync bundle file with airgapped machine","text":"<p>Copy the <code>bundle_file</code> from the previous step to the target machine. Place the file under the <code>images</code> directory in the k0s data directory. Use following commands to place bundle into the default location:</p> <pre><code># mkdir -p /var/lib/k0s/images\n# cp bundle_file /var/lib/k0s/images/bundle_file\n</code></pre>"},{"location":"airgap-install/#3-ensure-pull-policy-in-the-k0syaml-optional","title":"3. Ensure pull policy in the k0s.yaml (optional)","text":"<p>Use the following k0s.yaml to force containerd to never pull images for the k0s components. Otherwise containerd pulls the images, which are not found from the bundle, from the internet. <pre><code>apiVersion: k0s.k0sproject.io/v1beta1\nkind: Cluster\nmetadata:\n  name: k0s\nspec:\n  images:\n    default_pull_policy: Never\n</code></pre></p>"},{"location":"airgap-install/#4-controller","title":"4. Controller","text":"<p>Set up the controller node as usual. Please refer to the getting started guide.</p>"},{"location":"airgap-install/#5-run-worker","title":"5. Run worker","text":"<p>Do the worker set up as usually on the airgapped machine. During the start up k0s worker will import all bundles from the <code>$K0S_DATA_DIR/images</code> before even starting <code>kubelet</code></p>"},{"location":"architecture/","title":"Architecture","text":"<p>Note: As with any young project, things change rapidly. Thus all the details in this architecture documentation may not be always up-to-date, but the high level concepts and patterns should still apply.</p>"},{"location":"architecture/#packaging","title":"Packaging","text":"<p>k0s is packaged as single, self-extracting binary which embeds Kubernetes binaries. This has many benefits: - Everything can be, and is, statically compiled - No OS level deps - No RPMs, dep's, snaps or any other OS specific packaging needed. Single \"package\" for all OSes - We can fully control the versions of each and every dependency</p> <p></p>"},{"location":"architecture/#control-plane","title":"Control plane","text":"<p>k0s as a single binary acts as the process supervisor for all other control plane components. This means there's no container engine or kubelet running on controllers (by default). Which means there is no way for a cluster user to schedule workloads onto controller nodes.</p> <p></p> <p>k0s creates, manages and configures each of the components. k0s runs all control plane components as \"naked\" processes. So on the controller node there's no container engine running.</p>"},{"location":"architecture/#storage","title":"Storage","text":"<p>Typically Kubernetes control plane supports only etcd as the datastore. In addition to etcd, k0s supports many other datastore options. This is achieved by including kine. Kine allows wide variety of backend data stores to be used such as MySQL, PostgreSQL, SQLite and dqlite. See more in storage documentation</p> <p>In case of k0s managed etcd, k0s manages the full lifecycle of the etcd cluster. This means for example that by joining a new controller node with <code>k0s controller \"long-join-token\"</code> k0s will automatically adjust the etcd cluster membership info to allow the new member to join the cluster.</p> <p>Note: Currently k0s cannot shrink the etcd cluster. For now user needs to manually remove the etcd member and only after that shutdown the k0s controller on the removed node.</p>"},{"location":"architecture/#worker-node","title":"Worker node","text":"<p>Like for the control plane, k0s creates and manages the core worker components as naked processes on the worker node. </p> <p>By default, k0s workers use containerd as a high-level runtime and runc as a low-level runtime. Custom runtimes are also supported as described here.</p>"},{"location":"cloud-providers/","title":"Using cloud providers","text":"<p>k0s builds Kubernetes components in \"providerless\" mode. This means that there is no cloud providers built into k0s managed Kubernetes components.</p> <p>This means the cloud providers have to be configured \"externally\". The following steps outline how to enable cloud providers support in your k0s cluster.</p> <p>For more information on running Kubernetes with cloud providers see the official documentation.</p>"},{"location":"cloud-providers/#enabling-cloud-provider-support-in-kubelet","title":"Enabling cloud provider support in kubelet","text":"<p>Even when all components are built with \"providerless\" mode, we need to be able to enable cloud provider \"mode\" for kubelet. This is done by running the workers with <code>--enable-cloud-provider=true</code>. This enables <code>--cloud-provider=external</code> on kubelet process.</p>"},{"location":"cloud-providers/#deploying-the-actual-cloud-provider","title":"Deploying the actual cloud provider","text":"<p>From Kubernetes point of view, it does not really matter how and where the cloud providers controller(s) are running. Of course the easiest way is to deploy them on the cluster itself. </p> <p>To deploy your cloud provider as k0s managed stack you can use the built-in manifest deployer. Simply drop all the needed manifests under e.g. <code>/var/lib/k0s/manifests/aws/</code> directory and k0s will deploy everything.</p> <p>Some cloud providers do need some configuration files to be present on all the nodes or some other pre-requisites. Consult your cloud providers documentation for needed steps.</p>"},{"location":"configuration/","title":"Configuration Options","text":"<p>k0s Control plane can be configured via a YAML config file. By default <code>k0s controller</code> command reads a file called <code>k0s.yaml</code> but can be told to read any yaml file via <code>--config</code> option.</p>"},{"location":"configuration/#configuration-file-reference","title":"Configuration file reference","text":"<p>Note: Many of the options configure things deep down in the \"stack\" on various components. So please make sure you understand what is being configured and whether or not it works in your specific environment.</p> <p>A full config file with defaults generated by the <code>k0s default-config</code> command:</p> <pre><code>apiVersion: k0s.k0sproject.io/v1beta1\nkind: Cluster\nmetadata:\nname: k0s\nspec:\napi:\nport: 6443\nk0sApiPort: 9443\nexternalAddress: my-lb-address.example.com\naddress: 192.168.68.104\nsans:\n- 192.168.68.104\nstorage:\ntype: etcd\netcd:\npeerAddress: 192.168.68.104\nnetwork:\npodCIDR: 10.244.0.0/16\nserviceCIDR: 10.96.0.0/12\nprovider: calico\ncalico:\nmode: vxlan\nvxlanPort: 4789\nvxlanVNI: 4096\nmtu: 1450\nwireguard: false\nflexVolumeDriverPath: /usr/libexec/k0s/kubelet-plugins/volume/exec/nodeagent~uds\nwithWindowsNodes: false\noverlay: Always\npodSecurityPolicy:\ndefaultPolicy: 00-k0s-privileged\ntelemetry:\ninterval: 10m0s\nenabled: true\ninstallConfig:\nusers:\netcdUser: etcd\nkineUser: kube-apiserver\nkonnectivityUser: konnectivity-server\nkubeAPIserverUser: kube-apiserver\nkubeSchedulerUser: kube-scheduler\nimages:\nkonnectivity:\nimage: us.gcr.io/k8s-artifacts-prod/kas-network-proxy/proxy-agent\nversion: v0.0.13\nmetricsserver:\nimage: gcr.io/k8s-staging-metrics-server/metrics-server\nversion: v0.3.7\nkubeproxy:\nimage: k8s.gcr.io/kube-proxy\nversion: v1.20.6\ncoredns:\nimage: docker.io/coredns/coredns\nversion: 1.7.0\ncalico:\ncni:\nimage: calico/cni\nversion: v3.16.2\nflexvolume:\nimage: calico/pod2daemon-flexvol\nversion: v3.16.2\nnode:\nimage: calico/node\nversion: v3.16.2\nkubecontrollers:\nimage: calico/kube-controllers\nversion: v3.16.2\nkonnectivity:\nagentPort: 8132\nadminPort: 8133\n</code></pre>"},{"location":"configuration/#specapi","title":"<code>spec.api</code>","text":"<ul> <li><code>externalAddress</code>: If k0s controllers are running behind a loadbalancer provide the loadbalancer address here. This will configure all cluster components to connect to this address and also configures this address to be used when joining new nodes into the cluster.</li> <li><code>address</code>: The local address to bind API on. Also used as one of the addresses pushed on the k0s create service certificate on the API. Defaults to first non-local address found on the node.</li> <li><code>sans</code>: List of additional addresses to push to API servers serving certificate</li> <li><code>extraArgs</code>: Map of key-values (strings) for any extra arguments you wish to pass down to Kubernetes api-server process</li> <li><code>port</code>: custom port for kube-api server to listen on (default value 6443)</li> <li><code>k0sApiPort</code>: custom port for k0s-api server to listen on (default value 9443)</li> </ul> <p>Keep in mind, in case if <code>port</code> and <code>k0sApiPort</code> are used with <code>externalAddress</code> setting, the LB serving at <code>externalAddress</code> must listen on the same ports.  </p>"},{"location":"configuration/#speccontrollermanager","title":"<code>spec.controllerManager</code>","text":"<ul> <li><code>extraArgs</code>: Map of key-values (strings) for any extra arguments you wish to pass down to Kubernetes controller manager process</li> </ul>"},{"location":"configuration/#specscheduler","title":"<code>spec.scheduler</code>","text":"<ul> <li><code>extraArgs</code>: Map of key-values (strings) for any extra arguments you wish to pass down to Kubernetes scheduler process</li> </ul>"},{"location":"configuration/#specstorage","title":"<code>spec.storage</code>","text":"<ul> <li><code>type</code>: Type of the data store, either <code>etcd</code> or <code>kine</code>.</li> <li><code>etcd.peerAddress</code>: Nodes address to be used for etcd cluster peering.</li> <li><code>kine.dataSource</code>: kine datasource URL.</li> </ul> <p>Using type <code>etcd</code> will make k0s to create and manage an elastic etcd cluster within the controller nodes.</p>"},{"location":"configuration/#specnetwork","title":"<code>spec.network</code>","text":"<ul> <li><code>provider</code>: Network provider, either <code>calico</code>, <code>kuberouter</code> or <code>custom</code>. In case of <code>custom</code> user can push any network provider. (default <code>kuberouter</code>)</li> <li><code>podCIDR</code>: Pod network CIDR to be used in the cluster</li> <li><code>serviceCIDR</code>: Network CIDR to be used for cluster VIP services.</li> </ul> <p>Note: In case of custom network it's fully in users responsibility to configure ALL the CNI related setups. This includes the CNI provider itself plus all the host levels setups it might need such as CNI binaries.</p> <p>Note: After the cluster has been initialized with one network provider, it is not currently supported to change the network provider.</p>"},{"location":"configuration/#specnetworkcalico","title":"<code>spec.network.calico</code>","text":"<ul> <li><code>mode</code>: <code>vxlan</code> (default) or <code>ipip</code></li> <li><code>vxlanPort</code>: The UDP port to use for VXLAN (default <code>4789</code>)</li> <li><code>vxlanVNI</code>: The virtual network ID to use for VXLAN. (default: <code>4096</code>)</li> <li><code>mtu</code>: MTU to use for overlay network (default <code>1450</code>)</li> <li><code>wireguard</code>: enable wireguard based encryption (default <code>false</code>). Your host system must be wireguard ready. See https://docs.projectcalico.org/security/encrypt-cluster-pod-traffic for details.</li> <li><code>flexVolumeDriverPath</code>: The host path to use for Calicos flex-volume-driver (default: <code>/usr/libexec/k0s/kubelet-plugins/volume/exec/nodeagent~uds</code>). This should only need to be changed if the default path is unwriteable. See https://github.com/projectcalico/calico/issues/2712 for details. This option should ideally be paired with a custom volumePluginDir in the profile used on your worker nodes.</li> <li><code>ipAutodetectionMethod</code>: To force non-default behaviour for Calico to pick up the interface for pod network inter-node routing. (default <code>\"\"</code>, i.e. not set so Calico will use it's own defaults) See more at: https://docs.projectcalico.org/reference/node/configuration#ip-autodetection-methods</li> </ul>"},{"location":"configuration/#specnetworkkuberouter","title":"<code>spec.network.kuberouter</code>","text":"<ul> <li><code>autoMTU</code>: Autodetection of used MTU (default <code>true</code>)</li> <li><code>mtu</code>: Override MTU setting, if set <code>autoMTU</code> needs to be set to <code>false</code></li> <li><code>peerRouterIPs</code>: comma separated list of global peer addresses</li> <li><code>peerRouterASNs</code>: comma separated list of global peer ASNs</li> </ul> <p>Kube-router allows many aspects of the networking to be configured per node, service and pod. For further details see kube-router user guide</p>"},{"location":"configuration/#specpodsecuritypolicy","title":"<code>spec.podSecurityPolicy</code>","text":"<p>Configures the default psp to be set. k0s creates two PSPs out of box:</p> <ul> <li><code>00-k0s-privileged</code> (default): no restrictions, always also used for Kubernetes/k0s level system pods</li> <li><code>99-k0s-restricted</code>: no host namespaces or root users allowed, no bind mounts from host</li> </ul> <p>As a user you can of course create any supplemental PSPs and bind them to users / access accounts as you need.</p>"},{"location":"configuration/#specworkerprofiles","title":"<code>spec.workerProfiles</code>","text":"<p>Array of <code>spec.workerProfiles.workerProfile</code> Each element has following properties: - <code>name</code>: string, name, used as profile selector for the worker process - <code>values</code>: mapping object</p> <p>For each profile the control plane will create separate ConfigMap with kubelet-config yaml. Based on the <code>--profile</code> argument given to the <code>k0s worker</code> the corresponding ConfigMap would be used to extract <code>kubelet-config.yaml</code> from. <code>values</code> are recursively merged with default <code>kubelet-config.yaml</code></p> <p>There are a few fields that cannot be overridden: - <code>clusterDNS</code> - <code>clusterDomain</code> - <code>apiVersion</code> - <code>kind</code></p> <p>Example:</p> <pre><code>spec:\n  workerProfiles:\n    - name: custom-role\n      values:\n         key: value\n         mapping:\n             innerKey: innerValue\n</code></pre> <p>Custom volumePluginDir:</p> <pre><code>spec:\n  workerProfiles:\n    - name: custom-role\n      values:\n         volumePluginDir: /var/libexec/k0s/kubelet-plugins/volume/exec\n</code></pre>"},{"location":"configuration/#specimages","title":"<code>spec.images</code>","text":"<p>Each node under the <code>images</code> key has the same structure <pre><code>spec:\n  images:\n    konnectivity:\n      image: calico/kube-controllers\n      version: v3.16.2\n</code></pre></p> <p>Following keys are available:</p> <ul> <li><code>spec.images.konnectivity</code></li> <li><code>spec.images.metricsserver</code></li> <li><code>spec.images.kubeproxy</code></li> <li><code>spec.images.coredns</code></li> <li><code>spec.images.calico.cni</code></li> <li><code>spec.images.calico.flexvolume</code></li> <li><code>spec.images.calico.node</code></li> <li><code>spec.images.calico.kubecontrollers</code></li> <li><code>spec.images.repository</code></li> </ul> <p>If <code>spec.images.repository</code> is set and not empty, every image will be pulled from <code>images.repository</code> If <code>spec.images.default_pull_policy</code> is set ant not empty, it will be used as a pull policy for each bundled image.</p> <p>Example: <pre><code>images:\n  repository: \"my.own.repo\"\n  konnectivity:\n    image: calico/kube-controllers\n    version: v3.16.2\n  metricsserver:\n    image: gcr.io/k8s-staging-metrics-server/metrics-server\n    version: v0.3.7\n</code></pre></p> <p>In the runtime the image names will be calculated as <code>my.own.repo/calico/kube-controllers:v3.16.2</code> and <code>my.own.repo/k8s-staging-metrics-server/metrics-server</code>.</p> <p>This only affects the location where images are getting pulled, omitting an image specification here will not disable the component from being deployed.</p>"},{"location":"configuration/#specextensionshelm","title":"<code>spec.extensions.helm</code>","text":"<p>List of Helm repositories and charts to deploy during cluster bootstrap. For more information, see Helm Charts.</p>"},{"location":"configuration/#speckonnectivity","title":"<code>spec.konnectivity</code>","text":"<p>Konnectivity related settings</p> <ul> <li><code>agentPort</code> agent port to listen on (default 8132)</li> <li><code>adminPort</code> admin port to listen on (default 8133)</li> </ul>"},{"location":"configuration/#telemetry","title":"Telemetry","text":"<p>To build better end user experience we collect and send telemetry data from clusters. It is enabled by default and can be disabled by settings corresponding option as <code>false</code> The default interval is 10 minutes, any valid value for <code>time.Duration</code> string representation can be used as a value. Example <pre><code>spec:\n    telemetry:\n      interval: 2m0s\n      enabled: true\n</code></pre></p>"},{"location":"configuration/#configuration-validation","title":"Configuration Validation","text":"<p>k0s command-line interface has the ability to validate config syntax:</p> <pre><code>$ k0s validate config --config path/to/config/file\n</code></pre> <p><code>validate config</code> sub-command can validate the following:</p> <ol> <li>YAML formatting</li> <li>SANs addresses</li> <li>Network providers</li> <li>Worker profiles </li> </ol>"},{"location":"conformance-testing/","title":"Kubernetes conformance testing for k0s","text":"<p>We run the conformance testing for the last RC build for a release. Follow the instructions as the conformance testing repository.</p> <p>In a nutshell, you need to: - Setup k0s on some VMs/bare metal boxes - Download, if you do not already have, sonobuoy tool - Run the conformance tests with something like <code>sonobuoy run --mode=certified-conformance</code> - Wait for couple hours - Collect results</p>"},{"location":"containerd_config/","title":"containerd configuration","text":"<p>containerd is an industry-standard container runtime.</p> <p>NOTE: In most use cases changes to the containerd configuration will not be required. </p> <p>In order to make changes to containerd configuration first you need to generate a default containerd configuration by running: <pre><code>containerd config default &gt; /etc/k0s/containerd.toml\n</code></pre> This command will set the default values to <code>/etc/k0s/containerd.toml</code>. </p> <p><code>k0s</code> runs containerd with the following default values: <pre><code>/var/lib/k0s/bin/containerd \\\n    --root=/var/lib/k0s/containerd \\\n    --state=/var/lib/k0s/run/containerd \\\n    --address=/var/lib/k0s/run/containerd.sock \\\n    --config=/etc/k0s/containerd.toml\n</code></pre></p> <p>Before proceeding further, add the following default values to the configuration file: <pre><code>version = 2\nroot = \"/var/lib/k0s/containerd\"\nstate = \"/var/lib/k0s/run/containerd\"\n...\n\n[grpc]\n  address = \"/var/lib/k0s/run/containerd.sock\"\n</code></pre></p> <p>Next if you want to change CRI look into this section</p> <p><code>[plugins.\"io.containerd.runtime.v1.linux\"]     shim = \"containerd-shim\"     runtime = \"runc\"</code></p>"},{"location":"containerd_config/#using-gvisor","title":"Using gVisor","text":"<p>gVisor is an application kernel, written in Go, that implements a substantial portion of the Linux system call interface. It provides an additional layer of isolation between running applications and the host operating system.</p> <p>First you must install the needed gVisor binaries into the host. <pre><code>(\nset -e\n  URL=https://storage.googleapis.com/gvisor/releases/release/latest\n  wget ${URL}/runsc ${URL}/runsc.sha512 \\\n${URL}/gvisor-containerd-shim ${URL}/gvisor-containerd-shim.sha512 \\\n${URL}/containerd-shim-runsc-v1 ${URL}/containerd-shim-runsc-v1.sha512\n  sha512sum -c runsc.sha512 \\\n-c gvisor-containerd-shim.sha512 \\\n-c containerd-shim-runsc-v1.sha512\n  rm -f *.sha512\n  chmod a+rx runsc gvisor-containerd-shim containerd-shim-runsc-v1\n  sudo mv runsc gvisor-containerd-shim containerd-shim-runsc-v1 /usr/local/bin\n)\n</code></pre></p> <p>See gVisor install docs</p> <p>Next we need to prepare the config for <code>k0s</code> managed containerD to utilize gVisor as additional runtime: <pre><code>cat &lt;&lt;EOF | sudo tee /etc/k0s/containerd.toml\ndisabled_plugins = [\"restart\"]\n[plugins.linux]\n  shim_debug = true\n[plugins.cri.containerd.runtimes.runsc]\n  runtime_type = \"io.containerd.runsc.v1\"\nEOF\n</code></pre></p> <p>Then we can start and join the worker as normally into the cluster: <pre><code>k0s worker $token\n</code></pre></p> <p>By default containerd uses nromal runc as the runtime. To make gVisor runtime usable for workloads we must register it to Kubernetes side: <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: node.k8s.io/v1beta1\nkind: RuntimeClass\nmetadata:\n  name: gvisor\nhandler: runsc\nEOF\n</code></pre></p> <p>After this we can use it for our workloads: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: nginx-gvisor\nspec:\nruntimeClassName: gvisor\ncontainers:\n- name: nginx\nimage: nginx\n</code></pre></p> <p>We can verify the created nginx pod is actually running under gVisor runtime: <pre><code># kubectl exec nginx-gvisor -- dmesg | grep -i gvisor\n[    0.000000] Starting gVisor...\n</code></pre></p>"},{"location":"containerd_config/#using-custom-nvidia-container-runtime","title":"Using custom <code>nvidia-container-runtime</code>","text":"<p>By default CRI is set tu runC and if you want to configure Nvidia GPU support you will have to replace <code>runc</code> with <code>nvidia-container-runtime</code> as shown below:</p> <pre><code>[plugins.\"io.containerd.runtime.v1.linux\"]\n    shim = \"containerd-shim\"\n    runtime = \"nvidia-container-runtime\"\n</code></pre> <p>Note To run <code>nvidia-container-runtime</code> on your node please look here for detailed instructions.</p> <p>After changes to the configuration, restart <code>k0s</code> and in this case containerd will be using newly configured runtime.</p>"},{"location":"custom-cri-runtime/","title":"Custom CRI runtime","text":"<p>k0s supports users bringing their own CRI runtime (for example, docker). In which case, k0s will not start nor manage the runtime, and it is fully up to the user to configure it properly.</p> <p>To run a k0s worker with a custom CRI runtime use the option <code>--cri-socket</code>.  It takes input in the form of <code>&lt;type&gt;:&lt;socket&gt;</code> where:</p> <ul> <li><code>type</code>: Either <code>remote</code> or <code>docker</code>. Use <code>docker</code> for pure docker setup, <code>remote</code> for anything else.</li> <li><code>socket</code>: Path to the socket, examples: <code>unix:///var/run/docker.sock</code></li> </ul> <p>To run k0s with pre-existing docker setup run the worker with <code>k0s worker --cri-socket docker:unix:///var/run/docker.sock &lt;token&gt;</code>.</p> <p>When <code>docker</code> is used as a runtime, k0s will configure kubelet to create the dockershim socket at <code>/var/run/dockershim.sock</code>.</p>"},{"location":"dual-stack/","title":"Dual-stack networking","text":"<p>Note: Dual stack networking setup requires Calico or a custom CNI to be configured as the CNI provider.</p> <p>To enable dual-stack networking use the following k0s.yaml as an example. This settings will set up bundled calico cni, enable feature gates for the Kubernetes components and set up kubernetes-controller-manager. <pre><code>spec:\n  network:\n    podCIDR: \"10.244.0.0/16\"\n    serviceCIDR: \"10.96.0.0/12\"\n    calico:\n      mode: \"bird\"\n    dualStack:\n      enabled: true\n      IPv6podCIDR: \"fd00::/108\"\n      IPv6serviceCIDR: \"fd01::/108\"\n</code></pre></p>"},{"location":"dual-stack/#cni-settings","title":"CNI settings","text":""},{"location":"dual-stack/#calico-settings","title":"Calico settings","text":"<p>Calico doesn't support tunneling for the IPv6, so \"vxlan\" and \"ipip\" backend wouldn't work.  If you need to have cross-pod connectivity, you need to use \"bird\" as a backend mode.  In any other mode the pods would be able to reach only pods on the same node.</p>"},{"location":"dual-stack/#external-cni","title":"External CNI","text":"<p>The <code>k0s.yaml</code> dualStack section will enable all the neccessary feature gates for the Kubernetes components but in case of using external CNI it must be set up with IPv6 support.</p>"},{"location":"dual-stack/#additional-materials","title":"Additional materials","text":"<p>https://kubernetes.io/docs/concepts/services-networking/dual-stack/</p> <p>https://kubernetes.io/docs/tasks/network/validate-dual-stack/ </p> <p>https://www.projectcalico.org/dual-stack-operation-with-calico-on-kubernetes/</p> <p>https://docs.projectcalico.org/networking/ipv6</p>"},{"location":"experimental-windows/","title":"Running k0s worker nodes in Windows","text":""},{"location":"experimental-windows/#experimental-status","title":"Experimental status","text":"<p>Windows support feature is under active development and MUST BE considered as experemential.</p>"},{"location":"experimental-windows/#requirements","title":"Requirements","text":"<p>The cluster must have at least one worker node and control plane running on Linux. Windows can be used for running additional worker nodes.</p>"},{"location":"experimental-windows/#build","title":"Build","text":"<p><code>make clean k0s.exe</code></p> <p>This should create k0s.exe with staged kubelet.exe and kube-proxy.exe</p>"},{"location":"experimental-windows/#description","title":"Description","text":"<p>the k0s.exe supervises kubelet.exe and kube-proxy.exe During the first run calico install script created as <code>C:\\bootstrap.ps1</code></p> <p>The bootstrap script downloads the calico binaries, builds pause container and set ups vSwitch settings.</p>"},{"location":"experimental-windows/#running","title":"Running","text":"<p>It is expected to have docker EE installed on the windows node (we need it during the initial calico set up)</p> <pre><code>C:\\&gt;k0s.exe worker --cri-socket=docker:tcp://127.0.0.1:2375 --cidr-range=&lt;cidr_range&gt; --cluster-dns=&lt;clusterdns&gt; --api-server=&lt;k0s api&gt; &lt;token&gt;\n</code></pre> <p>Cluster control plane must be inited with proper config (see section below)</p>"},{"location":"experimental-windows/#configuration","title":"Configuration","text":""},{"location":"experimental-windows/#strict-affinity","title":"Strict-affinity","text":"<p>To run windows node we need to have strict affinity enabled.</p> <p>There is a configuration field <code>spec.network.calico.withWindowsNodes</code>, equals false by default. If set to the true, the additional calico related manifest <code>/var/lib/k0s/manifests/calico/calico-IPAMConfig-ipamconfig.yaml</code> would be created with the following values</p> <p><pre><code>---\napiVersion: crd.projectcalico.org/v1\nkind: IPAMConfig\nmetadata:\n  name: default\nspec:\n  strictAffinity: true\n</code></pre> Another way is to use calicoctl manually: <pre><code>calicoctl ipam configure --strictaffinity=true\n</code></pre></p>"},{"location":"experimental-windows/#network-connectivity-in-aws","title":"Network connectivity in AWS","text":"<p>The network interface attached to your EC2 instance MUST have disabled \u201cChange Source/Dest. Check\u201d option. In AWS console option can be found on the Actions menu for a selected network interface.</p>"},{"location":"experimental-windows/#hacks","title":"Hacks","text":"<p>We need to figure out proper way to pass cluster settings from controller plane to worker.</p> <p>While we don't have it, there are CLI arguments: - cidr-range - cluster-dns - api-server </p>"},{"location":"experimental-windows/#some-useful-commands","title":"Some useful commands","text":"<p>Run pod with cmd.exe shell <pre><code>kubectl run win --image=hello-world:nanoserver --command=true -i --attach=true -- cmd.exe\n</code></pre></p> <p>Manifest for pod with IIS web-server <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: iis\nspec:\n  containers:\n  - name: iis\n    image: mcr.microsoft.com/windows/servercore/iis\n    imagePullPolicy: IfNotPresent\n</code></pre></p>"},{"location":"extensions/","title":"Cluster extensions","text":"<p>k0s allows users to use extensions to extend cluster functionality.</p> <p>At the moment the only supported type of extensions is helm based charts.</p> <p>The default configuration has no extensions.</p>"},{"location":"extensions/#helm-based-extensions","title":"Helm based extensions","text":""},{"location":"extensions/#configuration","title":"Configuration","text":"<p>Example.   <code>helm:   repositories:   - name: stable     url: https://charts.helm.sh/stable   - name: prometheus-community     url: https://prometheus-community.github.io/helm-charts   charts:   - name: prometheus-stack     chartname: prometheus-community/prometheus     version: \"11.16.8\"     values: |       storageSpec:         emptyDir:           medium: Memory     namespace: default <pre><code>By using the configuration above, the cluster would:\n\n- add stable and prometheus-community chart repositories\n- install the `prometheus-community/prometheus` chart of the specified version to the `default` namespace.\n\nThe chart installation is implemented by using CRD `helm.k0sproject.io/Chart`.\nFor every given helm extension the cluster creates a Chart CRD instance.\nThe cluster has a controller which monitors for the Chart CRDs, supporting the following operations:\n- install\n- upgrade\n- delete\n\nFor security reasons, the cluster operates only on Chart CRDs instantiated in the `kube-system` namespace, however, the target namespace could be any.\n\n#### CRD definition\n</code></pre> apiVersion: helm.k0sproject.io/v1beta1 kind: Chart metadata:   creationTimestamp: \"2020-11-10T14:17:53Z\"   generation: 2   labels:     k0s.k0sproject.io/stack: helm   name: k0s-addon-chart-test-addon   namespace: kube-system   resourceVersion: \"627\"   selfLink: /apis/helm.k0sproject.io/v1beta1/namespaces/kube-system/charts/k0s-addon-chart-test-addon   uid: ebe59ed4-1ff8-4d41-8e33-005b183651ed spec:   chartName: prometheus-community/prometheus   namespace: default   values: |     storageSpec:       emptyDir:         medium: Memory   version: 11.16.8 status:   appVersion: 2.21.0   namespace: default   releaseName: prometheus-1605017878   revision: 2   updated: 2020-11-10 14:18:08.235656 +0000 UTC m=+41.871656901   version: 11.16.8</code></p> <p>The <code>Chart.spec</code> defines the chart information.</p> <p>The <code>Chart.status</code> keeps the information about the last operation performed by the operator.</p>"},{"location":"helm-charts/","title":"Helm Charts","text":"<p>k0s supports deploying applications with Helm charts in two ways:</p> <ul> <li>Helm command can be used in runtime to install applications like described in the Helm Quickstart Guide.</li> <li>Helm charts can be inserted directly into the k0s configuration file (k0s.yaml). This way a separate install of <code>helm</code> tool is not needed and the charts are automatically deployed at the k0s bootstrap phase.</li> </ul>"},{"location":"helm-charts/#helm-charts-in-k0s-configuration","title":"Helm charts in k0s configuration","text":"<p>When Helm charts are added into k0s configuration file, you'll get a declarative way to configure the cluster. k0s controller manages the setup of the defined extension Helm charts as part of the cluster bootstrap process.</p>"},{"location":"helm-charts/#example","title":"Example","text":"<p>This example configures Prometheus from \"stable\" Helms chart repository. Just add the following to your k0s configuration file (k0s.yaml) and restart k0s. Prometheus should now start automatically with k0s.</p> <pre><code>spec:\n    extensions:\n      helm:\n        repositories:\n        - name: stable\n          url: https://charts.helm.sh/stable\n        - name: prometheus-community\n          url: https://prometheus-community.github.io/helm-charts\n        charts:\n        - name: prometheus-stack\n          chartname: prometheus-community/prometheus\n          version: \"11.16.8\"\nvalues: |\nstorageSpec:\n              emptyDir:\n                medium: Memory\n          namespace: default\n</code></pre> <p>Some example extensions that you could use with Helm charts:</p> <ul> <li>Ingress controllers: Nginx ingress, Traefix ingress (tutorial)</li> <li>Volume storage providers: OpenEBS, Rook, Longhorn</li> <li>Monitoring: Prometheus, Grafana</li> </ul>"},{"location":"high-availability/","title":"Control Plane High Availability","text":""},{"location":"high-availability/#control-plane-high-availability","title":"Control Plane High Availability","text":"<p>The following pre-requisites are required in order to configure an HA control plane:</p>"},{"location":"high-availability/#requirements","title":"Requirements","text":""},{"location":"high-availability/#load-balancer","title":"Load Balancer","text":"<p>A load balancer with a single external address should be configured as the IP gateway for the controllers. The load balancer should allow traffic to each controller on the following ports:</p> <ul> <li>6443</li> <li>8132</li> <li>8133</li> <li>9443</li> </ul>"},{"location":"high-availability/#cluster-configuration","title":"Cluster configuration","text":"<p>On each controller node, a k0s.yaml configuration file should be configured. The following options need to match on each node, otherwise the control plane components will end up in very unknown states:</p> <ul> <li><code>network</code></li> <li><code>storage</code>: Needless to say, one cannot create a clustered control plane with each node only storing data locally on SQLite.</li> <li><code>externalAddress</code></li> </ul> <p>Full configuration file refrence</p>"},{"location":"install/","title":"Getting Started","text":"<p>In this tutorial you'll create a full Kubernetes cluster with just one node including both the controller and the worker. This is well suited for environments where the high-availability and multiple nodes are not needed. This is the easiest install method to start experimenting k0s.</p>"},{"location":"install/#prerequisites","title":"Prerequisites","text":"<p>This tutorial has been written for Debian/Ubuntu, but it can be used for any Linux running one of the supported init systems: Systemd or OpenRC.</p> <p>Before proceeding, make sure to review the System Requirements.</p>"},{"location":"install/#installation-steps","title":"Installation steps","text":""},{"location":"install/#1-download-k0s","title":"1. Download k0s","text":"<p>The k0s download script downloads the latest stable k0s and makes it executable from /usr/bin/k0s. <pre><code>$ curl -sSLf https://get.k0s.sh | sudo sh\n</code></pre></p>"},{"location":"install/#2-install-k0s-as-a-service","title":"2. Install k0s as a service","text":"<p>The <code>k0s install</code> sub-command will install k0s as a system service on the local host running one of the supported init systems: Systemd or OpenRC. Install can be executed for workers, controllers or single node (controller+worker) instances.</p> <p>This command will install a single node k0s including the controller and worker functions with the default configuration:</p> <pre><code>$ sudo k0s install controller --single\n</code></pre> <p>The <code>k0s install controller</code> sub-command accepts the same flags and parameters as the <code>k0s controller</code>. See manual install for an example for entering a custom config file.</p>"},{"location":"install/#3-start-k0s-as-a-service","title":"3. Start k0s as a service","text":"<p>To start the k0s service, run <pre><code>$ sudo systemctl start k0scontroller\n</code></pre> It usually takes 1-2 minutes until the node is ready for deploying applications.</p> <p>If you want to enable the k0s service to be started always after the node restart, enable the service. This command is optional.  <pre><code>$ sudo systemctl enable k0scontroller\n</code></pre></p>"},{"location":"install/#4-check-service-logs-and-k0s-status","title":"4. Check service, logs and k0s status","text":"<p>You can check the service status and logs like this: <pre><code>$ sudo systemctl status k0scontroller\n     Loaded: loaded (/etc/systemd/system/k0scontroller.service; enabled; vendor preset: enabled)\nActive: active (running) since Fri 2021-02-26 08:37:23 UTC; 1min 25s ago\n       Docs: https://docs.k0sproject.io\n   Main PID: 1408647 (k0s)\nTasks: 96\nMemory: 1.2G\n     CGroup: /system.slice/k0scontroller.service\n     ....\n</code></pre></p> <p>To get general information about your k0s instance: <pre><code>$ sudo k0s status\nVersion: v0.11.0\nProcess ID: 436\nParent Process ID: 1\nRole: controller+worker\nInit System: linux-systemd\n</code></pre></p>"},{"location":"install/#5-access-your-cluster-using-kubectl","title":"5. Access your cluster using kubectl","text":"<p>The Kubernetes command-line tool 'kubectl' is included into k0s. You can use it for example to deploy your application or check your node status like this: <pre><code>$ sudo k0s kubectl get nodes\nNAME   STATUS   ROLES    AGE    VERSION\nk0s    Ready    &lt;none&gt;   4m6s   v1.20.6-k0s1\n</code></pre></p>"},{"location":"install/#6-clean-up","title":"6. Clean-up","text":"<p>If you want to remove the k0s installation you should first stop the service: <pre><code>$ sudo systemctl stop k0scontroller\n</code></pre></p> <p>Then you can execute <code>k0s reset</code>, which cleans up the installed system service, data directories, containers, mounts and network namespaces. There are still few bits (e.g. iptables) that cannot be easily cleaned up and thus a reboot after the reset is highly recommended. <pre><code>$ sudo k0s reset\n</code></pre></p>"},{"location":"install/#next-steps","title":"Next Steps","text":"<ul> <li>Installing with k0sctl for deploying and upgrading multi-node clusters with one command</li> <li>Manual Install for advanced users for manually deploying multi-node clusters</li> <li>Control plane configuration options for example for networking and datastore configuration</li> <li>Worker node configuration options for example for node labels and kubelet arguments</li> <li>Support for cloud providers for example for load balancer or storage configuration</li> <li>Installing the Traefik Ingress Controller, a tutorial for ingress deployment</li> <li>Airgap/Offline installation, a tutorial for airgap deployment</li> </ul>"},{"location":"k0s-in-docker/","title":"Running k0s in Docker","text":"<p>In this tutorial you'll create a k0s cluster on top of docker. By default, both controller and worker are run in the same container to provide an easy local testing \"cluster\". The tutorial also shows how to add additional worker nodes to the cluster.</p>"},{"location":"k0s-in-docker/#prerequisites","title":"Prerequisites","text":"<p>Docker environment on Mac, Windows or Linux. Get Docker.</p>"},{"location":"k0s-in-docker/#container-images","title":"Container images","text":"<p>The k0s containers are published both on Docker Hub and GitHub. The examples in this page show Docker Hub, because it's more simple to use. Using GitHub requires a separate authentication (not covered here). Alternative links:</p> <ul> <li>docker.io/k0sproject/k0s:latest</li> <li>docker.pkg.github.com/k0sproject/k0s/k0s:\"version\"</li> </ul>"},{"location":"k0s-in-docker/#installation-steps","title":"Installation steps","text":""},{"location":"k0s-in-docker/#1-start-k0s","title":"1. Start k0s","text":"<p>You can run your own k0s in Docker easily with: <pre><code>docker run -d --name k0s --hostname k0s --privileged -v /var/lib/k0s -p 6443:6443 docker.io/k0sproject/k0s:latest\n</code></pre></p>"},{"location":"k0s-in-docker/#2-create-additional-workers-optional","title":"2. Create additional workers (optional)","text":"<p>If you want to attach multiple workers nodes into the cluster you can then distribute your application containers to separate workers.</p> <p>First, we need a join token for the worker: <pre><code>token=$(docker exec -t -i k0s k0s token create --role=worker)\n</code></pre></p> <p>Then create and join a new worker by running the container with: <pre><code>docker run -d --name k0s-worker1 --hostname k0s-worker1 --privileged -v /var/lib/k0s docker.io/k0sproject/k0s:latest k0s worker $token\n</code></pre></p> <p>Repeat for as many workers you need, and have resources for. :)</p>"},{"location":"k0s-in-docker/#3-access-your-cluster","title":"3. Access your cluster","text":"<p>You can access your cluster with kubectl: <pre><code>docker exec k0s kubectl get nodes\n</code></pre></p> <p>Alternatively, grab the kubeconfig file with <code>docker exec k0s cat /var/lib/k0s/pki/admin.conf</code> and paste it e.g. into Lens.</p>"},{"location":"k0s-in-docker/#docker-compose-alternative","title":"Docker Compose (alternative)","text":"<p>You can also run k0s with Docker Compose: <pre><code>version: \"3.9\"\nservices:\nk0s:\ncontainer_name: k0s\nimage: docker.io/k0sproject/k0s:latest\ncommand: k0s controller --config=/etc/k0s/config.yaml --enable-worker\nhostname: k0s\nprivileged: true\nvolumes:\n- \"/var/lib/k0s\"\ntmpfs:\n- /run\n- /var/run\nports:\n- \"6443:6443\"\nnetwork_mode: \"bridge\"\nenvironment:\nK0S_CONFIG: |-\napiVersion: k0s.k0sproject.io/v1beta1\nkind: Cluster\nmetadata:\nname: k0s\n# Any additional configuration goes here ...\n</code></pre></p>"},{"location":"k0s-in-docker/#known-limitations","title":"Known limitations","text":""},{"location":"k0s-in-docker/#no-custom-docker-networks","title":"No custom Docker networks","text":"<p>Currently, we cannot run k0s nodes if the containers are configured to use custom networks e.g. with <code>--net my-net</code>. This is caused by the fact that Docker sets up a custom DNS service within the network and that messes up CoreDNS. We know that there are some workarounds possible, but they are bit hackish. And on the other hand, running k0s cluster(s) in bridge network should not cause issues.</p>"},{"location":"k0s-in-docker/#next-steps","title":"Next Steps","text":"<ul> <li>Installing with k0sctl for deploying and upgrading multi-node clusters with one command</li> <li>Control plane configuration options for example for networking and datastore configuration</li> <li>Worker node configuration options for example for node labels and kubelet arguments</li> <li>Support for cloud providers for example for load balancer or storage configuration</li> <li>Installing the Traefik Ingress Controller, a tutorial for ingress deployment</li> </ul>"},{"location":"k0s-multi-node/","title":"Manual Install (for advanced users)","text":"<p>In this tutorial you'll create a multi-node cluster, which is locally managed in each node. It requires several steps to install each node separately and connect the nodes together with the access tokens. This tutorial is targeted for advanced users who want to setup their k0s nodes manually.</p>"},{"location":"k0s-multi-node/#prerequisites","title":"Prerequisites","text":"<p>This tutorial has been written for Debian/Ubuntu, but it can be used for any Linux running one of the supported init systems: Systemd or OpenRC.</p> <p>Before proceeding, make sure to review the System Requirements.</p> <p>To speed-up the usage of <code>k0s</code> command, you may want to enable shell completion.</p>"},{"location":"k0s-multi-node/#installation-steps","title":"Installation steps","text":""},{"location":"k0s-multi-node/#1-download-k0s","title":"1. Download k0s","text":"<p>The k0s download script downloads the latest stable k0s and makes it executable from /usr/bin/k0s. <pre><code>$ curl -sSLf https://get.k0s.sh | sudo sh\n</code></pre> The download script accepts the following environment variables:</p> <ol> <li><code>K0S_VERSION=v0.11.0</code> - select the version of k0s to be installed</li> <li><code>DEBUG=true</code> - outputs commands and their arguments as they are executed.</li> </ol> <p>If you need to use environment variables and you use sudo, you may need <code>--preserve-env</code> like <pre><code>curl -sSLf https://get.k0s.sh | sudo --preserve-env=K0S_VERSION sh\n</code></pre></p>"},{"location":"k0s-multi-node/#2-bootstrap-a-controller-node","title":"2. Bootstrap a controller node","text":"<p>Create a configuration file:</p> <p><pre><code>$ k0s default-config &gt; k0s.yaml\n</code></pre> If you wish to modify some of the settings, please check out the configuration documentation.</p> <p><pre><code>$ k0s install controller -c k0s.yaml\n</code></pre> <pre><code>$ systemctl start k0scontroller\n</code></pre></p> <p>k0s process will act as a \"supervisor\" for all of the control plane components. In a few seconds you'll have the control plane up-and-running.</p>"},{"location":"k0s-multi-node/#3-create-a-join-token","title":"3. Create a join token","text":"<p>To be able to join workers into the cluster a token is needed. The token embeds information, which enables mutual trust between the worker and controller(s) and allows the node to join the cluster as worker.</p> <p>To get a token run the following command on one of the existing controller nodes: <pre><code>$ k0s token create --role=worker\n</code></pre></p> <p>This will output a long token string, which you will use to add a worker to the cluster. For enhanced security, it's possible to set an expiration time for the token by using: <pre><code>$ k0s token create --role=worker --expiry=100h &gt; token-file\n</code></pre></p>"},{"location":"k0s-multi-node/#4-add-workers-to-the-cluster","title":"4. Add workers to the cluster","text":"<p>To join the worker we need to run k0s in the worker mode with the token from the previous step: <pre><code>$ k0s install worker --token-file /path/to/token/file\n</code></pre> <pre><code>$ systemctl start k0sworker\n</code></pre></p>"},{"location":"k0s-multi-node/#about-tokens","title":"About tokens","text":"<p>The tokens are actually base64 encoded kubeconfigs. </p> <p>Why:</p> <ul> <li>Well defined structure</li> <li>Can be used directly as bootstrap auth configs for kubelet</li> <li>Embeds CA info for mutual trust</li> </ul> <p>The actual bearer token embedded in the kubeconfig is a bootstrap token. For controller join token and for worker join token we use different usage attributes so we can make sure we can validate the token role on the controller side.</p>"},{"location":"k0s-multi-node/#5-add-controllers-to-the-cluster","title":"5. Add controllers to the cluster","text":"<p>To add new controller nodes to the cluster, you must be using either etcd or an external data store (MySQL or Postgres) via kine. Please pay an extra attention to the high availability configuration, and make sure this configuration is identical for all controller nodes.</p> <p>To create a join token for the new controller, run the following on an existing controller: <pre><code>$ k0s token create --role=controller --expiry=1h &gt; token-file\n</code></pre></p> <p>On the new controller, run: <pre><code>$ sudo k0s install controller --token-file /path/to/token/file\n</code></pre> <pre><code>$ systemctl start k0scontroller\n</code></pre></p>"},{"location":"k0s-multi-node/#6-check-service-and-k0s-status","title":"6. Check service and k0s status","text":"<p>You can check the service status and logs like this: <pre><code>$ sudo systemctl status k0scontroller\n     Loaded: loaded (/etc/systemd/system/k0scontroller.service; enabled; vendor preset: enabled)\n     Active: active (running) since Fri 2021-02-26 08:37:23 UTC; 1min 25s ago\n       Docs: https://docs.k0sproject.io\n   Main PID: 1408647 (k0s)\n      Tasks: 96\n     Memory: 1.2G\n     CGroup: /system.slice/k0scontroller.service\n     ....\n</code></pre></p> <p>To get general information about your k0s instance: <pre><code>$ sudo k0s status\nVersion: v0.11.0\nProcess ID: 436\nParent Process ID: 1\nRole: controller\nInit System: linux-systemd\n</code></pre></p>"},{"location":"k0s-multi-node/#7-access-your-cluster","title":"7. Access your cluster","text":"<p>The Kubernetes command-line tool 'kubectl' is included into k0s binary. You can use it for example to deploy your application or check your node status like this: <pre><code>$ sudo k0s kubectl get nodes\nNAME   STATUS   ROLES    AGE    VERSION\nk0s    Ready    &lt;none&gt;   4m6s   v1.20.6-k0s1\n</code></pre></p> <p>You can also access your cluster easily with LENS. Just copy the kubeconfig  <pre><code>sudo cat /var/lib/k0s/pki/admin.conf\n</code></pre> and paste it to LENS. Note that in the kubeconfig you need add your controller's host ip address to the server field (replacing localhost) in order to access the cluster from an external network.</p>"},{"location":"k0s-multi-node/#next-steps","title":"Next Steps","text":"<ul> <li>Installing with k0sctl for deploying and upgrading multi-node clusters with one command</li> <li>Control plane configuration options for example for networking and datastore configuration</li> <li>Worker node configuration options for example for node labels and kubelet arguments</li> <li>Support for cloud providers for example for load balancer or storage configuration</li> <li>Installing the Traefik Ingress Controller, a tutorial for ingress deployment</li> </ul>"},{"location":"k0s-reset/","title":"k0s reset","text":"<p>k0s Command-Line interface allows users to remove all k0s related files from the host.</p> <p>The reset command operates under the assumption that k0s is installed as a service on the host. To prevent accidental triggering, it will not run if the k0s service is running. So first, we need to stop the service:</p> <p>For controller nodes:</p> <pre><code>$ systemctl stop k0scontroller\n</code></pre> <p>If the service is not stopped k0s will return an error message:</p> <pre><code>$ k0s reset\nFATA k0s seems to be running! please stop k0s before reset. </code></pre> <p>If k0s has been stopped, the reset operation will proceed:</p> <pre><code>$ k0s reset\nINFO[2021-02-25 15:58:41] Uninstalling the k0s service                 \nINFO[2021-02-25 15:58:42] no config file given, using defaults         \nINFO[2021-02-25 15:58:42] deleting user: etcd                          \nINFO[2021-02-25 15:58:42] deleting user: kube-apiserver                \nINFO[2021-02-25 15:58:42] deleting user: konnectivity-server           \nINFO[2021-02-25 15:58:42] deleting user: kube-scheduler                \nINFO[2021-02-25 15:58:42] starting containerd for cleanup operations... \nINFO[2021-02-25 15:58:42] containerd succesfully started               \nINFO[2021-02-25 15:58:42] attempting to clean up kubelet volumes...    \nINFO[2021-02-25 15:58:42] successfully removed kubelet mounts!         \nINFO[2021-02-25 15:58:42] attempting to clean up network namespaces... \nINFO[2021-02-25 15:58:42] successfully removed network namespaces!     \nINFO[2021-02-25 15:58:42] attempting to stop containers...             \nINFO[2021-02-25 15:58:49] successfully removed k0s containers!         \nINFO[2021-02-25 15:58:49] deleting k0s generated data-dir (/var/lib/k0s) and run-dir (/run/k0s) \nERRO[2021-02-25 15:58:50] k0s cleanup operations done. To ensure a full reset, a node reboot is recommended. </code></pre>"},{"location":"k0s-single-node/","title":"Creating a single-node cluster","text":"<p>These instructions outline a quick method for running a local k0s master and worker in a single node.</p> <p>NOTE:  This method of running k0s is only recommended for dev, test or POC environments.</p>"},{"location":"k0s-single-node/#prerequisites","title":"Prerequisites","text":"<p>Install k0s as documented in the installation instructions.</p>"},{"location":"k0s-single-node/#start-k0s","title":"Start k0s","text":"<pre><code>$ sudo k0s install controller --single\nINFO[2021-02-25 15:34:59] Installing k0s service\n$ sudo systemctl start k0scontroller.service\n</code></pre>"},{"location":"k0s-single-node/#use-k0s-to-access-the-cluster","title":"Use k0s to access the cluster","text":"<pre><code>$ k0s kubectl get nodes\n</code></pre>"},{"location":"k0sctl-install/","title":"Installing with k0sctl","text":"<p>This tutorial is based on k0sctl tool and it's targeted for creating a multi-node cluster for remote hosts. It describes an install method, which is automatic and easily repeatable. This is recommended for production clusters and the automatic upgrade requires using this install method. The automatic upgrade process is also described in this tutorial.</p> <p>k0sctl is a command-line tool for bootstrapping and managing k0s clusters. k0sctl connects to the provided hosts using SSH and gathers information about the hosts. Based on the findings it proceeds to configure the hosts, deploys k0s and connects the k0s nodes together to form a cluster.</p> <p></p>"},{"location":"k0sctl-install/#prerequisites","title":"Prerequisites","text":"<p>k0sctl can be executed on Linux, MacOS and Windows. See more details from the k0sctl github repository. For hosts running k0s, see the System Requirements.</p>"},{"location":"k0sctl-install/#installation-steps","title":"Installation steps","text":""},{"location":"k0sctl-install/#1-install-k0sctl-tool","title":"1. Install k0sctl tool","text":"<p>k0sctl is a single binary and the installation and download instructions can be found in the k0sctl github repository.</p>"},{"location":"k0sctl-install/#2-configure-the-cluster","title":"2. Configure the cluster","text":"<p>First create a k0sctl configuration file: <pre><code>$ k0sctl init &gt; k0sctl.yaml\n</code></pre></p> <p>A <code>k0sctl.yaml</code> file will be created in the current directory:</p> <pre><code>apiVersion: k0sctl.k0sproject.io/v1beta1\nkind: Cluster\nmetadata:\nname: k0s-cluster\nspec:\nhosts:\n- role: controller\nssh:\naddress: 10.0.0.1 # replace with the controller's IP address\nuser: root\nkeyPath: ~/.ssh/id_rsa\n- role: worker\nssh:\naddress: 10.0.0.2 # replace with the worker's IP address\nuser: root\nkeyPath: ~/.ssh/id_rsa\n</code></pre> <p>As a mandatory step, each host must be given a valid IP address (which is reachable by k0sctl) and the connection details for an SSH connection. k0sctl configuration specifications can be found in k0sctl documentation.</p>"},{"location":"k0sctl-install/#3-deploy-the-cluster","title":"3. Deploy the cluster","text":"<p>Next step is to run <code>k0sctl apply</code> to perform the cluster deployment: <pre><code>$ k0sctl apply --config k0sctl.yaml \n\n\u2800\u28ff\u28ff\u2847\u2800\u2800\u2880\u28f4\u28fe\u28ff\u281f\u2801\u28b8\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u287f\u281b\u2801\u2800\u28b8\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u2800\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\n\u2800\u28ff\u28ff\u2847\u28e0\u28f6\u28ff\u287f\u280b\u2800\u2800\u2800\u28b8\u28ff\u2847\u2800\u2800\u2800\u28e0\u2800\u2800\u2880\u28e0\u2846\u28b8\u28ff\u28ff\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2588\u2588\u2588          \u2588\u2588\u2588    \u2588\u2588\u2588\n\u2800\u28ff\u28ff\u28ff\u28ff\u28df\u280b\u2800\u2800\u2800\u2800\u2800\u28b8\u28ff\u2847\u2800\u28b0\u28fe\u28ff\u2800\u2800\u28ff\u28ff\u2847\u28b8\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u2800\u2588\u2588\u2588          \u2588\u2588\u2588    \u2588\u2588\u2588\n\u2800\u28ff\u28ff\u284f\u283b\u28ff\u28f7\u28e4\u2840\u2800\u2800\u2800\u2838\u281b\u2801\u2800\u2838\u280b\u2801\u2800\u2800\u28ff\u28ff\u2847\u2808\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u28b9\u28ff\u28ff\u2800\u2588\u2588\u2588          \u2588\u2588\u2588    \u2588\u2588\u2588\n\u2800\u28ff\u28ff\u2847\u2800\u2800\u2819\u28bf\u28ff\u28e6\u28c0\u2800\u2800\u2800\u28e0\u28f6\u28f6\u28f6\u28f6\u28f6\u28f6\u28ff\u28ff\u2847\u28b0\u28f6\u28f6\u28f6\u28f6\u28f6\u28f6\u28f6\u28f6\u28fe\u28ff\u28ff\u2800\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\u2588    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n\nINFO k0sctl 0.0.0 Copyright 2021, Mirantis Inc.   \nINFO Anonymized telemetry will be sent to Mirantis. \nINFO By continuing to use k0sctl you agree to these terms: \nINFO https://k0sproject.io/licenses/eula          INFO ==&gt; Running phase: Connect to hosts \nINFO [ssh] 10.0.0.1:22: connected              \nINFO [ssh] 10.0.0.2:22: connected              INFO ==&gt; Running phase: Detect host operating systems \nINFO [ssh] 10.0.0.1:22: is running Ubuntu 20.10 \nINFO [ssh] 10.0.0.2:22: is running Ubuntu 20.10 INFO ==&gt; Running phase: Prepare hosts    \nINFO [ssh] 10.0.0.1:22: installing kubectl     INFO ==&gt; Running phase: Gather host facts \nINFO [ssh] 10.0.0.1:22: discovered 10.12.18.133 as private address INFO ==&gt; Running phase: Validate hosts   INFO ==&gt; Running phase: Gather k0s facts INFO ==&gt; Running phase: Download K0s on the hosts \nINFO [ssh] 10.0.0.2:22: downloading k0s 0.11.0 \nINFO [ssh] 10.0.0.1:22: downloading k0s 0.11.0 INFO ==&gt; Running phase: Configure K0s    \nWARN [ssh] 10.0.0.1:22: generating default configuration \nINFO [ssh] 10.0.0.1:22: validating configuration \nINFO [ssh] 10.0.0.1:22: configuration was changed INFO ==&gt; Running phase: Initialize K0s Cluster \nINFO [ssh] 10.0.0.1:22: installing k0s controller \nINFO [ssh] 10.0.0.1:22: waiting for the k0s service to start \nINFO [ssh] 10.0.0.1:22: waiting for kubernetes api to respond INFO ==&gt; Running phase: Install workers  \nINFO [ssh] 10.0.0.1:22: generating token       \nINFO [ssh] 10.0.0.2:22: writing join token     \nINFO [ssh] 10.0.0.2:22: installing k0s worker  \nINFO [ssh] 10.0.0.2:22: starting service       \nINFO [ssh] 10.0.0.2:22: waiting for node to become ready INFO ==&gt; Running phase: Disconnect from hosts INFO ==&gt; Finished in 2m2s                \nINFO k0s cluster version 0.11.0 is now installed  \nINFO Tip: To access the cluster you can now fetch the admin kubeconfig using: \nINFO      k0sctl kubeconfig              </code></pre></p> <p>And -- presto! Your k0s cluster is up and running.</p>"},{"location":"k0sctl-install/#4-access-the-cluster","title":"4. Access the cluster","text":"<p>To access your k0s cluster, you first need to get the kubeconfig. k0sctl does this for you like this: <pre><code>$ k0sctl kubeconfig &gt; kubeconfig\n</code></pre></p> <p>Then you can access your cluster for example by using kubectl or LENS. <pre><code>$ kubectl get pods --kubeconfig kubeconfig -A\nNAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE\nkube-system   calico-kube-controllers-5f6546844f-w8x27   1/1     Running   0          3m50s\nkube-system   calico-node-vd7lx                          1/1     Running   0          3m44s\nkube-system   coredns-5c98d7d4d8-tmrwv                   1/1     Running   0          4m10s\nkube-system   konnectivity-agent-d9xv2                   1/1     Running   0          3m31s\nkube-system   kube-proxy-xp9r9                           1/1     Running   0          4m4s\nkube-system   metrics-server-6fbcd86f7b-5frtn            1/1     Running   0          3m51s\n</code></pre></p>"},{"location":"k0sctl-install/#upgrade-a-k0s-cluster-using-k0sctl","title":"Upgrade a k0s cluster using k0sctl","text":"<p>There's no dedicated upgrade sub-command in k0sctl, the configuration file describes the desired state of the cluster and when passed to <code>k0sctl apply</code>, it will perform a discovery of the current state and do what ever is needed to bring the cluster to the desired state, by for example performing an upgrade.</p>"},{"location":"k0sctl-install/#k0sctl-cluster-upgrade-process","title":"K0sctl cluster upgrade process","text":"<p>The following steps will be performed during a k0sctl cluster upgrade:</p> <ol> <li>Upgrade each controller one-by-one; As long as there\u2019s multiple controllers configured there\u2019s no downtime</li> <li>Upgrade workers in batches; 10% of the worker nodes are upgraded at a time<ul> <li>Each worker is first drained allowing the workload to \u201cmove\u201d to other nodes before the actual upgrade of the worker node components</li> <li>The process continues after we see the upgraded nodes back in \u201cReady\u201d state</li> <li>Drain can be skipped with a --no-drain option</li> </ul> </li> </ol> <p>The desired cluster version can be configured In the k0sctl configuration by setting the value of <code>spec.k0s.version</code>: <pre><code>spec:\nk0s:\nversion: 0.11.0\n</code></pre></p> <p>When a version has not been specified, k0sctl will check online for the latest version and default to using that.</p> <pre><code>$ k0sctl apply\n...\n...\nINFO[0001] ==&gt; Running phase: Upgrade controllers \nINFO[0001] [ssh] 10.0.0.23:22: starting upgrade\nINFO[0001] [ssh] 10.0.0.23:22: Running with legacy service name, migrating... \nINFO[0011] [ssh] 10.0.0.23:22: waiting for the k0s service to start \nINFO[0016] ==&gt; Running phase: Upgrade workers  \nINFO[0016] Upgrading 1 workers in parallel              \nINFO[0016] [ssh] 10.0.0.17:22: upgrade starting  \nINFO[0027] [ssh] 10.0.0.17:22: waiting for node to become ready again \nINFO[0027] [ssh] 10.0.0.17:22: upgrade successful   \nINFO[0027] ==&gt; Running phase: Disconnect from hosts \nINFO[0027] ==&gt; Finished in 27s                 \nINFO[0027] k0s cluster version 0.11.0 is now installed \nINFO[0027] Tip: To access the cluster you can now fetch the admin kubeconfig using: \nINFO[0027]      k0sctl kubeconfig </code></pre>"},{"location":"k0sctl-install/#known-limitations","title":"Known limitations","text":"<ul> <li>k0sctl will not perform any discovery of hosts, it only operates on the hosts listed in the provided configuration</li> <li>k0sctl can currently only add more nodes to the cluster but cannot remove existing ones</li> </ul>"},{"location":"k0sctl-install/#next-steps","title":"Next Steps","text":"<ul> <li>Control plane configuration options for example for networking and datastore configuration</li> <li>Worker node configuration options for example for node labels and kubelet arguments</li> <li>Support for cloud providers for example for load balancer or storage configuration</li> <li>Installing the Traefik Ingress Controller, a tutorial for ingress deployment</li> </ul>"},{"location":"manifests/","title":"Manifest Deployer","text":"<p>To run k0s with your preferred extensions you have two options.</p> <ol> <li> <p>Define your extensions as Helm charts as described in here.</p> </li> <li> <p>Use Manifest Deployer, which is included into k0s. This option is described on below.</p> </li> </ol>"},{"location":"manifests/#overview","title":"Overview","text":"<p>k0s embeds Manifest Deployer, which runs on the controller nodes and provides an easy way to deploy manifests automatically at runtime. </p> <p>By default, k0s reads all manifests under <code>/var/lib/k0s/manifests</code> and ensures that their state matches the cluster state. Moreover, when a manifest file is removed, k0s will automatically prune all the resources associated with it. </p> <p>For the most part, Manifest Deployer can be used like the <code>kubectl apply</code> command. The main difference is that Manifest Deployer constantly monitors the directory for changes. Thus, users don't need to manually apply the changes that are made to the manifest files.</p>"},{"location":"manifests/#note","title":"Note","text":"<ul> <li>Each directory that is a direct descendant of <code>/var/lib/k0s/manifests</code> is considered as its own \"stack\", but nested directories (further subfolders) will be excluded from the stack mechanism and thus, they are not automatically deployed by the Manifest Deployer.</li> <li>k0s uses this mechanism for some of its internal in-cluster components and other resources. Make sure you only touch the manifests not managed by k0s.</li> <li>Namespace must be explicitly defined in the manifests. There's no default namespace used by Manifest Deployer.</li> </ul>"},{"location":"manifests/#example","title":"Example","text":"<p>You can try Manifest Deployer by creating a new folder under <code>/var/lib/k0s/manifests</code> and then create a manifest file like <code>nginx.yaml</code> with the following content:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: nginx\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  namespace: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 3\ntemplate:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - containerPort: 80\n</code></pre> <p>You should soon see the new pods appearing: <pre><code>$ sudo k0s kubectl get pods --namespace nginx\nNAME                                READY   STATUS    RESTARTS   AGE\nnginx-deployment-66b6c48dd5-8zq7d   1/1     Running   0          10m\nnginx-deployment-66b6c48dd5-br4jv   1/1     Running   0          10m\nnginx-deployment-66b6c48dd5-sqvhb   1/1     Running   0          10m\n</code></pre></p>"},{"location":"networking/","title":"Networking","text":""},{"location":"networking/#in-cluster-networking","title":"In-cluster networking","text":"<p>k0s supports two CNI providers out-of-box plus the ability to bring your own CNI config.</p> <p>k0s uses Kube-router as the default, built-in network provider. It allows setting up CNI networking without any overlays by utilizing BGP as the main mechanism for in-cluster networking:</p> <p>A key design tenet of Kube-router is to use standard Linux networking stack and toolset. There is no overlays or SDN pixie dust, but just plain good old networking.</p> <p>k0s also supports Calico as alternative, built-in network provider. Calico is a container networking solution making use of layer 3 to route packets to pods. It supports for example pod specific network policies helping to secure kubernetes clusters in demanding use cases. Calico uses vxlan overlay network by default. Also ipip (IP-in-IP) is supported by configuration.</p> <p>When deploying k0s with the default settings, all pods on a node can communicate with all pods on all nodes. No configuration changes are needed to get started.</p> <p>It is possible for a user to opt-out of k0s managing the network setup. Users are able to utilize any network plugin following the CNI specification. By configuring <code>custom</code> as the network provider (in k0s.yaml) it is expected that the user sets up the networking. This can be achieved e.g. by pushing network provider manifests into <code>/var/lib/k0s/manifests</code> from where k0s controllers will pick them up and deploy into the cluster. More on the automatic manifest handling here.</p>"},{"location":"networking/#notes-for-network-provider-selection","title":"Notes for network provider selection","text":"<p>The main difference between Kube-Router and Calico network providers is the layer they operate on. In short, kube-router operates on Layer 2 where Calico operates on Layer 3.</p> <p>Few points for each providers to take into consideration.</p> <p>Kube-router: - supports armv7 (among many other archs) - uses bit less resources (~15%) - does NOT support dual-stack (IPv4/IPv6) networking - does NOT support Windows nodes</p> <p>Calico: - does NOT support armv7 - uses bit more resources - does support dual-stack (IPv4/IPv6) networking - does support Windows nodes</p> <p>Note: After the cluster has been initialized with one network provider, it is not currently supported to change the network provider. Changing network provider requires full re-deployment of the cluster.</p>"},{"location":"networking/#controllers-worker-communication","title":"Controller(s) - Worker communication","text":"<p>As one of the goals of k0s is to allow deployment of totally isolated control plane we cannot rely on the fact that there is an IP route between controller nodes and the pod network. To enable this communication path, which is mandated by conformance tests, we use Konnectivity service to proxy the traffic from API server (control plane) into the worker nodes. Possible firewalls should be configured with outbound access so that Konnectivity agents running on the worker nodes can establish the connection. This ensures that we can always fulfill all the Kubernetes API functionalities, but still operate the control plane in total isolation from the workers.</p> <p></p>"},{"location":"networking/#needed-open-ports-protocols","title":"Needed open ports &amp; protocols","text":"Protocol Port Service Direction Notes TCP 2380 etcd peers controller &lt;-&gt; controller TCP 6443 kube-apiserver Worker, CLI =&gt; controller authenticated kube API using kube TLS client certs, ServiceAccount tokens with RBAC TCP 179 kube-router worker &lt;-&gt; worker BGP routing sessions between peers UDP 4789 Calico worker &lt;-&gt; worker Calico VXLAN overlay TCP 10250 kubelet Master, Worker =&gt; Host <code>*</code> authenticated kubelet API for the master node <code>kube-apiserver</code> (and <code>heapster</code>/<code>metrics-server</code> addons) using TLS client certs TCP 9443 k0s-api controller &lt;-&gt; controller k0s controller join API, TLS with token auth TCP 8132,8133 konnectivity server worker &lt;-&gt; controller konnectivity is used as \"reverse\" tunnel between kube-apiserver and worker kubelets"},{"location":"raspberry-pi4/","title":"Creating Raspberry Pi 4 Cluster","text":"<p>This is a highly opinionated example of deploying the k0s distribution of Kubernetes to a cluster comprised of Raspberry Pi 4 Computers with Ubuntu 20.04 LTS as the operating system.</p>"},{"location":"raspberry-pi4/#prerequisites","title":"Prerequisites","text":"<p>The following tools should be installed on your local workstation to use this example:</p> <ul> <li>Kubectl <code>v1.19.4</code>+</li> <li>Raspberry Pi Imager <code>v1.5</code>+</li> </ul>"},{"location":"raspberry-pi4/#walkthrough","title":"Walkthrough","text":"<p>In order to deploy k0s on your Raspberry Pi systems we'll follow these steps:</p> <ol> <li>Hardware &amp; Operating System Setup</li> <li>Networking Configurations</li> <li>Node Configurations</li> <li>Deploying Kubernetes</li> </ol> <p>These steps require a fair amount of pre-requisite knowledge of Linux and assume a basic understanding of the Ubuntu Linux Distribution as well as Kubernetes.</p> <p>If you're feeling out of sorts, consider reading through the Kubernetes Basics Documentation for more context and some less complex exercises to get started with.</p>"},{"location":"raspberry-pi4/#hardware-operating-system","title":"Hardware &amp; Operating System","text":"<p>Note that this example was developed with Raspberry Pi 4 Model B Computers with 8GB of RAM and 64GB SD Cards. You may have to do some manual editing of the example code and k0s configuration to get things working in your environment if you use lower spec machines.</p>"},{"location":"raspberry-pi4/#downloading-ubuntu","title":"Downloading Ubuntu","text":"<p>When this example was developed the following image was used:</p> <ul> <li>Ubuntu Server 20.04.1 LTS RASPI 4 Image</li> </ul> <p>It's likely later versions and other versions will work but when in doubt consider using the above image for the nodes.</p>"},{"location":"raspberry-pi4/#installing-ubuntu","title":"Installing Ubuntu","text":"<p>The Raspberry Pi Foundation provides a convenient imaging tool for writing the Ubuntu image to your nodes' SD cards by selecting the image and the SD card you want to write it to.</p> <p>If all goes well it should be as simple as flashing the SD, inserting it into the Raspberry Pi system, and turning it on and Ubuntu will bootstrap itself. If you run into any trouble however, Ubuntu provides documentation on installing Ubuntu on Raspberry Pi Computers.</p> <p>Note that the default login credentials for the systems once cloud-init finishes bootstrapping the system will be user <code>ubuntu</code> with password <code>ubuntu</code> and you'll be required to change the password on first login.</p>"},{"location":"raspberry-pi4/#networking","title":"Networking","text":"<p>For this example it's assumed that you have all computers connected to eachother on the same subnet, but the rest is pretty much open ended.</p> <p>Make sure that you review the k0s required ports documentation to ensure that your network and firewall configurations will allow necessary traffic for the cluster.</p> <p>Review the Ubuntu Server Networking Configuration Documentation and ensure that all systems get a static IP address on the network, or that the network is providing a static DHCP lease for the nodes.</p>"},{"location":"raspberry-pi4/#openssh","title":"OpenSSH","text":"<p>Ubuntu Server will deploy and enable OpenSSH by default, but make sure that for whichever user you're going to deploy the cluster with on the build system their SSH Key is copied to each node's root user, or you may have to do additional manual configurations to run the example.</p> <p>Effectively before you start, you should have it configured so that the current user can run:</p> <pre><code>$ ssh root@${HOST}\n</code></pre> <p>Where <code>${HOST}</code> is any node and the login will succeed with no further prompts.</p>"},{"location":"raspberry-pi4/#setup-nodes","title":"Setup Nodes","text":"<p>Each node (whether control plane or not) will need some additional setup to prepare for k0s deployment.</p>"},{"location":"raspberry-pi4/#cgroup-configuration","title":"CGroup Configuration","text":"<p>Ensure that the following packages are installed on each node:</p> <pre><code>$ apt-get install cgroup-lite cgroup-tools cgroupfs-mount\n</code></pre> <p>Additionally not all Ubuntu images are going to have the <code>memory</code> cgroup enabled in the Kernel by default, one simple way to enable this is by adding it to the Kernel command line.</p> <p>Open the file <code>/boot/firmware/cmdline.txt</code> which is responsible for managing the Kernel parameters, and ensure that the following parameters exist (add them if not):</p> <pre><code>cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1\n</code></pre> <p>Make sure you <code>reboot</code> each node to ensure the <code>memory</code> cgroup is loaded.</p>"},{"location":"raspberry-pi4/#swap-optional","title":"Swap (Optional)","text":"<p>While this is technical optional if you have limited RAM (like &lt; 2GB) it can be helpful to enable swap to ease some memory pressure.</p> <p>You can create a swapfile by running the following:</p> <pre><code>fallocate -l 2G /swapfile\nchmod 0600 /swapfile\nmkswap /swapfile\nswapon -a\n</code></pre> <p>To ensure that the usage of swap is not too agressive, make sure you set the <code>sudo sysctl vm.swappiness=10</code> (the default is generally higher). Configure this in <code>/etc/sysctl.d/*</code> to be persistent.</p> <p>Lastly to ensure that your swap is mounted after reboots, make sure the following line exists in your <code>/etc/fstab</code> configuration:</p> <pre><code>/swapfile         none           swap sw       0 0\n</code></pre>"},{"location":"raspberry-pi4/#kernel-modules","title":"Kernel Modules","text":"<p>Some important Kernel modules to keep track of are the <code>overlay</code>, <code>nf_conntrack</code> and <code>br_netfilter</code> modules, ensure those are loaded:</p> <pre><code>$ modprobe overlay\n$ modprobe nf_conntrack\n$ modprobe br_netfilter\n</code></pre> <p>Add each of these modules to your <code>/etc/modules-load.d/modules.conf</code> file as well to ensure they persist after reboot.</p>"},{"location":"raspberry-pi4/#download-k0s","title":"Download k0s","text":"<p>Download a k0s release, for example:</p> <p><pre><code>$ wget -O /tmp/k0s https://github.com/k0sproject/k0s/releases/download/v0.9.1/k0s-v0.9.1-arm64\n$ chmod a+x /tmp/k0s\n$ sudo mv /tmp/k0s /usr/bin/k0s\n</code></pre> or use the k0s download script (as one command) to download the latest stable k0s and make it executable in /usr/bin/k0s. <pre><code>$ curl -sSLf https://get.k0s.sh | sudo sh\n</code></pre></p> <p>Now you'll be able to run <code>k0s</code>: <pre><code>$ k0s version\nv0.9.1\n</code></pre></p>"},{"location":"raspberry-pi4/#deploying-kubernetes","title":"Deploying Kubernetes","text":"<p>Each node is now setup to handle being a control plane node or worker node.</p>"},{"location":"raspberry-pi4/#control-plane-node","title":"Control Plane Node","text":"<p>For this demonstration, we'll use a non-ha control plane with a single node.</p>"},{"location":"raspberry-pi4/#systemd-service","title":"Systemd Service","text":"<p>Create a systemd service:</p> <pre><code>$ sudo k0s install controller\n</code></pre> <p>Enable and start the service:</p> <pre><code>$ systemctl enable --now k0scontroller\n</code></pre> <p>Run <code>systemctl status k0scontroller</code> to verify the service status.</p>"},{"location":"raspberry-pi4/#worker-tokens","title":"Worker Tokens","text":"<p>For each worker node that you expect to have, create a join token (and save this for later steps):</p> <pre><code>$ k0s token create --role worker\n</code></pre>"},{"location":"raspberry-pi4/#worker","title":"Worker","text":"<p>For any number of worker nodes which you created join tokens for we'll need to deploy a worker service and start it.</p>"},{"location":"raspberry-pi4/#systemd-service_1","title":"Systemd Service","text":"<p>Create the join token for the worker:</p> <pre><code>$ mkdir -p /var/lib/k0s/\n$ echo ${TOKEN_CONTENT} &gt; /var/lib/k0s/join-token\n</code></pre> <p>Where <code>${TOKEN_CONTENT}</code> is one of the join tokens you created in the control plane setup.</p> <p>Then deploy the systemd service for the worker:</p> <pre><code>$ sudo k0s install worker --token-file /var/lib/k0s/join-token\n</code></pre> <p>Enable and start the service:</p> <pre><code>$ systemctl enable --now k0sworker\n</code></pre> <p>Run <code>systemctl status k0sworker</code> to verify the service status.</p>"},{"location":"raspberry-pi4/#connecting-to-your-cluster","title":"Connecting To Your Cluster","text":"<p>Now generate a <code>kubeconfig</code> for the cluster and start managing it with <code>kubectl</code>:</p> <pre><code>ssh root@${CONTROL_PLANE_NODE} k0s kubeconfig create --groups \"system:masters\" k0s &gt; config.yaml\nexport KUBECONFIG=$(pwd)/config.yaml\nkubectl create clusterrolebinding k0s-admin-binding --clusterrole=admin --user=k0s\n</code></pre> <p>Where <code>${CONTROL_PLANE_NODE}</code> is the address of your control plane node.</p> <p>Now the cluster can be accessed and used:</p> <pre><code>$ kubectl get nodes,deployments,pods -A\nNAME         STATUS   ROLES    AGE     VERSION\nnode/k8s-4   Ready    &lt;none&gt;   5m9s    v1.20.1-k0s1\nnode/k8s-5   Ready    &lt;none&gt;   5m      v1.20.1-k0s1\nnode/k8s-6   Ready    &lt;none&gt;   4m45s   v1.20.1-k0s1\n\nNAMESPACE     NAME                                      READY   UP-TO-DATE   AVAILABLE   AGE\nkube-system   deployment.apps/calico-kube-controllers   1/1     1            1           12m\nkube-system   deployment.apps/coredns                   1/1     1            1           12m\n\nNAMESPACE     NAME                                           READY   STATUS        RESTARTS   AGE\nkube-system   pod/calico-kube-controllers-5f6546844f-rjdkz   1/1     Running       0          12m\nkube-system   pod/calico-node-j475n                          1/1     Running       0          5m9s\nkube-system   pod/calico-node-lnfrf                          1/1     Running       0          4m45s\nkube-system   pod/calico-node-pzp7x                          1/1     Running       0          5m\nkube-system   pod/coredns-5c98d7d4d8-bg9pl                   1/1     Running       0          12m\nkube-system   pod/konnectivity-agent-548hp                   1/1     Running       0          4m45s\nkube-system   pod/konnectivity-agent-66cr8                   1/1     Running       0          4m49s\nkube-system   pod/konnectivity-agent-lxt9z                   1/1     Running       0          4m58s\nkube-system   pod/kube-proxy-ct6bg                           1/1     Running       0          5m\nkube-system   pod/kube-proxy-hg8t2                           1/1     Running       0          4m45s\nkube-system   pod/kube-proxy-vghs9                           1/1     Running       0          5m9s\n</code></pre> <p>Enjoy!</p>"},{"location":"shell-completion/","title":"Enabling Shell Completion","text":"<p>The k0s completion script for Bash, zsh, fish and powershell can be generated with the command <code>k0s completion &lt; shell &gt;</code>. </p> <p>Sourcing the completion script in your shell enables k0s autocompletion.</p>"},{"location":"shell-completion/#bash","title":"Bash","text":"<pre><code>echo 'source &lt;(k0s completion bash)' &gt;&gt;~/.bashrc\n</code></pre> <pre><code># To load completions for each session, execute once:\n$ k0s completion bash &gt; /etc/bash_completion.d/k0s\n</code></pre>"},{"location":"shell-completion/#zsh","title":"Zsh","text":"<p>If shell completion is not already enabled in your environment you will need to enable it. You can execute the following once: <pre><code>$ echo \"autoload -U compinit; compinit\" &gt;&gt; ~/.zshrc\n</code></pre> <pre><code># To load completions for each session, execute once:\n$ k0s completion zsh &gt; \"${fpath[1]}/_k0s\"\n</code></pre> You will need to start a new shell for this setup to take effect.</p>"},{"location":"shell-completion/#fish","title":"Fish","text":"<p><pre><code>$ k0s completion fish | source\n</code></pre> <pre><code># To load completions for each session, execute once:\n$ k0s completion fish &gt; ~/.config/fish/completions/k0s.fish\n</code></pre></p>"},{"location":"system-requirements/","title":"System requirements","text":"<p>These are the k0s system requirements.</p>"},{"location":"system-requirements/#hardware","title":"Hardware","text":"<p>The following table shows the minimum HW requirements. All values are approximations and results may vary.</p> Role Virtual CPU (vCPU) Memory (RAM) Controller node 1 vCPU (2 recommended) 1 GB (2 recommended) Worker node 1 vCPU (2 recommended) 1 GB (2 recommended) Controller + worker 1 vCPU (2 recommended) 1 GB (2 recommended) <p>For optimal storage performance we recommend using an SSD disk. Cluster latency and throughput are sensitive to storage: https://etcd.io/docs/current/op-guide/performance/</p> <p>k0s part of the storage consumption is presented in the following table. Note that the operating system and application requirements must be added on top.</p> Role Storage (k0s part) Controller node ~0.5 GB Worker node ~1.3 GB Controller + worker ~1.7 GB"},{"location":"system-requirements/#host-operating-system","title":"Host operating system","text":"<ul> <li>Linux (kernel v3.10 or newer)</li> <li>Windows Server 2019</li> </ul>"},{"location":"system-requirements/#architecture","title":"Architecture","text":"<ul> <li>Intel (x86-64)</li> <li>ARM (ARM64)</li> </ul>"},{"location":"system-requirements/#networking","title":"Networking","text":"<p>See networking for the needed open ports.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>There are few common cases we've seen where k0s fails to run properly. </p>"},{"location":"troubleshooting/#coredns-in-crashloop","title":"CoreDNS in crashloop","text":"<p>The most common case we've encountered so far has been CoreDNS getting into crashloop on the node(s).</p> <p>With kubectl you see something like this: <pre><code>$ kubectl get pod --all-namespaces\nNAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE\nkube-system   calico-kube-controllers-5f6546844f-25px6   1/1     Running   0          167m\nkube-system   calico-node-fwjx5                          1/1     Running   0          164m\nkube-system   calico-node-t4tx5                          1/1     Running   0          164m\nkube-system   calico-node-whwsg                          1/1     Running   0          164m\nkube-system   coredns-5c98d7d4d8-tfs4q                   1/1     Error     17         167m\nkube-system   konnectivity-agent-9jkfd                   1/1     Running   0          164m\nkube-system   konnectivity-agent-bvhdb                   1/1     Running   0          164m\nkube-system   konnectivity-agent-r6mzj                   1/1     Running   0          164m\nkube-system   kube-proxy-kr2r9                           1/1     Running   0          164m\nkube-system   kube-proxy-tbljr                           1/1     Running   0          164m\nkube-system   kube-proxy-xbw7p                           1/1     Running   0          164m\nkube-system   metrics-server-7d4bcb75dd-pqkrs            1/1     Running   0          167m\n</code></pre></p> <p>When you check the logs, it'll show something like this: <pre><code>$ kubectl -n kube-system logs coredns-5c98d7d4d8-tfs4q\nplugin/loop: Loop (127.0.0.1:55953 -&gt; :1053) detected for zone \".\", see https://coredns.io/plugins/loop#troubleshooting. Query: \"HINFO 4547991504243258144.3688648895315093531.\"\n</code></pre></p> <p>This is most often caused by systemd-resolved stub (or something similar) running locally and CoreDNS detects a possible loop with DNS queries.</p> <p>The easiest but most crude way to workaround is to disable the systemd-resolved stub and revert the hosts <code>/etc/resolv.conf</code> to original</p> <p>Read more at CoreDNS troubleshooting docs.</p>"},{"location":"troubleshooting/#k0s-controller-fails-on-arm-boxes","title":"<code>k0s controller</code> fails on ARM boxes","text":"<p>In the logs you probably see ETCD not starting up properly.</p> <p>Etcd is not fully supported on ARM architecture, thus you need to run <code>k0s controller</code> and thus also etcd process with env <code>ETCD_UNSUPPORTED_ARCH=arm64</code>.</p> <p>As Etcd is not fully supported on ARM architecture it also means that k0s controlplane with etcd itself is not fully supported on ARM either.</p>"},{"location":"troubleshooting/#pods-pending-when-using-cloud-providers","title":"Pods pending when using cloud providers","text":"<p>Once we enable cloud provider support on kubelet on worker nodes, kubelet will automatically add a taint <code>node.cloudprovider.kubernetes.io/uninitialized</code> for the node. This tain will prevent normal workloads to be scheduled on the node until the cloud provider controller actually runs second initialization on the node and removes the taint. This means that these nodes are not available for scheduling until the cloud provider controller is actually successfully running on the cluster.</p> <p>For troubleshooting your specific cloud provider see its documentation.</p>"},{"location":"troubleshooting/#k0s-not-working-with-read-only-usr","title":"k0s not working with read only <code>/usr</code>","text":"<p>By default k0s does not run on nodes where <code>/usr</code> is read only.</p> <p>This can be fixed by changing the default path for <code>volumePluginDir</code> in your k0s config. You will need to change to values, one for the kubelet itself, and one for Calico.</p> <p>Here is a snippet of an example config with the default values changed:</p> <pre><code>spec:\ncontrollerManager:\nextraArgs:\nflex-volume-plugin-dir: \"/etc/kubernetes/kubelet-plugins/volume/exec\"\nnetwork:\ncalico:\nflexVolumeDriverPath: /etc/k0s/kubelet-plugins/volume/exec/nodeagent~uds\nworkerProfiles:\n- name: coreos\nvalues:\nvolumePluginDir: /etc/k0s/kubelet-plugins/volume/exec/\n</code></pre> <p>With this config you can start your controller as usual. Any workers will need to be started with</p> <p><code>k0s worker --profile coreos [TOKEN]</code></p>"},{"location":"troubleshooting/#profiling","title":"Profiling","text":"<p>We drop any debug related information and symbols from the compiled binary by utilzing <code>-w -s</code> linker flags.</p> <p>To keep those symbols use <code>DEBUG</code> env variable:</p> <pre><code>$ DEBUG=true make k0s\nCGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -ldflags=\" -X github.com/k0sproject/k0s/pkg/build.Version=v0.9.0-7-g97e5bac -X \\\"github.com/k0sproject/k0s/pkg/build.EulaNotice=\\\" -X github.com/k0sproject/k0s/pkg/telemetry.segmentToken=\" \\\n            -o k0s.code main.go\n</code></pre> <p>Any value not equal to the \"false\" would work.</p> <p>To add custom linker flags use <code>LDFLAGS</code> variable.</p> <pre><code>$ LD_FLAGS=\"--custom-flag=value\" make k0s\nCGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -ldflags=\"--custom-flag=value -X github.com/k0sproject/k0s/pkg/build.Version=v0.9.0-7-g97e5bac -X \\\"github.com/k0sproject/k0s/pkg/build.EulaNotice=\\\" -X github.com/k0sproject/k0s/pkg/telemetry.segmentToken=\" \\\n        -o k0s.code main.go\n</code></pre>"},{"location":"user-management/","title":"User Management","text":""},{"location":"user-management/#adding-a-cluster-user","title":"Adding a Cluster User","text":"<p>To add a user to the cluster, use the kubeconfig create command. This will output a kubeconfig for the user, which can be used for authentication.</p> <p>On the controller, run the following to generate a kubeconfig for a user:</p> <pre><code>$ k0s kubeconfig create [username]\n</code></pre>"},{"location":"user-management/#enabling-access-to-cluster-resources","title":"Enabling Access to Cluster Resources","text":"<p>To allow the user access to the cluster, the user needs to be created with the <code>system:masters</code> group: <pre><code>$ k0s kubeconfig create --groups \"system:masters\" testUser &gt; k0s.config\n</code></pre></p> <p>Create a <code>roleBinding</code> to grant the user access to the resources: <pre><code>$ k0s kubectl create clusterrolebinding --kubeconfig k0s.config testUser-admin-binding --clusterrole=admin --user=testUser\n</code></pre></p>"},{"location":"worker-node-config/","title":"Configuration options for worker nodes","text":"<p>Currently <code>k0s worker</code> command does not take in any special yaml configuration. There still is ways how to configure the workers, the following chapters provide instructions for ways you can configure how the worker runs various components.</p>"},{"location":"worker-node-config/#node-labels","title":"Node labels","text":"<p><code>k0s worker</code> command accepts <code>--labels</code> flag with which you can make the newly joined worker node the register itself in the Kubernetes API with the given set of labels.</p> <p>So for example when running the worker with <code>k0s worker --token-file k0s.token --labels=\"k0sproject.io/foo=bar,k0sproject.io/other=xyz\"</code> will result in: <pre><code>/ # kubectl get node --show-labels\nNAME      STATUS     ROLES    AGE   VERSION        LABELS\nworker0   NotReady   &lt;none&gt;   10s   v1.20.2-k0s1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,k0sproject.io/foo=bar,k0sproject.io/other=xyz,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker0,kubernetes.io/os=linux\n</code></pre></p> <p>Note: Setting the labels is only effective on the first registration of the node and changing them afterwards has no effect.</p>"},{"location":"worker-node-config/#kubelet-args","title":"Kubelet args","text":"<p><code>k0s worker</code> command accepts a generic flag to pass in any set of argument for kubelet process.</p> <p>For example running <code>k0s worker --token-file=k0s.token --kubelet-extra-args=\"--node-ip=1.2.3.4 --address=0.0.0.0\"</code> will \"pass on\" the given flags to kubelet as-is. As the flags are passed as-is make sure you are passing in properly formatted and valued flags as k0s will NOT validate those at all.</p>"},{"location":"cli/","title":"Index","text":""},{"location":"cli/#k0s","title":"k0s","text":"<p>k0s - Zero Friction Kubernetes</p>"},{"location":"cli/#synopsis","title":"Synopsis","text":"<p>k0s - The zero friction Kubernetes - https://k0sproject.io</p>"},{"location":"cli/#options","title":"Options","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n      --debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n  -h, --help                     help for k0s\n  -l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s api  - Run the controller api</li> <li>k0s completion    - Generate completion script</li> <li>k0s controller    - Run controller</li> <li>k0s default-config    - Output the default k0s configuration yaml to stdout</li> <li>k0s docs    - Generate Markdown docs for the k0s binary</li> <li>k0s etcd    - Manage etcd cluster</li> <li>k0s install  - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo)</li> <li>k0s kubeconfig    - Create a kubeconfig file for a specified user</li> <li>k0s status    - Helper command for get general information about k0s</li> <li>k0s token  - Manage join tokens</li> <li>k0s validate    - Helper command for validating the config file</li> <li>k0s version  - Print the k0s version</li> <li>k0s worker    - Run worker</li> </ul>"},{"location":"cli/k0s/","title":"K0s","text":""},{"location":"cli/k0s/#k0s","title":"k0s","text":"<p>k0s - Zero Friction Kubernetes</p>"},{"location":"cli/k0s/#synopsis","title":"Synopsis","text":"<p>k0s - The zero friction Kubernetes - https://k0sproject.io</p>"},{"location":"cli/k0s/#options","title":"Options","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n      --debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n  -h, --help                     help for k0s\n  -l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s api  - Run the controller api</li> <li>k0s completion    - Generate completion script</li> <li>k0s controller    - Run controller</li> <li>k0s default-config    - Output the default k0s configuration yaml to stdout</li> <li>k0s docs    - Generate Markdown docs for the k0s binary</li> <li>k0s etcd    - Manage etcd cluster</li> <li>k0s install  - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo)</li> <li>k0s kubeconfig    - Create a kubeconfig file for a specified user</li> <li>k0s status    - Helper command for get general information about k0s</li> <li>k0s token  - Manage join tokens</li> <li>k0s validate    - Helper command for validating the config file</li> <li>k0s version  - Print the k0s version</li> <li>k0s worker    - Run worker</li> </ul>"},{"location":"cli/k0s_api/","title":"K0s api","text":""},{"location":"cli/k0s_api/#k0s-api","title":"k0s api","text":"<p>Run the controller api</p> <pre><code>k0s api [flags]\n</code></pre>"},{"location":"cli/k0s_api/#options","title":"Options","text":"<pre><code>  -h, --help   help for api\n</code></pre>"},{"location":"cli/k0s_api/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n      --debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n  -l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_api/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s  - k0s - Zero Friction Kubernetes</li> </ul>"},{"location":"cli/k0s_completion/","title":"K0s completion","text":""},{"location":"cli/k0s_completion/#k0s-completion","title":"k0s completion","text":"<p>Generate completion script</p>"},{"location":"cli/k0s_completion/#synopsis","title":"Synopsis","text":"<p>To load completions:</p> <p>Bash:</p> <p>$ source &lt;(k0s completion bash)</p>"},{"location":"cli/k0s_completion/#to-load-completions-for-each-session-execute-once","title":"To load completions for each session, execute once:","text":"<p>$ k0s completion bash &gt; /etc/bash_completion.d/k0s</p> <p>Zsh:</p>"},{"location":"cli/k0s_completion/#if-shell-completion-is-not-already-enabled-in-your-environment-you-will-need","title":"If shell completion is not already enabled in your environment you will need","text":""},{"location":"cli/k0s_completion/#to-enable-it-you-can-execute-the-following-once","title":"to enable it.  You can execute the following once:","text":"<p>$ echo \"autoload -U compinit; compinit\" &gt;&gt; ~/.zshrc</p>"},{"location":"cli/k0s_completion/#to-load-completions-for-each-session-execute-once_1","title":"To load completions for each session, execute once:","text":"<p>$ k0s completion zsh &gt; \"${fpath[1]}/_k0s\"</p>"},{"location":"cli/k0s_completion/#you-will-need-to-start-a-new-shell-for-this-setup-to-take-effect","title":"You will need to start a new shell for this setup to take effect.","text":"<p>Fish:</p> <p>$ k0s completion fish | source</p>"},{"location":"cli/k0s_completion/#to-load-completions-for-each-session-execute-once_2","title":"To load completions for each session, execute once:","text":"<p>$ k0s completion fish &gt; ~/.config/fish/completions/k0s.fish</p> <pre><code>k0s completion [bash|zsh|fish|powershell]\n</code></pre>"},{"location":"cli/k0s_completion/#options","title":"Options","text":"<pre><code>  -h, --help   help for completion\n</code></pre>"},{"location":"cli/k0s_completion/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n      --debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n  -l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_completion/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s  - k0s - Zero Friction Kubernetes</li> </ul>"},{"location":"cli/k0s_controller/","title":"K0s controller","text":""},{"location":"cli/k0s_controller/#k0s-controller","title":"k0s controller","text":"<p>Run controller</p> <pre><code>k0s controller [join-token] [flags]\n</code></pre>"},{"location":"cli/k0s_controller/#examples","title":"Examples","text":"<pre><code>    Command to associate master nodes:\n    CLI argument:\n    $ k0s controller [join-token]\n\n    or CLI flag:\n    $ k0s controller --token-file [path_to_file]\n    Note: Token can be passed either as a CLI argument or as a flag\n</code></pre>"},{"location":"cli/k0s_controller/#options","title":"Options","text":"<pre><code>      --cri-socket string   contrainer runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket]\n      --enable-worker       enable worker (default false)\n  -h, --help                help for controller\n      --profile string      worker profile to use on the node (default \"default\")\n      --token-file string   Path to the file containing join-token.\n</code></pre>"},{"location":"cli/k0s_controller/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n      --debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n  -l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_controller/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s  - k0s - Zero Friction Kubernetes</li> </ul>"},{"location":"cli/k0s_default-config/","title":"K0s default config","text":""},{"location":"cli/k0s_default-config/#k0s-default-config","title":"k0s default-config","text":"<p>Output the default k0s configuration yaml to stdout</p> <pre><code>k0s default-config [flags]\n</code></pre>"},{"location":"cli/k0s_default-config/#options","title":"Options","text":"<pre><code>  -h, --help   help for default-config\n</code></pre>"},{"location":"cli/k0s_default-config/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n      --debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n  -l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_default-config/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s  - k0s - Zero Friction Kubernetes</li> </ul>"},{"location":"cli/k0s_docs/","title":"K0s docs","text":""},{"location":"cli/k0s_docs/#k0s-docs","title":"k0s docs","text":"<p>Generate Markdown docs for the k0s binary</p> <pre><code>k0s docs [flags]\n</code></pre>"},{"location":"cli/k0s_docs/#options","title":"Options","text":"<pre><code>  -h, --help   help for docs\n</code></pre>"},{"location":"cli/k0s_docs/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n      --debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n  -l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_docs/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s  - k0s - Zero Friction Kubernetes</li> </ul>"},{"location":"cli/k0s_etcd/","title":"K0s etcd","text":""},{"location":"cli/k0s_etcd/#k0s-etcd","title":"k0s etcd","text":"<p>Manage etcd cluster</p>"},{"location":"cli/k0s_etcd/#options","title":"Options","text":"<pre><code>  -h, --help   help for etcd\n</code></pre>"},{"location":"cli/k0s_etcd/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n      --debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n  -l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_etcd/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s  - k0s - Zero Friction Kubernetes</li> <li>k0s etcd leave    - Sign off a given etc node from etcd cluster</li> <li>k0s etcd member-list    - Returns etcd cluster members list</li> </ul>"},{"location":"cli/k0s_etcd_leave/","title":"K0s etcd leave","text":""},{"location":"cli/k0s_etcd_leave/#k0s-etcd-leave","title":"k0s etcd leave","text":"<p>Sign off a given etc node from etcd cluster</p> <pre><code>k0s etcd leave [flags]\n</code></pre>"},{"location":"cli/k0s_etcd_leave/#options","title":"Options","text":"<pre><code>  -h, --help                  help for leave\n      --peer-address string   etcd peer address\n</code></pre>"},{"location":"cli/k0s_etcd_leave/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n      --debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n  -l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_etcd_leave/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s etcd    - Manage etcd cluster</li> </ul>"},{"location":"cli/k0s_etcd_member-list/","title":"K0s etcd member list","text":""},{"location":"cli/k0s_etcd_member-list/#k0s-etcd-member-list","title":"k0s etcd member-list","text":"<p>Returns etcd cluster members list</p> <pre><code>k0s etcd member-list [flags]\n</code></pre>"},{"location":"cli/k0s_etcd_member-list/#options","title":"Options","text":"<pre><code>  -h, --help   help for member-list\n</code></pre>"},{"location":"cli/k0s_etcd_member-list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n      --debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n  -l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_etcd_member-list/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s etcd    - Manage etcd cluster</li> </ul>"},{"location":"cli/k0s_install/","title":"K0s install","text":""},{"location":"cli/k0s_install/#k0s-install","title":"k0s install","text":"<p>Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo)</p>"},{"location":"cli/k0s_install/#options","title":"Options","text":"<pre><code>  -h, --help   help for install\n</code></pre>"},{"location":"cli/k0s_install/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n      --debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n  -l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_install/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s  - k0s - Zero Friction Kubernetes</li> <li>k0s install controller    - Helper command for setting up k0s as controller node on a brand-new system. Must be run as root (or with sudo)</li> <li>k0s install worker    - Helper command for setting up k0s as a worker node on a brand-new system. Must be run as root (or with sudo)</li> </ul>"},{"location":"cli/k0s_install_controller/","title":"K0s install controller","text":""},{"location":"cli/k0s_install_controller/#k0s-install-controller","title":"k0s install controller","text":"<p>Helper command for setting up k0s as controller node on a brand-new system. Must be run as root (or with sudo)</p> <pre><code>k0s install controller [flags]\n</code></pre>"},{"location":"cli/k0s_install_controller/#examples","title":"Examples","text":"<pre><code>All default values of controller command will be passed to the service stub unless overriden. \n\nWith controller subcommand you can setup a single node cluster by running:\n\n    k0s install controller --enable-worker\n</code></pre>"},{"location":"cli/k0s_install_controller/#options","title":"Options","text":"<pre><code>      --cri-socket string   contrainer runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket]\n      --enable-worker       enable worker (default false)\n  -h, --help                help for controller\n      --profile string      worker profile to use on the node (default \"default\")\n      --token-file string   Path to the file containing join-token.\n</code></pre>"},{"location":"cli/k0s_install_controller/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n      --debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n  -l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_install_controller/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s install  - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo)</li> </ul>"},{"location":"cli/k0s_install_worker/","title":"K0s install worker","text":""},{"location":"cli/k0s_install_worker/#k0s-install-worker","title":"k0s install worker","text":"<p>Helper command for setting up k0s as a worker node on a brand-new system. Must be run as root (or with sudo)</p> <pre><code>k0s install worker [flags]\n</code></pre>"},{"location":"cli/k0s_install_worker/#examples","title":"Examples","text":"<pre><code>Worker subcommand allows you to pass in all available worker parameters. \nAll default values of worker command will be passed to the service stub unless overriden.\n\nWindows flags like \"--api-server\", \"--cidr-range\" and \"--cluster-dns\" will be ignored since install command doesn't yet support Windows services\n</code></pre>"},{"location":"cli/k0s_install_worker/#options","title":"Options","text":"<pre><code>      --api-server string       HACK: api-server for the windows worker node\n      --cidr-range string       HACK: cidr range for the windows worker node (default \"10.96.0.0/12\")\n      --cluster-dns string      HACK: cluster dns for the windows worker node (default \"10.96.0.10\")\n      --cri-socket string       contrainer runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket]\n      --enable-cloud-provider   Whether or not to enable cloud provider support in kubelet\n  -h, --help                    help for worker\n      --profile string          worker profile to use on the node (default \"default\")\n      --token-file string       Path to the file containing token.\n</code></pre>"},{"location":"cli/k0s_install_worker/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n      --debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n  -l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_install_worker/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s install  - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo)</li> </ul>"},{"location":"cli/k0s_kubeconfig/","title":"K0s kubeconfig","text":""},{"location":"cli/k0s_kubeconfig/#k0s-kubeconfig","title":"k0s kubeconfig","text":"<p>Create a kubeconfig file for a specified user</p> <pre><code>k0s kubeconfig [command] [flags]\n</code></pre>"},{"location":"cli/k0s_kubeconfig/#options","title":"Options","text":"<pre><code>  -h, --help   help for kubeconfig\n</code></pre>"},{"location":"cli/k0s_kubeconfig/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n      --debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n  -l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_kubeconfig/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s  - k0s - Zero Friction Kubernetes</li> <li>k0s kubeconfig admin    - Display Admin's Kubeconfig file</li> <li>k0s kubeconfig create  - Create a kubeconfig for a user</li> </ul>"},{"location":"cli/k0s_kubeconfig_admin/","title":"K0s kubeconfig admin","text":""},{"location":"cli/k0s_kubeconfig_admin/#k0s-kubeconfig-admin","title":"k0s kubeconfig admin","text":"<p>Display Admin's Kubeconfig file</p>"},{"location":"cli/k0s_kubeconfig_admin/#synopsis","title":"Synopsis","text":"<p>Print kubeconfig for the Admin user to stdout</p> <pre><code>k0s kubeconfig admin [command] [flags]\n</code></pre>"},{"location":"cli/k0s_kubeconfig_admin/#examples","title":"Examples","text":"<pre><code>    $ k0s kubeconfig admin &gt; ~/.kube/config\n    $ export KUBECONFIG=~/.kube/config\n    $ kubectl get nodes\n</code></pre>"},{"location":"cli/k0s_kubeconfig_admin/#options","title":"Options","text":"<pre><code>  -h, --help   help for admin\n</code></pre>"},{"location":"cli/k0s_kubeconfig_admin/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n      --debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n  -l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_kubeconfig_admin/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s kubeconfig    - Create a kubeconfig file for a specified user</li> </ul>"},{"location":"cli/k0s_kubeconfig_create/","title":"K0s kubeconfig create","text":""},{"location":"cli/k0s_kubeconfig_create/#k0s-kubeconfig-create","title":"k0s kubeconfig create","text":"<p>Create a kubeconfig for a user</p>"},{"location":"cli/k0s_kubeconfig_create/#synopsis","title":"Synopsis","text":"<p>Create a kubeconfig with a signed certificate and public key for a given user (and optionally user groups) Note: A certificate once signed cannot be revoked for a particular user</p> <pre><code>k0s kubeconfig create [username] [flags]\n</code></pre>"},{"location":"cli/k0s_kubeconfig_create/#examples","title":"Examples","text":"<pre><code>    Command to create a kubeconfig for a user:\n    CLI argument:\n    $ k0s kubeconfig create [username]\n\n    optionally add groups:\n    $ k0s kubeconfig create [username] --groups [groups]\n</code></pre>"},{"location":"cli/k0s_kubeconfig_create/#options","title":"Options","text":"<pre><code>      --groups string   Specify groups\n  -h, --help            help for create\n</code></pre>"},{"location":"cli/k0s_kubeconfig_create/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n      --debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n  -l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_kubeconfig_create/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s kubeconfig    - Create a kubeconfig file for a specified user</li> </ul>"},{"location":"cli/k0s_status/","title":"K0s status","text":""},{"location":"cli/k0s_status/#k0s-status","title":"k0s status","text":"<p>Helper command for get general information about k0s</p> <pre><code>k0s status [flags]\n</code></pre>"},{"location":"cli/k0s_status/#examples","title":"Examples","text":"<pre><code>The command will return information about system init, PID, k0s role, kubeconfig and similar.\n</code></pre>"},{"location":"cli/k0s_status/#options","title":"Options","text":"<pre><code>  -h, --help         help for status\n  -o, --out string   sets type of out put to json or yaml\n</code></pre>"},{"location":"cli/k0s_status/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n      --debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n  -l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_status/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s  - k0s - Zero Friction Kubernetes</li> </ul>"},{"location":"cli/k0s_token/","title":"K0s token","text":""},{"location":"cli/k0s_token/#k0s-token","title":"k0s token","text":"<p>Manage join tokens</p> <pre><code>k0s token [flags]\n</code></pre>"},{"location":"cli/k0s_token/#options","title":"Options","text":"<pre><code>  -h, --help                help for token\n      --kubeconfig string   path to kubeconfig file [$KUBECONFIG]\n</code></pre>"},{"location":"cli/k0s_token/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n      --debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n  -l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_token/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s  - k0s - Zero Friction Kubernetes</li> <li>k0s token create    - Create join token</li> </ul>"},{"location":"cli/k0s_token_create/","title":"K0s token create","text":""},{"location":"cli/k0s_token_create/#k0s-token-create","title":"k0s token create","text":"<p>Create join token</p> <pre><code>k0s token create [flags]\n</code></pre>"},{"location":"cli/k0s_token_create/#options","title":"Options","text":"<pre><code>      --expiry string   set duration time for token (default \"0\")\n  -h, --help            help for create\n      --role string     Either worker or controller (default \"worker\")\n      --wait            wait forever (default false)\n</code></pre>"},{"location":"cli/k0s_token_create/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n      --debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n  -l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_token_create/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s token  - Manage join tokens</li> </ul>"},{"location":"cli/k0s_validate/","title":"K0s validate","text":""},{"location":"cli/k0s_validate/#k0s-validate","title":"k0s validate","text":"<p>Helper command for validating the config file</p>"},{"location":"cli/k0s_validate/#options","title":"Options","text":"<pre><code>  -h, --help   help for validate\n</code></pre>"},{"location":"cli/k0s_validate/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n      --debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n  -l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_validate/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s  - k0s - Zero Friction Kubernetes</li> <li>k0s validate config  - Helper command for validating the config file</li> </ul>"},{"location":"cli/k0s_validate_config/","title":"K0s validate config","text":""},{"location":"cli/k0s_validate_config/#k0s-validate-config","title":"k0s validate config","text":"<p>Helper command for validating the config file</p>"},{"location":"cli/k0s_validate_config/#synopsis","title":"Synopsis","text":"<p>Example:    k0s validate config --config path_to_config.yaml</p> <pre><code>k0s validate config [flags]\n</code></pre>"},{"location":"cli/k0s_validate_config/#options","title":"Options","text":"<pre><code>  -h, --help   help for config\n</code></pre>"},{"location":"cli/k0s_validate_config/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n      --debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n  -l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_validate_config/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s validate    - Helper command for validating the config file</li> </ul>"},{"location":"cli/k0s_version/","title":"K0s version","text":""},{"location":"cli/k0s_version/#k0s-version","title":"k0s version","text":"<p>Print the k0s version</p> <pre><code>k0s version [flags]\n</code></pre>"},{"location":"cli/k0s_version/#options","title":"Options","text":"<pre><code>  -h, --help   help for version\n</code></pre>"},{"location":"cli/k0s_version/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n      --debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n  -l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_version/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s  - k0s - Zero Friction Kubernetes</li> </ul>"},{"location":"cli/k0s_worker/","title":"K0s worker","text":""},{"location":"cli/k0s_worker/#k0s-worker","title":"k0s worker","text":"<p>Run worker</p> <pre><code>k0s worker [join-token] [flags]\n</code></pre>"},{"location":"cli/k0s_worker/#examples","title":"Examples","text":"<pre><code>    Command to add worker node to the master node:\n    CLI argument:\n    $ k0s worker [token]\n\n    or CLI flag:\n    $ k0s worker --token-file [path_to_file]\n    Note: Token can be passed either as a CLI argument or as a flag\n</code></pre>"},{"location":"cli/k0s_worker/#options","title":"Options","text":"<pre><code>      --api-server string       HACK: api-server for the windows worker node\n      --cidr-range string       HACK: cidr range for the windows worker node (default \"10.96.0.0/12\")\n      --cluster-dns string      HACK: cluster dns for the windows worker node (default \"10.96.0.10\")\n      --cri-socket string       contrainer runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket]\n      --enable-cloud-provider   Whether or not to enable cloud provider support in kubelet\n  -h, --help                    help for worker\n      --profile string          worker profile to use on the node (default \"default\")\n      --token-file string       Path to the file containing token.\n</code></pre>"},{"location":"cli/k0s_worker/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>  -c, --config string            config file (default: ./k0s.yaml)\n      --data-dir string          Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break!\n  -d, --debug                    Debug logging (default: false)\n      --debugListenOn string     Http listenOn for debug pprof handler (default \":6060\")\n  -l, --logging stringToString   Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info])\n</code></pre>"},{"location":"cli/k0s_worker/#see-also","title":"SEE ALSO","text":"<ul> <li>k0s  - k0s - Zero Friction Kubernetes</li> </ul>"},{"location":"contributors/CODE_OF_CONDUCT/","title":"k0s Community Code of Conduct","text":"<p>k0s follows the CNCF Code of Conduct.</p>"},{"location":"contributors/github_workflow/","title":"Github Workflow","text":"<ul> <li>Fork The Project</li> <li>Adding the Forked Remote</li> <li>Create &amp; Rebase Your Feature Branch</li> <li>Commit &amp; Push</li> <li>Open a Pull Request<ul> <li>Get a code review</li> <li>Squash commits</li> <li>Push Your Final Changes</li> </ul> </li> </ul> <p>This guide assumes you have already cloned the upstream repo to your system via git clone, or via <code>go get github.com/k0sproject/k0s</code>.</p>"},{"location":"contributors/github_workflow/#fork-the-project","title":"Fork The Project","text":"<ol> <li>Go to http://github.com/k0sproject/k0s</li> <li>On the top, right-hand side, click on \"fork\" and select your username for the fork destination.</li> </ol>"},{"location":"contributors/github_workflow/#adding-the-forked-remote","title":"Adding the Forked Remote","text":"<p><pre><code>export GITHUB_USER={ your github's username }\n</code></pre> <pre><code>cd $WORKDIR/k0s\ngit remote add $GITHUB_USER git@github.com:${GITHUB_USER}/k0s.git\n\n# Prevent push to Upstream\ngit remote set-url --push origin no_push\n\n# Set your fork remote as a default push target\ngit push --set-upstream $GITHUB_USER main\n</code></pre></p> <p>Your remotes should look something like this: <pre><code>\u279c git remote -v\norigin  https://github.com/k0sproject/k0s (fetch)\norigin  no_push (push)\nmy_fork git@github.com:{ github_username }/k0s.git (fetch)\nmy_fork git@github.com:{ github_username }/k0s.git (push)\n</code></pre></p>"},{"location":"contributors/github_workflow/#create-rebase-your-feature-branch","title":"Create &amp; Rebase Your Feature Branch","text":"<p>Create a feature branch and switch to it: <pre><code>git checkout -b my_feature_branch\n</code></pre> Rebase your branch: <pre><code>git fetch origin\n\ngit rebase origin/main\nCurrent branch my_feature_branch is up to date.\n</code></pre> Please don't use <code>git pull</code> instead of the above <code>fetch / rebase</code>. <code>git pull</code> does a merge, which leaves merge commits. These make the commit history messy and violate the principle that commits ought to be individually understandable and useful.</p>"},{"location":"contributors/github_workflow/#commit-push","title":"Commit &amp; Push","text":"<p>Commit and sign your changes: <pre><code>git commit --signoff\n</code></pre> The commit message should have a short title as first line, an empty line and then a longer description that explains why the change was made, unless it is obvious.</p> <p>You can go back and edit/build/test some more, then <code>commit --amend</code> in a few cycles.</p> <p>When ready, push your changes to your fork's repository: <pre><code>git push --set-upstream my_fork my_feature_branch\n</code></pre></p>"},{"location":"contributors/github_workflow/#open-a-pull-request","title":"Open a Pull Request","text":"<p>Github Docs</p>"},{"location":"contributors/github_workflow/#get-a-code-review","title":"Get a code review","text":"<p>Once your pull request has been opened it will be assigned to one or more reviewers, and will go through a series of smoke tests.</p> <p>Commit changes made in response to review comments should be added to the same branch on your fork.</p> <p>Very small PRs are easy to review. Very large PRs are very difficult to review.</p>"},{"location":"contributors/github_workflow/#squashing-commits","title":"Squashing Commits","text":"<p>Commits on your branch should represent meaningful milestones or units of work. Small commits that contain typo fixes, rebases, review feedbacks, etc should be squashed.</p> <p>To do that, it's best to perform an interactive rebase:</p>"},{"location":"contributors/github_workflow/#example","title":"Example","text":"<p>Rebase your feature branch against upstream main branch: <pre><code>git rebase -i origin/main\n</code></pre></p> <p>If your PR has 3 commits, output would be similar to this: <pre><code>pick f7f3f6d Changed some code\npick 310154e fixed some typos\npick a5f4a0d made some review changes\n\n# Rebase 710f0f8..a5f4a0d onto 710f0f8\n#\n# Commands:\n# p, pick &lt;commit&gt; = use commit\n# r, reword &lt;commit&gt; = use commit, but edit the commit message\n# e, edit &lt;commit&gt; = use commit, but stop for amending\n# s, squash &lt;commit&gt; = use commit, but meld into previous commit\n# f, fixup &lt;commit&gt; = like \"squash\", but discard this commit's log message\n# x, exec &lt;command&gt; = run command (the rest of the line) using shell\n# b, break = stop here (continue rebase later with 'git rebase --continue')\n# d, drop &lt;commit&gt; = remove commit\n# l, label &lt;label&gt; = label current HEAD with a name\n# t, reset &lt;label&gt; = reset HEAD to a label\n# m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;]\n# .       create a merge commit using the original merge commit's\n# .       message (or the oneline, if no original merge commit was\n# .       specified). Use -c &lt;commit&gt; to reword the commit message.\n#\n# These lines can be re-ordered; they are executed from top to bottom.\n#\n# However, if you remove everything, the rebase will be aborted.\n#\n# Note that empty commits are commented out\n</code></pre> Use a command line text editor to change the word <code>pick</code> to <code>f</code> of <code>fixup</code> for the commits you want to squash, then save your changes and continue the rebase:</p> <p>Per the output above, you can see that: <pre><code>fixup &lt;commit&gt; = like \"squash\", but discard this commit's log message\n</code></pre> Which means that when rebased, the commit message \"fixed some typos\" will be removed, and squashed with the parent commit.</p>"},{"location":"contributors/github_workflow/#push-your-final-changes","title":"Push Your Final Changes","text":"<p>Once done, you can push the final commits to your branch: <pre><code>git push --force\n</code></pre> You can run multiple iteration of <code>rebase</code>/<code>push -f</code>, if needed.</p>"},{"location":"contributors/overview/","title":"Contributing to k0s","text":"<p>Thank you for taking the time to make a contribution to k0s. The following document is a set of guidelines and instructions for contributing to k0s.</p> <p>When contributing to this repository, please consider first discussing the change you wish to make by opening an issue.</p>"},{"location":"contributors/overview/#code-of-conduct","title":"Code of Conduct","text":"<p>Our code of conduct can be found in the link below. Please follow it in all your interactions with the project.</p> <ul> <li>Code Of Conduct </li> </ul>"},{"location":"contributors/overview/#github-workflow","title":"Github Workflow","text":"<p>We Use Github Flow, so all code changes are tracked via Pull Requests. A detailed guide on the recommended workflow can be found below:</p> <ul> <li>Github Workflow</li> </ul>"},{"location":"contributors/overview/#code-testing","title":"Code Testing","text":"<p>All submitted PRs go through a set of tests and reviews. You can run most of these tests before a PR is submitted. In fact, we recommend it, because it will save on many possible review iterations and automated tests. The testing guidelines can be found here:</p> <ul> <li>Contributor's Guide to Testing</li> </ul>"},{"location":"contributors/overview/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed as followed:</p> <ul> <li>All content residing under the \"docs/\" directory of this repository is licensed under \"Creative Commons Attribution Share Alike 4.0 International\" (CC-BY-SA-4.0). See docs/LICENCE for details.</li> <li>Content outside of the above mentioned directories or restrictions above is available under the \"Apache License 2.0\".</li> </ul>"},{"location":"contributors/overview/#community","title":"Community","text":"<p>Some of you might have noticed we have official community blog hosted on Medium. If you are not yet following us, we'd like to invite you to do so now!  Make sure to follow us on Twitter as well \ud83d\ude0a We have also agreed to share Slack with Lens IDE team. They are close friends of the k0s crew, and many of the Kubernetes users are using Lens anyway, so it felt very natural for us to share their Slack. </p> <p>You\u2019ll find us on dedicated k0s channel. Please join the k0s Slack channel to hear the latest news, discussions and provide your feedback!</p>"},{"location":"contributors/testing/","title":"Testing","text":""},{"location":"contributors/testing/#testing-your-code","title":"Testing Your Code","text":"<p>k0s uses github actions to run automated tests on any PR, before merging. However, a PR will not be reviewed before all tests are green, so to save time and prevent your PR from going stale, it is best to test it before submitting the PR.</p>"},{"location":"contributors/testing/#run-local-verifications","title":"Run Local Verifications","text":"<p>Please run the following style and formatting commands and fix/check-in any changes:</p>"},{"location":"contributors/testing/#1-linting","title":"1. Linting","text":"<p>We use golangci-lint for style verification. In the repository's root directory, simply run: <pre><code>make lint\n</code></pre></p>"},{"location":"contributors/testing/#2-go-fmt","title":"2. Go fmt","text":"<pre><code>go fmt ./...\n</code></pre>"},{"location":"contributors/testing/#3-pre-submit-flight-checks","title":"3. Pre-submit Flight Checks","text":"<p>In the repository root directory, make sure that:</p> <ul> <li><code>make build</code> runs successfully.</li> <li><code>make check-basic</code> runs successfully.</li> <li><code>make check-unit</code> has no errors.</li> <li><code>make check-hacontrolplane</code> runs successfully. </li> </ul> <p>Please note that this last test is prone to \"flakiness\", so it might fail on occasion. If it fails constantly, take a deeper look at your code to find the source of the problem.</p> <p>If you find that all tests passed, you may open a pull request upstream.</p>"},{"location":"contributors/testing/#opening-a-pull-request","title":"Opening A Pull Request","text":""},{"location":"contributors/testing/#draft-mode","title":"Draft Mode","text":"<p>You may open a pull request in draft mode. All automated tests will still run against the PR, but the PR will not be assigned for review. Once a PR is ready for review, transition it from Draft mode, and code owners will be notified.</p>"},{"location":"contributors/testing/#conformance-testing","title":"Conformance Testing","text":"<p>Once a PR has been reviewed and all other tests have passed, a code owner will run a full end-to-end conformance test against the PR. This is usually the last step before merging.</p>"},{"location":"contributors/testing/#pre-requisites-for-pr-merge","title":"Pre-Requisites for PR Merge","text":"<p>In order for a PR to be merged, the following conditions should exist: 1. The PR has passed all the automated tests (style, build &amp; conformance tests). 2. PR commits have been signed with the <code>--signoff</code> option. 3. PR was reviewed and approved by a code owner. 4. PR is rebased against upstream's main branch.</p>"},{"location":"examples/ambassador-ingress/","title":"Installing the Ambassador Gateway on k0s","text":"<p>In this tutorial, you'll learn how to run k0s under Docker and configure it with the Ambassador API Gateway and a MetalLB service loadbalancer. We'll also deploy a sample  service and expose it with an Ambassador mapping resource.</p> <p>Utilizing the extensible bootstrapping functionality with Helm,  it's as simple as adding the right extensions to the <code>k0s.yaml</code> file  when configuring your cluster.</p>"},{"location":"examples/ambassador-ingress/#running-k0s-under-docker","title":"Running k0s under docker","text":"<p>If you're not on a platform natively supported by k0s, running under docker is a viable option  (see k0s in Docker). Since we're going to create a custom configuration file we'll need to map that into the k0s container - and of course we'll need to expose the ports required by Ambassador for outside access.</p> <p>Start by running k0s under docker :</p> <pre><code>docker run -d --name k0s --hostname k0s --privileged -v /var/lib/k0s -p 6443:6443 docker.io/k0sproject/k0s:latest\n</code></pre> <p>Once running, export the default k0s configuration file using</p> <pre><code>docker exec k0s k0s default-config &gt; k0s.yaml\n</code></pre> <p>and export the cluster config so you can access it with kubectl:</p> <p><pre><code>docker exec k0s cat /var/lib/k0s/pki/admin.conf &gt; k0s-cluster.conf\nexport KUBECONFIG=$KUBECONFIG:&lt;absolute path to k0s-cluster.conf&gt;\n</code></pre> (somewhat brute-force but gets the job done)</p>"},{"location":"examples/ambassador-ingress/#configuring-k0syaml","title":"Configuring k0s.yaml","text":"<p>Open the file in your favorite code editor and add the following extensions at the bottom:</p> <p><pre><code>extensions:\nhelm:\nrepositories:\n- name: datawire\nurl: https://www.getambassador.io\n- name: bitnami\nurl: https://charts.bitnami.com/bitnami\ncharts:\n- name: ambassador\nchartname: datawire/ambassador\nversion: \"6.5.13\"\nnamespace: ambassador\nvalues: |2\nservice:\nexternalIPs:\n- 172.17.0.2\n- name: metallb\nchartname: bitnami/metallb\nversion: \"1.0.1\"\nnamespace: default\nvalues: |2\nconfigInline:\naddress-pools:\n- name: generic-cluster-pool\nprotocol: layer2\naddresses:\n- 172.17.0.2\n</code></pre> (you might need to replace the 172.17.0.2 IP with your local IP which you can find higher up in the generated file under spec.api.address)</p> <p>As you can see it adds both Ambassador and Metallb (required for LoadBalancers) with corresponding repositories and (minimal) configurations. This example only uses your local network - providing a range of IPs for  MetalLB that are addressable on your LAN is suggested if you want to access these services from anywhere on  your network.</p> <p>Now stop/remove your k0s container with <code>docker stop k0s</code> and <code>docker rm k0s</code>, then start it again with additional ports and the above config file mapped into it:</p> <pre><code>docker run --name k0s --hostname k0s --privileged -v /var/lib/k0s -v &lt;path to k0s.yaml file&gt;:/k0s.yaml -p 6443:6443 -p 80:80 -p 443:443 -p 8080:8080 docker.io/k0sproject/k0s:latest\n</code></pre> <p>Let it start, and eventually (this can take some time) you'll be able to list the Ambassador Services:</p> <pre><code>kubectl get services -n ambassador\nNAME                          TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE\nambassador-1611224811         LoadBalancer   10.99.84.151    172.17.0.2    80:30327/TCP,443:30355/TCP   2m11s\nambassador-1611224811-admin   ClusterIP      10.96.79.130    &lt;none&gt;        8877/TCP                     2m11s\nambassador-1611224811-redis   ClusterIP      10.110.33.229   &lt;none&gt;        6379/TCP                     2m11s\n</code></pre> <p>Install the Ambassador edgectl tool  and run the login command:</p> <pre><code>edgectl login --namespace=ambassador localhost\n</code></pre> <p>This will open your browser and take you to the Ambassador Console - all ready to go.</p>"},{"location":"examples/ambassador-ingress/#deploy-map-a-service","title":"Deploy / map a service","text":"<p>Let's deploy and map the Swagger Petstore service; create a petstore.yaml file with the following content.</p> <pre><code>---\napiVersion: v1\nkind: Service\nmetadata:\nname: petstore\nnamespace: ambassador\nspec:\nports:\n- name: http\nport: 80\ntargetPort: 8080\nselector:\napp: petstore\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: petstore\nnamespace: ambassador\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: petstore\nstrategy:\ntype: RollingUpdate\ntemplate:\nmetadata:\nlabels:\napp: petstore\nspec:\ncontainers:\n- name: petstore-backend\nimage: docker.io/swaggerapi/petstore3:unstable\nports:\n- name: http\ncontainerPort: 8080\n---\napiVersion: getambassador.io/v2\nkind:  Mapping\nmetadata:\nname: petstore\nnamespace: ambassador\nspec:\nprefix: /petstore/\nservice: petstore\n</code></pre> <p>Once you've created this, apply it:</p> <pre><code>kubectl apply -f petstore.yaml\nservice/petstore created\ndeployment.apps/petstore created\nmapping.getambassador.io/petstore created\n</code></pre> <p>and you should be able to curl the service:</p> <pre><code>curl -k 'https://localhost/petstore/api/v3/pet/findByStatus?status=available'\n[{\"id\":1,\"category\":{\"id\":2,\"name\":\"Cats\"},\"name\":\"Cat 1\",\"photoUrls\":[\"url1\",\"url2\"],\"tags\":[{\"id\":1,\"name\":\"tag1\"},{\"id\":2,\"name\":\"tag2\"}],\"status\":\"available\"},{\"id\":2,\"category\":{\"id\":2,\"name\":\"Cats\"},\"name\":\"Cat 2\",\"photoUrls\":[\"url1\",\"url2\"],\"tags\":[{\"id\":1,\"name\":\"tag2\"},{\"id\":2,\"name\":\"tag3\"}],\"status\":\"available\"},{\"id\":4,\"category\":{\"id\":1,\"name\":\"Dogs\"},\"name\":\"Dog 1\",\"photoUrls\":[\"url1\",\"url2\"],\"tags\":[{\"id\":1,\"name\":\"tag1\"},{\"id\":2,\"name\":\"tag2\"}],\"status\":\"available\"},{\"id\":7,\"category\":{\"id\":4,\"name\":\"Lions\"},\"name\":\"Lion 1\",\"photoUrls\":[\"url1\",\"url2\"],\"tags\":[{\"id\":1,\"name\":\"tag1\"},{\"id\":2,\"name\":\"tag2\"}],\"status\":\"available\"},{\"id\":8,\"category\":{\"id\":4,\"name\":\"Lions\"},\"name\":\"Lion 2\",\"photoUrls\":[\"url1\",\"url2\"],\"tags\":[{\"id\":1,\"name\":\"tag2\"},{\"id\":2,\"name\":\"tag3\"}],\"status\":\"available\"},{\"id\":9,\"category\":{\"id\":4,\"name\":\"Lions\"},\"name\":\"Lion 3\",\"photoUrls\":[\"url1\",\"url2\"],\"tags\":[{\"id\":1,\"name\":\"tag3\"},{\"id\":2,\"name\":\"tag4\"}],\"status\":\"available\"},{\"id\":10,\"category\":{\"id\":3,\"name\":\"Rabbits\"},\"name\":\"Rabbit 1\",\"photoUrls\":[\"url1\",\"url2\"],\"tags\":[{\"id\":1,\"name\":\"tag3\"},{\"id\":2,\"name\":\"tag4\"}],\"status\":\"available\"}]\n</code></pre> <p>or you can open https://localhost/petstore/ in your browser and change the URL of the specification to https://localhost/petstore/api/v3/openapi.json (since we mapped it to the /petstore prefix). </p> <p>If you navigate to the Mappings part of the Ambassador Console (opened above) you will see the corresponding  PetStore mapping as configured.</p>"},{"location":"examples/ambassador-ingress/#summary","title":"Summary","text":"<p>This should get you all set with running Ambassador under k0s. If you're not running under Docker just skip the docker-related steps above - but make sure that you have updated the k0s configuration in the same way as above. </p> <p>If you're stuck with or have any questions about Ambassador please try the Ambassador Slack to get help.</p>"},{"location":"examples/ansible-playbook/","title":"Creating a cluster with Ansible Playbook","text":"<p>Using Ansible and the k0s-ansible playbook, you can install a multi-node Kubernetes Cluster in a couple of minutes. Ansible is a popular infrastructure as code tool which helps you automate tasks to achieve the desired state in a system.</p> <p>This guide shows how you can install k0s on local virtual machines. In this guide, the following tools are used:</p> <ul> <li><code>multipass</code>, a lightweight VM manager that uses KVM on Linux, Hyper-V on Windows, and hypervisor.framework on macOS (installation guide).</li> <li><code>ansible</code>, a popular infrastructure as code tool (installation guide).</li> <li>and of course <code>kubectl</code> on your local machine (Installation guide).</li> </ul> <p>Before following this tutorial, you should have a general understanding of Ansible. A great way to start is the official Ansible User Guide.</p> <p>Please note: k0s users created k0s-ansible. Please send your feedback, bug reports, and pull requests to github.com/movd/k0s-ansible.</p> <p>Without further ado, let's jump right in.</p>"},{"location":"examples/ansible-playbook/#download-k0s-ansible","title":"Download k0s-ansible","text":"<p>On your local machine clone the k0s-ansible repository:</p> <pre><code>$ git clone https://github.com/movd/k0s-ansible.git\n$ cd k0s-ansible\n</code></pre>"},{"location":"examples/ansible-playbook/#create-virtual-machines","title":"Create virtual machines","text":"<p>For this tutorial, multipass was used. However, there is no interdependence. This playbook should also work with VMs created in alternative ways or Raspberry Pis.</p> <p>Next, create a couple of virtual machines. For the automation to work, each instance must have passwordless SSH access. To achieve this, we provision each instance with a cloud-init manifest that imports your current users' public SSH key and into a user <code>k0s</code>. For your convenience, a bash script is included that does just that:</p> <p><code>./tools/multipass_create_instances.sh 7</code> \u25c0\ufe0f this creates 7 virtual machines</p> <pre><code>$ ./tools/multipass_create_instances.sh 7\nCreate cloud-init to import ssh key...\n[1/7] Creating instance k0s-1 with multipass...\nLaunched: k0s-1\n[2/7] Creating instance k0s-2 with multipass...\nLaunched: k0s-2\n[3/7] Creating instance k0s-3 with multipass...\nLaunched: k0s-3\n[4/7] Creating instance k0s-4 with multipass...\nLaunched: k0s-4\n[5/7] Creating instance k0s-5 with multipass...\nLaunched: k0s-5\n[6/7] Creating instance k0s-6 with multipass...\nLaunched: k0s-6\n[7/7] Creating instance k0s-7 with multipass...\nLaunched: k0s-7\nName State IPv4 Image\nk0s-1 Running 192.168.64.32 Ubuntu 20.04 LTS\nk0s-2 Running 192.168.64.33 Ubuntu 20.04 LTS\nk0s-3 Running 192.168.64.56 Ubuntu 20.04 LTS\nk0s-4 Running 192.168.64.57 Ubuntu 20.04 LTS\nk0s-5 Running 192.168.64.58 Ubuntu 20.04 LTS\nk0s-6 Running 192.168.64.60 Ubuntu 20.04 LTS\nk0s-7 Running 192.168.64.61 Ubuntu 20.04 LTS\n</code></pre>"},{"location":"examples/ansible-playbook/#create-ansible-inventory","title":"Create Ansible inventory","text":"<p>After that, we create our inventory directory by copying the sample:</p> <pre><code>$ cp -rfp inventory/sample inventory/multipass\n</code></pre> <p>Now we need to create our inventory. The before built virtual machines need to be assigned to the different host groups required by the playbook's logic.</p> <ul> <li><code>initial_controller</code> = must contain a single node that creates the worker and controller tokens needed by the other nodes.</li> <li><code>controller</code> = can contain nodes that, together with the host from <code>initial_controller</code> form a highly available isolated control plane.</li> <li><code>worker</code> = must contain at least one node so that we can deploy Kubernetes objects.</li> </ul> <p>We could fill <code>inventory/multipass/inventory.yml</code> by hand with the metadata provided by <code>multipass list,</code> but since we are lazy and want to automate as much as possible, we can use the included Python script <code>multipass_generate_inventory.py</code>:</p> <p>To automatically fill our inventory run:</p> <pre><code>$ ./tools/multipass_generate_inventory.py\nDesignate first three instances as control plane\nCreated Ansible Inventory at: /Users/dev/k0s-ansible/tools/inventory.yml\n$ cp tools/inventory.yml inventory/multipass/inventory.yml\n</code></pre> <p>Now <code>inventory/multipass/inventory.yml</code> should look like this (Of course, your IP addresses might differ):</p> <pre><code>---\nall:\nchildren:\ninitial_controller:\nhosts:\nk0s-1:\ncontroller:\nhosts:\nk0s-2:\nk0s-3:\nworker:\nhosts:\nk0s-4:\nk0s-5:\nk0s-6:\nk0s-7:\nhosts:\nk0s-1:\nansible_host: 192.168.64.32\nk0s-2:\nansible_host: 192.168.64.33\nk0s-3:\nansible_host: 192.168.64.56\nk0s-4:\nansible_host: 192.168.64.57\nk0s-5:\nansible_host: 192.168.64.58\nk0s-6:\nansible_host: 192.168.64.60\nk0s-7:\nansible_host: 192.168.64.61\nvars:\nansible_user: k0s\n</code></pre>"},{"location":"examples/ansible-playbook/#test-the-connection-to-the-virtual-machines","title":"Test the connection to the virtual machines","text":"<p>To test the connection to your hosts just run:</p> <pre><code>$ ansible -i inventory/multipass/inventory.yml -m ping\nk0s-4 | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n...\n</code></pre> <p>If all is green and successful, you can proceed.</p>"},{"location":"examples/ansible-playbook/#provision-the-cluster-with-ansible","title":"Provision the cluster with Ansible","text":"<p>Finally, we can start provisioning the cluster. Applying the playbook, k0s will get downloaded and set up on all nodes, tokens will get exchanged, and a kubeconfig will get dumped to your local deployment environment.</p> <pre><code>$ ansible-playbook site.yml -i inventory/multipass/inventory.yml\n...\nTASK [k0s/initial_controller : print kubeconfig command] *******************************************************\nTuesday 22 December 2020  17:43:20 +0100 (0:00:00.257)       0:00:41.287 ******\nok: [k0s-1] =&gt; {\n    \"msg\": \"To use Cluster: export KUBECONFIG=/Users/dev/k0s-ansible/inventory/multipass/artifacts/k0s-kubeconfig.yml\"\n}\n...\nPLAY RECAP *****************************************************************************************************\nk0s-1                      : ok=21   changed=11   unreachable=0    failed=0    skipped=1    rescued=0    ignored=0\nk0s-2                      : ok=10   changed=5    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0\nk0s-3                      : ok=10   changed=5    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0\nk0s-4                      : ok=9    changed=5    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0\nk0s-5                      : ok=9    changed=5    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0\nk0s-6                      : ok=9    changed=5    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0\nk0s-7                      : ok=9    changed=5    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0\n\nTuesday 22 December 2020  17:43:36 +0100 (0:00:01.204)       0:00:57.478 ******\n===============================================================================\nprereq : Install apt packages -------------------------------------------------------------------------- 22.70s\nk0s/controller : Wait for k8s apiserver ----------------------------------------------------------------- 4.30s\nk0s/initial_controller : Create worker join token ------------------------------------------------------- 3.38s\nk0s/initial_controller : Wait for k8s apiserver --------------------------------------------------------- 3.36s\ndownload : Download k0s binary k0s-v0.9.0-rc1-amd64 ----------------------------------------------------- 3.11s\nGathering Facts ----------------------------------------------------------------------------------------- 2.85s\nGathering Facts ----------------------------------------------------------------------------------------- 1.95s\nprereq : Create k0s Directories ------------------------------------------------------------------------- 1.53s\nk0s/worker : Enable and check k0s service --------------------------------------------------------------- 1.20s\nprereq : Write the k0s config file ---------------------------------------------------------------------- 1.09s\nk0s/initial_controller : Enable and check k0s service --------------------------------------------------- 0.94s\nk0s/controller : Enable and check k0s service ----------------------------------------------------------- 0.73s\nGathering Facts ----------------------------------------------------------------------------------------- 0.71s\nGathering Facts ----------------------------------------------------------------------------------------- 0.66s\nGathering Facts ----------------------------------------------------------------------------------------- 0.64s\nk0s/worker : Write the k0s token file on worker --------------------------------------------------------- 0.64s\nk0s/worker : Copy k0s service file ---------------------------------------------------------------------- 0.53s\nk0s/controller : Write the k0s token file on controller ------------------------------------------------- 0.41s\nk0s/controller : Copy k0s service file ------------------------------------------------------------------ 0.40s\nk0s/initial_controller : Copy k0s service file ---------------------------------------------------------- 0.36s\n</code></pre>"},{"location":"examples/ansible-playbook/#use-the-cluster-with-kubectl","title":"Use the cluster with kubectl","text":"<p>While the playbook ran, a kubeconfig got copied to your local machine. You can use it to get simple access to your new Kubernetes cluster:</p> <pre><code>$ export KUBECONFIG=/Users/dev/k0s-ansible/inventory/multipass/artifacts/k0s-kubeconfig.yml\n$ kubectl cluster-info\nKubernetes control plane is running at https://192.168.64.32:6443\nCoreDNS is running at https://192.168.64.32:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\nMetrics-server is running at https://192.168.64.32:6443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\n\n$ kubectl get nodes -o wide\nNAME    STATUS     ROLES    AGE   VERSION        INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME\nk0s-4   Ready      &lt;none&gt;   21s   v1.20.1-k0s1   192.168.64.57   &lt;none&gt;        Ubuntu 20.04.1 LTS   5.4.0-54-generic   containerd://1.4.3\nk0s-5   Ready      &lt;none&gt;   21s   v1.20.1-k0s1   192.168.64.58   &lt;none&gt;        Ubuntu 20.04.1 LTS   5.4.0-54-generic   containerd://1.4.3\nk0s-6   NotReady   &lt;none&gt;   21s   v1.20.1-k0s1   192.168.64.60   &lt;none&gt;        Ubuntu 20.04.1 LTS   5.4.0-54-generic   containerd://1.4.3\nk0s-7   NotReady   &lt;none&gt;   21s   v1.20.1-k0s1   192.168.64.61   &lt;none&gt;        Ubuntu 20.04.1 LTS   5.4.0-54-generic   containerd://1.4.3\n</code></pre> <p>\u2b06\ufe0f Of course, the first three control plane nodes won't show up here because the control plane is fully isolated. You can check on the distributed etcd cluster by running this ad-hoc command (or ssh'ing directly into a controller node):</p> <pre><code>$ ansible k0s-1 -a \"k0s etcd member-list -c /etc/k0s/k0s.yaml\" -i inventory/multipass/inventory.yml | tail -1 | jq\n{\n  \"level\": \"info\",\n  \"members\": {\n    \"k0s-1\": \"https://192.168.64.32:2380\",\n    \"k0s-2\": \"https://192.168.64.33:2380\",\n    \"k0s-3\": \"https://192.168.64.56:2380\"\n  },\n  \"msg\": \"done\",\n  \"time\": \"2020-12-23T00:21:22+01:00\"\n}\n</code></pre> <p>After a while, all worker nodes become <code>Ready</code>. Your cluster is now waiting to get used. We can test by creating a simple nginx deployment.</p> <pre><code>$ kubectl create deployment nginx --image=gcr.io/google-containers/nginx --replicas=5\ndeployment.apps/nginx created\n\n$ kubectl expose deployment nginx --target-port=80 --port=8100\nservice/nginx exposed\n\n$ kubectl run hello-k0s --image=quay.io/prometheus/busybox --rm -it --restart=Never --command -- wget -qO- nginx:8100\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx on Debian!&lt;/title&gt;\n...\npod \"hello-k0s\" deleted\n</code></pre>"},{"location":"examples/traefik-ingress/","title":"Installing the Traefik Ingress Controller on k0s","text":"<p>In this tutorial, you'll learn how to configure k0s with the Traefik ingress controller, a MetalLB service loadbalancer, and deploy the Traefik Dashboard along with a service example. Utilizing the extensible bootstrapping functionality with Helm,  it's as simple as adding the right extensions to the <code>k0s.yaml</code> file  when configuring your cluster.</p>"},{"location":"examples/traefik-ingress/#configuring-k0syaml","title":"Configuring k0s.yaml","text":"<p>Modify your <code>k0s.yaml</code> file to include the Traefik and MetalLB helm charts as extensions, and these will install during the cluster's bootstrap.</p> <p>Note: You may want to have a small range of IP addresses that are addressable on your network, preferably outside the assignment pool allocated by your DHCP server. Providing an addressable range should allow you to access your LoadBalancer and Ingress services from anywhere on your local network. However, any valid IP range should work locally on your machine.</p> <pre><code>extensions:\nhelm:\nrepositories:\n- name: traefik\nurl: https://helm.traefik.io/traefik\n- name: bitnami\nurl: https://charts.bitnami.com/bitnami\ncharts:\n- name: traefik\nchartname: traefik/traefik\nversion: \"9.11.0\"\nnamespace: default\n- name: metallb\nchartname: bitnami/metallb\nversion: \"1.0.1\"\nnamespace: default\nvalues: |2\nconfigInline:\naddress-pools:\n- name: generic-cluster-pool\nprotocol: layer2\naddresses:\n- 192.168.0.5-192.168.0.10\n</code></pre> <p>Providing a range of IPs for MetalLB that are addressable on your LAN is suggested  if you want to access LoadBalancer and Ingress services from anywhere on your local network.</p>"},{"location":"examples/traefik-ingress/#retrieving-the-load-balancer-ip","title":"Retrieving the Load Balancer IP","text":"<p>Once you've started your cluster, you should confirm the deployment of Traefik and MetalLB. Executing a <code>kubectl get all</code> should include a response with the <code>metallb</code> and <code>traefik</code> resources,  along with a service loadbalancer that has an <code>EXTERNAL-IP</code> assigned to it.  See the example below:</p> <pre><code>root@k0s-host \u279c kubectl get all\nNAME                                                 READY   STATUS    RESTARTS   AGE\npod/metallb-1607085578-controller-864c9757f6-bpx6r   1/1     Running   0          81s\npod/metallb-1607085578-speaker-245c2                 1/1     Running   0          60s\npod/traefik-1607085579-77bbc57699-b2f2t              1/1     Running   0          81s\n\nNAME                         TYPE           CLUSTER-IP       EXTERNAL-IP      PORT(S)                      AGE\nservice/kubernetes           ClusterIP      10.96.0.1        &lt;none&gt;           443/TCP                      96s\nservice/traefik-1607085579   LoadBalancer   10.105.119.102   192.168.0.5      80:32153/TCP,443:30791/TCP   84s\n\nNAME                                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\ndaemonset.apps/metallb-1607085578-speaker   1         1         1       1            1           kubernetes.io/os=linux   87s\n\nNAME                                            READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/metallb-1607085578-controller   1/1     1            1           87s\ndeployment.apps/traefik-1607085579              1/1     1            1           84s\n\nNAME                                                       DESIRED   CURRENT   READY   AGE\nreplicaset.apps/metallb-1607085578-controller-864c9757f6   1         1         1       81s\nreplicaset.apps/traefik-1607085579-77bbc57699              1         1         1       81s\n</code></pre> <p>Take note of the <code>EXTERNAL-IP</code> given to the <code>service/traefik-n</code> LoadBalancer. In this example, <code>192.168.0.5</code> has been assigned and can be used to access services via the Ingress proxy:</p> <pre><code>NAME                         TYPE           CLUSTER-IP       EXTERNAL-IP      PORT(S)                      AGE\nservice/traefik-1607085579   LoadBalancer   10.105.119.102   192.168.0.5      80:32153/TCP,443:30791/TCP   84s\n# Receiving a 404 response here is normal, as you've not configured any Ingress resources to respond yet\nroot@k0s-host \u279c curl http://192.168.0.5\n404 page not found\n</code></pre>"},{"location":"examples/traefik-ingress/#deploy-and-access-the-traefik-dashboard","title":"Deploy and access the Traefik Dashboard","text":"<p>Now that you have an available and addressable load balancer on your cluster,  you can quickly deploy the Traefik dashboard and access it from anywhere on your local network  (provided that you configured MetalLB with an addressable range).</p> <p>Create the Traefik Dashboard IngressRoute  in a YAML file:</p> <pre><code>apiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\nname: dashboard\nspec:\nentryPoints:\n- web\nroutes:\n- match: PathPrefix(`/dashboard`) || PathPrefix(`/api`)\nkind: Rule\nservices:\n- name: api@internal\nkind: TraefikService\n</code></pre> <p>Next, deploy the resource:</p> <pre><code>root@k0s-host \u279c kubectl apply -f traefik-dashboard.yaml\ningressroute.traefik.containo.us/dashboard created\n</code></pre> <p>Once deployed, you should be able to access the dashboard using the <code>EXTERNAL-IP</code>  that you noted above by visiting <code>http://192.168.0.5</code> in your browser:</p> <p></p> <p>Now, create a simple <code>whoami</code> Deployment, Service,  and Ingress manifest:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: whoami-deployment\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: whoami\ntemplate:\nmetadata:\nlabels:\napp: whoami\nspec:\ncontainers:\n- name: whoami-container\nimage: containous/whoami\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: whoami-service\nspec:\nports:\n- name: http\ntargetPort: 80\nport: 80\nselector:\napp: whoami\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\nname: whoami-ingress\nspec:\nrules:\n- http:\npaths:\n- path: /whoami\npathType: Exact\nbackend:\nservice:\nname: whoami-service\nport:\nnumber: 80\n</code></pre> <p>Once you've created this, apply and test it:</p> <pre><code># apply the manifests\nroot@k0s-host \u279c kubectl apply -f whoami.yaml\ndeployment.apps/whoami-deployment created\nservice/whoami-service created\ningress.networking.k8s.io/whoami-ingress created\n# test the ingress and service\nroot@k0s-host \u279c curl http://192.168.0.5/whoami\nHostname: whoami-deployment-85bfbd48f-7l77c\nIP: 127.0.0.1\nIP: ::1\nIP: 10.244.214.198\nIP: fe80::b049:f8ff:fe77:3e64\nRemoteAddr: 10.244.214.196:34858\nGET /whoami HTTP/1.1\nHost: 192.168.0.5\nUser-Agent: curl/7.68.0\nAccept: */*\nAccept-Encoding: gzip\nX-Forwarded-For: 192.168.0.82\nX-Forwarded-Host: 192.168.0.5\nX-Forwarded-Port: 80\nX-Forwarded-Proto: http\nX-Forwarded-Server: traefik-1607085579-77bbc57699-b2f2t\nX-Real-Ip: 192.168.0.82\n</code></pre>"},{"location":"examples/traefik-ingress/#summary","title":"Summary","text":"<p>From here, it's possible to use 3rd party tools, such as ngrok, to go further and expose your LoadBalancer to the world. Doing so then enables dynamic certificate provisioning through Let's Encrypt utilizing either cert-manager or Traefik's own built-in ACME provider. This guide should have given you a general idea of getting started with Ingress on k0s and exposing your applications and services quickly.</p>"},{"location":"internal/host-dependencies/","title":"Host Dependencies","text":"<p>The goal of k0s is to only depend on kernel functionality. However, the kubelet currently has several host dependencies. Some of these may need to be bundled, but we should prefer upstream contributions to remove these dependencies.</p>"},{"location":"internal/host-dependencies/#list-of-hard-dependencies","title":"List of hard dependencies","text":"<ul> <li><code>find</code> -- PR by @ncopa to resolve this: https://github.com/kubernetes/kubernetes/pull/95189</li> <li><code>du</code> -- PR by @ncopa to resolve this: https://github.com/kubernetes/kubernetes/pull/95178 -- note that <code>du</code> dependency remains, but using POSIX-compliant argument </li> <li><code>nice</code></li> <li><code>iptables</code> -- as documented in https://github.com/k0sproject/k0s/issues/176 it is unclear whether <code>iptables</code> is needed. It appears to come from the <code>portmap</code> plugin, but the most robust solution may be to simply bundle <code>iptables</code> with k0s.</li> </ul>"},{"location":"internal/publishing_docs_using_mkdocs/","title":"Publishing Docs","text":"<p>We use mkdocs and mike for publishing docs to docs.k0sproject.io. This guide will provide a simple how-to on how to configure and deploy newly added docs to our website.</p>"},{"location":"internal/publishing_docs_using_mkdocs/#requirements","title":"Requirements","text":"<p>Install mike: https://github.com/jimporter/mike#installation</p>"},{"location":"internal/publishing_docs_using_mkdocs/#adding-a-new-link-to-the-navigation","title":"Adding A New link to the Navigation","text":"<ul> <li>All docs must live under the <code>docs</code> directory (I.E., changes to the main <code>README.md</code> are not reflected in the website).</li> <li>Add a new link under <code>nav</code> in the main mkdocs.yml file:     <code>nav:     - Overview: README.md     - Creating A Cluster:         - Quick Start Guide: create-cluster.md         - Run in Docker: k0s-in-docker.md         - Single node set-up: k0s-single-node.md     - Configuration Reference:         - Architecture: architecture.md         - Networking: networking.md         - Configuration Options: configuration.md         - Configuring Containerd: containerd_config.md         - Using A Custom CRI: custom-cri-runtime.md         - Using Cloud Providers: cloud-providers.md         - Running k0s with Traefik: examples/traefik-ingress.md         - Running k0s as a service: install.md         - k0s CLI Help Pages: cli/k0s.md     - Deploying Manifests: manifests.md     - FAQ: FAQ.md     - Troubleshooting: troubleshooting.md     - Contributing:         - Overview: contributors/overview.md         - Workflow: contributors/github_workflow.md         - Testing: contributors/testing.md</code></li> </ul> <ul> <li>Once your changes are pushed to <code>main</code>, the \"Publish Docs\" jos will start running: https://github.com/k0sproject/k0s/actions?query=workflow%3A%22Publish+docs+via+GitHub+Pages%22</li> <li>You should see the deployment outcome in the <code>gh-pages</code> deployment page: https://github.com/k0sproject/k0s/deployments/activity_log?environment=github-pages    </li> </ul>"},{"location":"internal/publishing_docs_using_mkdocs/#testing-docs-locally","title":"Testing docs locally","text":"<p>We've got a dockerized setup for easily testing docs in local environment. Simply run <code>docker-compose up</code> in the docs root folder. The docs will be available on <code>localhost:80</code>.</p> <p>Note If you have something already running locally on port <code>80</code> you need to change the mapped port on the <code>docker-compose.yml</code> file.</p>"},{"location":"internal/upgrading-calico/","title":"Upgrading Calico","text":"<p>k0s bundles Kubernetes manifests for Calico. The manifests are retrieved from the official Calico docs.</p> <p>As fetching and modifying the entire multi-thousand line file is error-prone, you may follow these steps to upgrade Calico to the latest version:</p> <ol> <li>run <code>./get-calico.sh</code></li> <li>check the git diff to see if it looks sensible</li> <li>re-apply our manual adjustments (documented below)</li> <li>run <code>make bindata-manifests</code></li> <li>compile, pray, and test</li> <li>commit and create a PR</li> </ol>"},{"location":"internal/upgrading-calico/#manual-adjustments","title":"Manual Adjustments","text":"<p>Note: All manual adjustments should be fairly obvious from the git diff. This section attempts to provide a sanity checklist to go through and make sure we still have those changes applied. The code blocks in this section are our modifications, not the calico originals.</p> <p><code>static/manifests/calico/DaemonSet/calico-node.yaml</code>:</p> <ul> <li>variable-based support for both vxlan and ipip (search for <code>ipip</code> to find): <pre><code>{{- if eq .Mode \"ipip\" }}\n# Enable IPIP\n- name: CALICO_IPV4POOL_IPIP\n  value: {{ .Overlay }}\n# Enable or Disable VXLAN on the default IP pool.\n- name: CALICO_IPV4POOL_VXLAN\n  value: \"Never\"\n{{- else if eq .Mode \"vxlan\" }}\n# Disable IPIP\n- name: CALICO_IPV4POOL_IPIP\n  value: \"Never\"\n# Enable VXLAN on the default IP pool.\n- name: CALICO_IPV4POOL_VXLAN\n  value: {{ .Overlay }}\n- name: FELIX_VXLANPORT\n  value: \"{{ .VxlanPort }}\"\n- name: FELIX_VXLANVNI\n  value: \"{{ .VxlanVNI }}\"\n{{- end }}\n</code></pre></li> <li>iptables auto detect: <pre><code># Auto detect the iptables backend\n- name: FELIX_IPTABLESBACKEND\n  value: \"auto\"\n</code></pre></li> <li>variable-based WireGuard support: <pre><code>{{- if .EnableWireguard }}\n- name: FELIX_WIREGUARDENABLED\n  value: \"true\"\n{{- end }}\n</code></pre></li> <li>variable-based cluster CIDR: <pre><code>- name: CALICO_IPV4POOL_CIDR\n  value: \"{{ .ClusterCIDR }}\"\n</code></pre></li> <li>custom backend and MTU <pre><code># calico-config.yaml\ncalico_backend: \"{{ .Mode }}\"\nveth_mtu: \"{{ .MTU }}\"\n</code></pre></li> <li>remove bgp from <code>CLUSTER_TYPE</code> <pre><code>- name: CLUSTER_TYPE\n  value: \"k8s\"\n</code></pre></li> <li>disable BIRD checks on liveness and readiness as we don't support BGP by removing <code>-bird-ready</code> and <code>-bird-live</code> from the readiness and liveness probes respectively</li> </ul>"},{"location":"internal/upgrading-calico/#container-image-names","title":"Container image names","text":"<p>Instead of hardcoded image names and versions use placeholders to support configuration level settings. Following placeholders are used:</p> <ul> <li><code>CalicoCNIImage</code> for calico/cni</li> <li><code>CalicoNodeImage</code> for calico/node</li> <li><code>CalicoKubeControllersImage</code> for calico/kube-controllers</li> </ul> <p>Also, all containers in manifests were modified to have 'imagePullPolicy' field: <pre><code>imagePullPolicy: {{ .PullPolicy }}\n</code></pre></p> <p>Example:  <pre><code># calico-node.yaml\nimage: {{ .CalicoCNIImage }}\n</code></pre></p>"}]}